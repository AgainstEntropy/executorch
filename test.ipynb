{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0.dev20240507\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from executorch.examples.models.llama2.builder import load_llama_model, DType\n",
    "from executorch.examples.models.llama2.source_transformation.quantize import WeightOnlyInt8QuantHandler\n",
    "from executorch.examples.models.llama2.source_transformation.sdpa import replace_sdpa_with_simple_sdpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2024-05-29 14:04:08,422 builder.py:84] Loading model with checkpoint=stories15M.pt, params=params.json, use_kv_cache=True, weight_type=WeightType.LLAMA\n",
      "[INFO 2024-05-29 14:04:08,451 builder.py:105] Loaded model with dtype=torch.float32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freqs_cos shape torch.Size([128, 24]), freqs_sin shape torch.Size([128, 24])\n",
      "quantize * ('layers.0.attention.wq', Linear(in_features=288, out_features=288, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.0.attention.wk', Linear(in_features=288, out_features=288, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.0.attention.wv', Linear(in_features=288, out_features=288, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.0.attention.wo', Linear(in_features=288, out_features=288, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.0.feed_forward.w1', Linear(in_features=288, out_features=768, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.0.feed_forward.w2', Linear(in_features=768, out_features=288, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.0.feed_forward.w3', Linear(in_features=288, out_features=768, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.1.attention.wq', Linear(in_features=288, out_features=288, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.1.attention.wk', Linear(in_features=288, out_features=288, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.1.attention.wv', Linear(in_features=288, out_features=288, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.1.attention.wo', Linear(in_features=288, out_features=288, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.1.feed_forward.w1', Linear(in_features=288, out_features=768, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.1.feed_forward.w2', Linear(in_features=768, out_features=288, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.1.feed_forward.w3', Linear(in_features=288, out_features=768, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.2.attention.wq', Linear(in_features=288, out_features=288, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.2.attention.wk', Linear(in_features=288, out_features=288, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.2.attention.wv', Linear(in_features=288, out_features=288, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.2.attention.wo', Linear(in_features=288, out_features=288, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.2.feed_forward.w1', Linear(in_features=288, out_features=768, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.2.feed_forward.w2', Linear(in_features=768, out_features=288, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.2.feed_forward.w3', Linear(in_features=288, out_features=768, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.3.attention.wq', Linear(in_features=288, out_features=288, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.3.attention.wk', Linear(in_features=288, out_features=288, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.3.attention.wv', Linear(in_features=288, out_features=288, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.3.attention.wo', Linear(in_features=288, out_features=288, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.3.feed_forward.w1', Linear(in_features=288, out_features=768, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.3.feed_forward.w2', Linear(in_features=768, out_features=288, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.3.feed_forward.w3', Linear(in_features=288, out_features=768, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.4.attention.wq', Linear(in_features=288, out_features=288, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.4.attention.wk', Linear(in_features=288, out_features=288, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.4.attention.wv', Linear(in_features=288, out_features=288, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.4.attention.wo', Linear(in_features=288, out_features=288, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.4.feed_forward.w1', Linear(in_features=288, out_features=768, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.4.feed_forward.w2', Linear(in_features=768, out_features=288, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.4.feed_forward.w3', Linear(in_features=288, out_features=768, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.5.attention.wq', Linear(in_features=288, out_features=288, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.5.attention.wk', Linear(in_features=288, out_features=288, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.5.attention.wv', Linear(in_features=288, out_features=288, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.5.attention.wo', Linear(in_features=288, out_features=288, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.5.feed_forward.w1', Linear(in_features=288, out_features=768, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.5.feed_forward.w2', Linear(in_features=768, out_features=288, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('layers.5.feed_forward.w3', Linear(in_features=288, out_features=768, bias=False)) with group_size None, bitwidth 8\n",
      "quantize * ('output', Linear(in_features=288, out_features=32000, bias=False)) with group_size None, bitwidth 8\n"
     ]
    }
   ],
   "source": [
    "# stories110M\n",
    "\n",
    "checkpoint = \"stories15M.pt\"\n",
    "params = \"params.json\"\n",
    "transforms = [\n",
    "    lambda m: WeightOnlyInt8QuantHandler(m).quantized_model(),\n",
    "    replace_sdpa_with_simple_sdpa,\n",
    "]\n",
    "\n",
    "model = load_llama_model(\n",
    "    checkpoint=checkpoint,\n",
    "    params_path=params,\n",
    "    use_kv_cache=True,\n",
    ").to_dtype(DType.fp32).source_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from executorch.backends.apple.mps.partition.mps_partitioner import (\n",
    "    MPSPartitioner,\n",
    ")\n",
    "from executorch.exir.backend.backend_details import CompileSpec\n",
    "compile_specs = [CompileSpec(\"use_fp16\", bytes([True]))]\n",
    "\n",
    "partitioners = [\n",
    "    MPSPartitioner(compile_specs)\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass to replace aten::_weight_int8pack_mm to llama_cpp::_weight_int8pack_mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from executorch.exir.pass_base import ExportPass\n",
    "from executorch.examples.models.llama2.custom_ops.llama_cpp_linear import *\n",
    "from executorch.exir.dialects._ops import ops as exir_ops\n",
    "\n",
    "class ReplaceMMPass(ExportPass):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def call_operator(self, op, args, kwargs, meta):\n",
    "        if op == exir_ops.edge.aten._weight_int8pack_mm.default:\n",
    "            return super().call_operator(exir_ops.edge.llama_cpp._weight_int8pack_mm.default, args, kwargs, meta)\n",
    "        else:\n",
    "            return super().call_operator(op, args, kwargs, meta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2024-05-29 13:37:13,459 mps_partitioner.py:121] Found 25 subgraphs to be partitioned.\n",
      "[INFO 2024-05-29 13:37:13,460 utils.py:527] The buffer node is a mutated buffer node, which is not constant.\n",
      "[INFO 2024-05-29 13:37:13,460 utils.py:527] The buffer node is a mutated buffer node, which is not constant.\n",
      "[INFO 2024-05-29 13:37:13,461 utils.py:527] The buffer node is a mutated buffer node, which is not constant.\n",
      "[INFO 2024-05-29 13:37:13,461 utils.py:527] The buffer node is a mutated buffer node, which is not constant.\n",
      "[INFO 2024-05-29 13:37:13,462 utils.py:527] The buffer node is a mutated buffer node, which is not constant.\n",
      "[INFO 2024-05-29 13:37:13,462 utils.py:527] The buffer node is a mutated buffer node, which is not constant.\n",
      "[INFO 2024-05-29 13:37:13,463 utils.py:527] The buffer node is a mutated buffer node, which is not constant.\n",
      "[INFO 2024-05-29 13:37:13,463 utils.py:527] The buffer node is a mutated buffer node, which is not constant.\n",
      "[INFO 2024-05-29 13:37:13,464 utils.py:527] The buffer node is a mutated buffer node, which is not constant.\n",
      "[INFO 2024-05-29 13:37:13,464 utils.py:527] The buffer node is a mutated buffer node, which is not constant.\n",
      "[INFO 2024-05-29 13:37:13,464 utils.py:527] The buffer node is a mutated buffer node, which is not constant.\n",
      "[INFO 2024-05-29 13:37:13,465 utils.py:527] The buffer node is a mutated buffer node, which is not constant.\n",
      "[INFO 2024-05-29 13:37:13,503 mps_preprocess.py:115] Visiting: aten_add_tensor_41, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:13,503 mps_preprocess.py:115] Visiting: aten_mul_tensor_102, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:13,503 mps_preprocess.py:115] Visiting: aten_mean_dim_12, aten.mean.dim\n",
      "[INFO 2024-05-29 13:37:13,504 mps_preprocess.py:115] Visiting: aten_add_tensor_42, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:13,504 mps_preprocess.py:115] Visiting: aten_rsqrt_default_12, aten.rsqrt.default\n",
      "[INFO 2024-05-29 13:37:13,504 mps_preprocess.py:115] Visiting: aten_mul_tensor_103, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:13,504 mps_preprocess.py:115] Visiting: aten_mul_tensor_104, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:13,505 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_60, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:13,661 mps_preprocess.py:115] Visiting: aten_sigmoid_default_5, aten.sigmoid.default\n",
      "[INFO 2024-05-29 13:37:13,662 mps_preprocess.py:115] Visiting: aten_mul_tensor_100, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:13,662 mps_preprocess.py:115] Visiting: aten_mul_tensor_101, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:13,758 mps_preprocess.py:115] Visiting: aten_add_tensor_39, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:13,759 mps_preprocess.py:115] Visiting: aten_mul_tensor_97, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:13,759 mps_preprocess.py:115] Visiting: aten_mean_dim_11, aten.mean.dim\n",
      "[INFO 2024-05-29 13:37:13,760 mps_preprocess.py:115] Visiting: aten_add_tensor_40, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:13,760 mps_preprocess.py:115] Visiting: aten_rsqrt_default_11, aten.rsqrt.default\n",
      "[INFO 2024-05-29 13:37:13,760 mps_preprocess.py:115] Visiting: aten_mul_tensor_98, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:13,760 mps_preprocess.py:115] Visiting: aten_mul_tensor_99, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:13,761 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_58, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:13,761 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_59, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:13,894 mps_preprocess.py:115] Visiting: aten_view_copy_default_105, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:13,895 mps_preprocess.py:115] Visiting: aten_view_copy_default_106, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:13,895 mps_preprocess.py:115] Visiting: aten_view_copy_default_100, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:13,895 mps_preprocess.py:115] Visiting: aten_view_copy_default_101, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:13,895 mps_preprocess.py:115] Visiting: aten_view_copy_default_102, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:13,895 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_44, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:13,896 mps_preprocess.py:115] Visiting: aten_view_copy_default_103, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:13,896 mps_preprocess.py:115] Visiting: aten_view_copy_default_104, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:13,896 mps_preprocess.py:115] Visiting: aten_permute_copy_default_27, aten.permute_copy.default\n",
      "[INFO 2024-05-29 13:37:13,896 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_45, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:13,896 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_20, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 13:37:13,897 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_21, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 13:37:13,897 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_22, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 13:37:13,897 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_23, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 13:37:13,897 mps_preprocess.py:115] Visiting: aten_view_copy_default_110, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:13,898 mps_preprocess.py:115] Visiting: aten_index_tensor_7, aten.index.Tensor\n",
      "[INFO 2024-05-29 13:37:13,898 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_53, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:13,898 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_54, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:13,898 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_55, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:13,898 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_56, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:13,898 mps_preprocess.py:115] Visiting: aten_index_put_default_11, aten.index_put.default\n",
      "[INFO 2024-05-29 13:37:13,899 mps_preprocess.py:115] Visiting: aten__to_copy_default_5, aten._to_copy.default\n",
      "[INFO 2024-05-29 13:37:13,899 mps_preprocess.py:115] Visiting: aten_mul_tensor_88, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:13,899 mps_preprocess.py:115] Visiting: aten_mul_tensor_90, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:13,899 mps_preprocess.py:115] Visiting: aten_mul_tensor_89, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:13,899 mps_preprocess.py:115] Visiting: aten_mul_tensor_91, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:13,899 mps_preprocess.py:115] Visiting: aten_mul_tensor_92, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:13,900 mps_preprocess.py:115] Visiting: aten_mul_tensor_94, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:13,900 mps_preprocess.py:115] Visiting: aten_mul_tensor_93, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:13,900 mps_preprocess.py:115] Visiting: aten_mul_tensor_95, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:13,900 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_22, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 13:37:13,901 mps_preprocess.py:115] Visiting: aten_sub_tensor_10, aten.sub.Tensor\n",
      "[INFO 2024-05-29 13:37:13,901 mps_preprocess.py:115] Visiting: aten_add_tensor_36, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:13,901 mps_preprocess.py:115] Visiting: aten_sub_tensor_11, aten.sub.Tensor\n",
      "[INFO 2024-05-29 13:37:13,901 mps_preprocess.py:115] Visiting: aten_add_tensor_37, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:13,902 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_23, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 13:37:13,902 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_40, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:13,902 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_41, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:13,902 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_42, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:13,903 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_43, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:13,903 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_47, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:13,903 mps_preprocess.py:115] Visiting: aten_cat_default_10, aten.cat.default\n",
      "[INFO 2024-05-29 13:37:13,903 mps_preprocess.py:115] Visiting: aten_cat_default_11, aten.cat.default\n",
      "[INFO 2024-05-29 13:37:13,903 mps_preprocess.py:115] Visiting: aten_expand_copy_default_31, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:13,904 mps_preprocess.py:115] Visiting: aten_view_copy_default_107, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:13,904 mps_preprocess.py:115] Visiting: aten_view_copy_default_108, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:13,904 mps_preprocess.py:115] Visiting: aten_clone_default_11, aten.clone.default\n",
      "[INFO 2024-05-29 13:37:13,904 mps_preprocess.py:115] Visiting: aten_permute_copy_default_25, aten.permute_copy.default\n",
      "[INFO 2024-05-29 13:37:13,904 mps_preprocess.py:115] Visiting: aten_permute_copy_default_26, aten.permute_copy.default\n",
      "[INFO 2024-05-29 13:37:13,905 mps_preprocess.py:115] Visiting: aten_view_copy_default_112, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:13,905 mps_preprocess.py:115] Visiting: aten_expand_copy_default_32, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:13,906 mps_preprocess.py:115] Visiting: aten_view_copy_default_109, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:13,906 mps_preprocess.py:115] Visiting: aten_expand_copy_default_35, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:13,906 mps_preprocess.py:115] Visiting: aten_view_copy_default_113, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:13,907 mps_preprocess.py:115] Visiting: aten_index_put_default_10, aten.index_put.default\n",
      "[INFO 2024-05-29 13:37:13,907 mps_preprocess.py:115] Visiting: aten_view_copy_default_117, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:13,907 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_20, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 13:37:13,908 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_21, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 13:37:13,909 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_46, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:13,909 mps_preprocess.py:115] Visiting: aten_expand_copy_default_30, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:13,909 mps_preprocess.py:115] Visiting: aten_clone_default_10, aten.clone.default\n",
      "[INFO 2024-05-29 13:37:13,910 mps_preprocess.py:115] Visiting: aten_view_copy_default_111, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:13,910 mps_preprocess.py:115] Visiting: aten_permute_copy_default_28, aten.permute_copy.default\n",
      "[INFO 2024-05-29 13:37:13,910 mps_preprocess.py:115] Visiting: aten_expand_copy_default_33, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:13,910 mps_preprocess.py:115] Visiting: aten_view_copy_default_114, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:13,911 mps_preprocess.py:115] Visiting: aten_bmm_default_10, aten.bmm.default\n",
      "[INFO 2024-05-29 13:37:13,911 mps_preprocess.py:115] Visiting: aten_view_copy_default_115, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:13,911 mps_preprocess.py:115] Visiting: aten_mul_tensor_96, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:13,912 mps_preprocess.py:115] Visiting: aten_add_tensor_38, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:13,912 mps_preprocess.py:115] Visiting: aten__softmax_default_5, aten._softmax.default\n",
      "[INFO 2024-05-29 13:37:13,912 mps_preprocess.py:115] Visiting: aten_expand_copy_default_34, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:13,912 mps_preprocess.py:115] Visiting: aten_view_copy_default_116, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:13,912 mps_preprocess.py:115] Visiting: aten_bmm_default_11, aten.bmm.default\n",
      "[INFO 2024-05-29 13:37:13,913 mps_preprocess.py:115] Visiting: aten_view_copy_default_118, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:13,913 mps_preprocess.py:115] Visiting: aten_permute_copy_default_29, aten.permute_copy.default\n",
      "[INFO 2024-05-29 13:37:13,913 mps_preprocess.py:115] Visiting: aten_view_copy_default_119, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:13,913 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_57, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:14,143 mps_preprocess.py:115] Visiting: aten_add_tensor_34, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:14,144 mps_preprocess.py:115] Visiting: aten_mul_tensor_85, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:14,144 mps_preprocess.py:115] Visiting: aten_mean_dim_10, aten.mean.dim\n",
      "[INFO 2024-05-29 13:37:14,144 mps_preprocess.py:115] Visiting: aten_add_tensor_35, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:14,145 mps_preprocess.py:115] Visiting: aten_rsqrt_default_10, aten.rsqrt.default\n",
      "[INFO 2024-05-29 13:37:14,145 mps_preprocess.py:115] Visiting: aten_mul_tensor_86, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:14,145 mps_preprocess.py:115] Visiting: aten_mul_tensor_87, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:14,145 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_50, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:14,146 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_51, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:14,146 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_52, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:14,238 mps_preprocess.py:115] Visiting: aten_sigmoid_default_4, aten.sigmoid.default\n",
      "[INFO 2024-05-29 13:37:14,239 mps_preprocess.py:115] Visiting: aten_mul_tensor_83, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:14,239 mps_preprocess.py:115] Visiting: aten_mul_tensor_84, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:14,333 mps_preprocess.py:115] Visiting: aten_add_tensor_32, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:14,334 mps_preprocess.py:115] Visiting: aten_mul_tensor_80, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:14,334 mps_preprocess.py:115] Visiting: aten_mean_dim_9, aten.mean.dim\n",
      "[INFO 2024-05-29 13:37:14,334 mps_preprocess.py:115] Visiting: aten_add_tensor_33, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:14,334 mps_preprocess.py:115] Visiting: aten_rsqrt_default_9, aten.rsqrt.default\n",
      "[INFO 2024-05-29 13:37:14,334 mps_preprocess.py:115] Visiting: aten_mul_tensor_81, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:14,335 mps_preprocess.py:115] Visiting: aten_mul_tensor_82, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:14,335 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_48, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:14,335 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_49, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:14,453 mps_preprocess.py:115] Visiting: aten_view_copy_default_85, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:14,453 mps_preprocess.py:115] Visiting: aten_view_copy_default_86, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:14,453 mps_preprocess.py:115] Visiting: aten_view_copy_default_80, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:14,454 mps_preprocess.py:115] Visiting: aten_view_copy_default_81, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:14,454 mps_preprocess.py:115] Visiting: aten_view_copy_default_82, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:14,454 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_36, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:14,454 mps_preprocess.py:115] Visiting: aten_view_copy_default_83, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:14,455 mps_preprocess.py:115] Visiting: aten_view_copy_default_84, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:14,455 mps_preprocess.py:115] Visiting: aten_permute_copy_default_22, aten.permute_copy.default\n",
      "[INFO 2024-05-29 13:37:14,455 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_37, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:14,455 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_16, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 13:37:14,455 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_17, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 13:37:14,456 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_18, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 13:37:14,456 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_19, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 13:37:14,456 mps_preprocess.py:115] Visiting: aten_view_copy_default_90, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:14,457 mps_preprocess.py:115] Visiting: aten_index_tensor_6, aten.index.Tensor\n",
      "[INFO 2024-05-29 13:37:14,457 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_43, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:14,457 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_44, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:14,457 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_45, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:14,457 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_46, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:14,458 mps_preprocess.py:115] Visiting: aten_index_put_default_9, aten.index_put.default\n",
      "[INFO 2024-05-29 13:37:14,458 mps_preprocess.py:115] Visiting: aten__to_copy_default_4, aten._to_copy.default\n",
      "[INFO 2024-05-29 13:37:14,458 mps_preprocess.py:115] Visiting: aten_mul_tensor_71, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:14,458 mps_preprocess.py:115] Visiting: aten_mul_tensor_73, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:14,458 mps_preprocess.py:115] Visiting: aten_mul_tensor_72, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:14,459 mps_preprocess.py:115] Visiting: aten_mul_tensor_74, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:14,459 mps_preprocess.py:115] Visiting: aten_mul_tensor_75, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:14,459 mps_preprocess.py:115] Visiting: aten_mul_tensor_77, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:14,459 mps_preprocess.py:115] Visiting: aten_mul_tensor_76, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:14,459 mps_preprocess.py:115] Visiting: aten_mul_tensor_78, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:14,460 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_18, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 13:37:14,460 mps_preprocess.py:115] Visiting: aten_sub_tensor_8, aten.sub.Tensor\n",
      "[INFO 2024-05-29 13:37:14,460 mps_preprocess.py:115] Visiting: aten_add_tensor_29, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:14,461 mps_preprocess.py:115] Visiting: aten_sub_tensor_9, aten.sub.Tensor\n",
      "[INFO 2024-05-29 13:37:14,461 mps_preprocess.py:115] Visiting: aten_add_tensor_30, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:14,461 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_19, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 13:37:14,462 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_32, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:14,462 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_33, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:14,462 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_34, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:14,463 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_35, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:14,463 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_39, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:14,463 mps_preprocess.py:115] Visiting: aten_cat_default_8, aten.cat.default\n",
      "[INFO 2024-05-29 13:37:14,463 mps_preprocess.py:115] Visiting: aten_cat_default_9, aten.cat.default\n",
      "[INFO 2024-05-29 13:37:14,463 mps_preprocess.py:115] Visiting: aten_expand_copy_default_25, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:14,464 mps_preprocess.py:115] Visiting: aten_view_copy_default_87, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:14,464 mps_preprocess.py:115] Visiting: aten_view_copy_default_88, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:14,464 mps_preprocess.py:115] Visiting: aten_clone_default_9, aten.clone.default\n",
      "[INFO 2024-05-29 13:37:14,464 mps_preprocess.py:115] Visiting: aten_permute_copy_default_20, aten.permute_copy.default\n",
      "[INFO 2024-05-29 13:37:14,464 mps_preprocess.py:115] Visiting: aten_permute_copy_default_21, aten.permute_copy.default\n",
      "[INFO 2024-05-29 13:37:14,465 mps_preprocess.py:115] Visiting: aten_view_copy_default_92, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:14,465 mps_preprocess.py:115] Visiting: aten_expand_copy_default_26, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:14,465 mps_preprocess.py:115] Visiting: aten_view_copy_default_89, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:14,466 mps_preprocess.py:115] Visiting: aten_expand_copy_default_29, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:14,466 mps_preprocess.py:115] Visiting: aten_view_copy_default_93, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:14,466 mps_preprocess.py:115] Visiting: aten_index_put_default_8, aten.index_put.default\n",
      "[INFO 2024-05-29 13:37:14,467 mps_preprocess.py:115] Visiting: aten_view_copy_default_97, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:14,467 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_16, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 13:37:14,468 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_17, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 13:37:14,468 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_38, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:14,468 mps_preprocess.py:115] Visiting: aten_expand_copy_default_24, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:14,469 mps_preprocess.py:115] Visiting: aten_clone_default_8, aten.clone.default\n",
      "[INFO 2024-05-29 13:37:14,469 mps_preprocess.py:115] Visiting: aten_view_copy_default_91, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:14,469 mps_preprocess.py:115] Visiting: aten_permute_copy_default_23, aten.permute_copy.default\n",
      "[INFO 2024-05-29 13:37:14,470 mps_preprocess.py:115] Visiting: aten_expand_copy_default_27, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:14,470 mps_preprocess.py:115] Visiting: aten_view_copy_default_94, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:14,470 mps_preprocess.py:115] Visiting: aten_bmm_default_8, aten.bmm.default\n",
      "[INFO 2024-05-29 13:37:14,470 mps_preprocess.py:115] Visiting: aten_view_copy_default_95, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:14,470 mps_preprocess.py:115] Visiting: aten_mul_tensor_79, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:14,471 mps_preprocess.py:115] Visiting: aten_add_tensor_31, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:14,471 mps_preprocess.py:115] Visiting: aten__softmax_default_4, aten._softmax.default\n",
      "[INFO 2024-05-29 13:37:14,471 mps_preprocess.py:115] Visiting: aten_expand_copy_default_28, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:14,471 mps_preprocess.py:115] Visiting: aten_view_copy_default_96, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:14,471 mps_preprocess.py:115] Visiting: aten_bmm_default_9, aten.bmm.default\n",
      "[INFO 2024-05-29 13:37:14,472 mps_preprocess.py:115] Visiting: aten_view_copy_default_98, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:14,472 mps_preprocess.py:115] Visiting: aten_permute_copy_default_24, aten.permute_copy.default\n",
      "[INFO 2024-05-29 13:37:14,472 mps_preprocess.py:115] Visiting: aten_view_copy_default_99, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:14,472 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_47, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:14,700 mps_preprocess.py:115] Visiting: aten_add_tensor_27, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:14,701 mps_preprocess.py:115] Visiting: aten_mul_tensor_68, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:14,701 mps_preprocess.py:115] Visiting: aten_mean_dim_8, aten.mean.dim\n",
      "[INFO 2024-05-29 13:37:14,701 mps_preprocess.py:115] Visiting: aten_add_tensor_28, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:14,701 mps_preprocess.py:115] Visiting: aten_rsqrt_default_8, aten.rsqrt.default\n",
      "[INFO 2024-05-29 13:37:14,702 mps_preprocess.py:115] Visiting: aten_mul_tensor_69, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:14,702 mps_preprocess.py:115] Visiting: aten_mul_tensor_70, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:14,702 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_40, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:14,702 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_41, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:14,702 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_42, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:14,789 mps_preprocess.py:115] Visiting: aten_sigmoid_default_3, aten.sigmoid.default\n",
      "[INFO 2024-05-29 13:37:14,790 mps_preprocess.py:115] Visiting: aten_mul_tensor_66, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:14,790 mps_preprocess.py:115] Visiting: aten_mul_tensor_67, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:14,891 mps_preprocess.py:115] Visiting: aten_add_tensor_25, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:14,891 mps_preprocess.py:115] Visiting: aten_mul_tensor_63, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:14,891 mps_preprocess.py:115] Visiting: aten_mean_dim_7, aten.mean.dim\n",
      "[INFO 2024-05-29 13:37:14,892 mps_preprocess.py:115] Visiting: aten_add_tensor_26, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:14,892 mps_preprocess.py:115] Visiting: aten_rsqrt_default_7, aten.rsqrt.default\n",
      "[INFO 2024-05-29 13:37:14,892 mps_preprocess.py:115] Visiting: aten_mul_tensor_64, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:14,892 mps_preprocess.py:115] Visiting: aten_mul_tensor_65, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:14,893 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_38, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:14,893 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_39, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:15,004 mps_preprocess.py:115] Visiting: aten_view_copy_default_65, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,005 mps_preprocess.py:115] Visiting: aten_view_copy_default_66, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,005 mps_preprocess.py:115] Visiting: aten_view_copy_default_60, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,006 mps_preprocess.py:115] Visiting: aten_view_copy_default_61, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,008 mps_preprocess.py:115] Visiting: aten_view_copy_default_62, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,010 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_28, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:15,011 mps_preprocess.py:115] Visiting: aten_view_copy_default_63, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,012 mps_preprocess.py:115] Visiting: aten_view_copy_default_64, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,013 mps_preprocess.py:115] Visiting: aten_permute_copy_default_17, aten.permute_copy.default\n",
      "[INFO 2024-05-29 13:37:15,013 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_29, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:15,014 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_12, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 13:37:15,014 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_13, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 13:37:15,014 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_14, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 13:37:15,015 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_15, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 13:37:15,015 mps_preprocess.py:115] Visiting: aten_view_copy_default_70, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,015 mps_preprocess.py:115] Visiting: aten_index_tensor_5, aten.index.Tensor\n",
      "[INFO 2024-05-29 13:37:15,016 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_33, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:15,016 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_34, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:15,016 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_35, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:15,016 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_36, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:15,016 mps_preprocess.py:115] Visiting: aten_index_put_default_7, aten.index_put.default\n",
      "[INFO 2024-05-29 13:37:15,017 mps_preprocess.py:115] Visiting: aten__to_copy_default_3, aten._to_copy.default\n",
      "[INFO 2024-05-29 13:37:15,017 mps_preprocess.py:115] Visiting: aten_mul_tensor_54, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:15,017 mps_preprocess.py:115] Visiting: aten_mul_tensor_56, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:15,018 mps_preprocess.py:115] Visiting: aten_mul_tensor_55, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:15,018 mps_preprocess.py:115] Visiting: aten_mul_tensor_57, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:15,018 mps_preprocess.py:115] Visiting: aten_mul_tensor_58, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:15,018 mps_preprocess.py:115] Visiting: aten_mul_tensor_60, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:15,019 mps_preprocess.py:115] Visiting: aten_mul_tensor_59, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:15,019 mps_preprocess.py:115] Visiting: aten_mul_tensor_61, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:15,019 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_14, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 13:37:15,022 mps_preprocess.py:115] Visiting: aten_sub_tensor_6, aten.sub.Tensor\n",
      "[INFO 2024-05-29 13:37:15,022 mps_preprocess.py:115] Visiting: aten_add_tensor_22, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:15,022 mps_preprocess.py:115] Visiting: aten_sub_tensor_7, aten.sub.Tensor\n",
      "[INFO 2024-05-29 13:37:15,024 mps_preprocess.py:115] Visiting: aten_add_tensor_23, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:15,025 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_15, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 13:37:15,027 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_24, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:15,028 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_25, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:15,029 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_26, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:15,029 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_27, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:15,030 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_31, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:15,030 mps_preprocess.py:115] Visiting: aten_cat_default_6, aten.cat.default\n",
      "[INFO 2024-05-29 13:37:15,031 mps_preprocess.py:115] Visiting: aten_cat_default_7, aten.cat.default\n",
      "[INFO 2024-05-29 13:37:15,031 mps_preprocess.py:115] Visiting: aten_expand_copy_default_19, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:15,031 mps_preprocess.py:115] Visiting: aten_view_copy_default_67, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,033 mps_preprocess.py:115] Visiting: aten_view_copy_default_68, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,033 mps_preprocess.py:115] Visiting: aten_clone_default_7, aten.clone.default\n",
      "[INFO 2024-05-29 13:37:15,033 mps_preprocess.py:115] Visiting: aten_permute_copy_default_15, aten.permute_copy.default\n",
      "[INFO 2024-05-29 13:37:15,034 mps_preprocess.py:115] Visiting: aten_permute_copy_default_16, aten.permute_copy.default\n",
      "[INFO 2024-05-29 13:37:15,034 mps_preprocess.py:115] Visiting: aten_view_copy_default_72, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,034 mps_preprocess.py:115] Visiting: aten_expand_copy_default_20, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:15,034 mps_preprocess.py:115] Visiting: aten_view_copy_default_69, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,035 mps_preprocess.py:115] Visiting: aten_expand_copy_default_23, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:15,035 mps_preprocess.py:115] Visiting: aten_view_copy_default_73, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,035 mps_preprocess.py:115] Visiting: aten_index_put_default_6, aten.index_put.default\n",
      "[INFO 2024-05-29 13:37:15,036 mps_preprocess.py:115] Visiting: aten_view_copy_default_77, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,036 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_12, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 13:37:15,037 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_13, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 13:37:15,038 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_30, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:15,038 mps_preprocess.py:115] Visiting: aten_expand_copy_default_18, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:15,038 mps_preprocess.py:115] Visiting: aten_clone_default_6, aten.clone.default\n",
      "[INFO 2024-05-29 13:37:15,038 mps_preprocess.py:115] Visiting: aten_view_copy_default_71, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,038 mps_preprocess.py:115] Visiting: aten_permute_copy_default_18, aten.permute_copy.default\n",
      "[INFO 2024-05-29 13:37:15,039 mps_preprocess.py:115] Visiting: aten_expand_copy_default_21, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:15,039 mps_preprocess.py:115] Visiting: aten_view_copy_default_74, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,040 mps_preprocess.py:115] Visiting: aten_bmm_default_6, aten.bmm.default\n",
      "[INFO 2024-05-29 13:37:15,040 mps_preprocess.py:115] Visiting: aten_view_copy_default_75, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,040 mps_preprocess.py:115] Visiting: aten_mul_tensor_62, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:15,041 mps_preprocess.py:115] Visiting: aten_add_tensor_24, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:15,042 mps_preprocess.py:115] Visiting: aten__softmax_default_3, aten._softmax.default\n",
      "[INFO 2024-05-29 13:37:15,042 mps_preprocess.py:115] Visiting: aten_expand_copy_default_22, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:15,042 mps_preprocess.py:115] Visiting: aten_view_copy_default_76, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,043 mps_preprocess.py:115] Visiting: aten_bmm_default_7, aten.bmm.default\n",
      "[INFO 2024-05-29 13:37:15,043 mps_preprocess.py:115] Visiting: aten_view_copy_default_78, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,043 mps_preprocess.py:115] Visiting: aten_permute_copy_default_19, aten.permute_copy.default\n",
      "[INFO 2024-05-29 13:37:15,044 mps_preprocess.py:115] Visiting: aten_view_copy_default_79, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,044 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_37, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:15,269 mps_preprocess.py:115] Visiting: aten_add_tensor_20, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:15,270 mps_preprocess.py:115] Visiting: aten_mul_tensor_51, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:15,270 mps_preprocess.py:115] Visiting: aten_mean_dim_6, aten.mean.dim\n",
      "[INFO 2024-05-29 13:37:15,271 mps_preprocess.py:115] Visiting: aten_add_tensor_21, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:15,271 mps_preprocess.py:115] Visiting: aten_rsqrt_default_6, aten.rsqrt.default\n",
      "[INFO 2024-05-29 13:37:15,271 mps_preprocess.py:115] Visiting: aten_mul_tensor_52, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:15,271 mps_preprocess.py:115] Visiting: aten_mul_tensor_53, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:15,272 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_30, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:15,272 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_31, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:15,272 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_32, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:15,355 mps_preprocess.py:115] Visiting: aten_sigmoid_default_2, aten.sigmoid.default\n",
      "[INFO 2024-05-29 13:37:15,356 mps_preprocess.py:115] Visiting: aten_mul_tensor_49, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:15,356 mps_preprocess.py:115] Visiting: aten_mul_tensor_50, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:15,441 mps_preprocess.py:115] Visiting: aten_add_tensor_18, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:15,441 mps_preprocess.py:115] Visiting: aten_mul_tensor_46, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:15,442 mps_preprocess.py:115] Visiting: aten_mean_dim_5, aten.mean.dim\n",
      "[INFO 2024-05-29 13:37:15,442 mps_preprocess.py:115] Visiting: aten_add_tensor_19, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:15,442 mps_preprocess.py:115] Visiting: aten_rsqrt_default_5, aten.rsqrt.default\n",
      "[INFO 2024-05-29 13:37:15,442 mps_preprocess.py:115] Visiting: aten_mul_tensor_47, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:15,443 mps_preprocess.py:115] Visiting: aten_mul_tensor_48, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:15,443 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_28, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:15,443 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_29, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:15,539 mps_preprocess.py:115] Visiting: aten_view_copy_default_45, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,540 mps_preprocess.py:115] Visiting: aten_view_copy_default_46, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,540 mps_preprocess.py:115] Visiting: aten_view_copy_default_40, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,540 mps_preprocess.py:115] Visiting: aten_view_copy_default_41, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,541 mps_preprocess.py:115] Visiting: aten_view_copy_default_42, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,541 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_20, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:15,541 mps_preprocess.py:115] Visiting: aten_view_copy_default_43, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,541 mps_preprocess.py:115] Visiting: aten_view_copy_default_44, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,542 mps_preprocess.py:115] Visiting: aten_permute_copy_default_12, aten.permute_copy.default\n",
      "[INFO 2024-05-29 13:37:15,542 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_21, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:15,542 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_8, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 13:37:15,542 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_9, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 13:37:15,542 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_10, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 13:37:15,543 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_11, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 13:37:15,543 mps_preprocess.py:115] Visiting: aten_view_copy_default_50, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,543 mps_preprocess.py:115] Visiting: aten_index_tensor_4, aten.index.Tensor\n",
      "[INFO 2024-05-29 13:37:15,543 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_23, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:15,543 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_24, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:15,544 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_25, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:15,544 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_26, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:15,544 mps_preprocess.py:115] Visiting: aten_index_put_default_5, aten.index_put.default\n",
      "[INFO 2024-05-29 13:37:15,544 mps_preprocess.py:115] Visiting: aten__to_copy_default_2, aten._to_copy.default\n",
      "[INFO 2024-05-29 13:37:15,545 mps_preprocess.py:115] Visiting: aten_mul_tensor_37, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:15,545 mps_preprocess.py:115] Visiting: aten_mul_tensor_39, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:15,545 mps_preprocess.py:115] Visiting: aten_mul_tensor_38, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:15,545 mps_preprocess.py:115] Visiting: aten_mul_tensor_40, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:15,545 mps_preprocess.py:115] Visiting: aten_mul_tensor_41, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:15,545 mps_preprocess.py:115] Visiting: aten_mul_tensor_43, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:15,546 mps_preprocess.py:115] Visiting: aten_mul_tensor_42, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:15,546 mps_preprocess.py:115] Visiting: aten_mul_tensor_44, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:15,546 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_10, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 13:37:15,547 mps_preprocess.py:115] Visiting: aten_sub_tensor_4, aten.sub.Tensor\n",
      "[INFO 2024-05-29 13:37:15,547 mps_preprocess.py:115] Visiting: aten_add_tensor_15, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:15,547 mps_preprocess.py:115] Visiting: aten_sub_tensor_5, aten.sub.Tensor\n",
      "[INFO 2024-05-29 13:37:15,547 mps_preprocess.py:115] Visiting: aten_add_tensor_16, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:15,547 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_11, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 13:37:15,548 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_16, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:15,548 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_17, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:15,548 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_18, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:15,549 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_19, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:15,549 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_23, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:15,549 mps_preprocess.py:115] Visiting: aten_cat_default_4, aten.cat.default\n",
      "[INFO 2024-05-29 13:37:15,549 mps_preprocess.py:115] Visiting: aten_cat_default_5, aten.cat.default\n",
      "[INFO 2024-05-29 13:37:15,549 mps_preprocess.py:115] Visiting: aten_expand_copy_default_13, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:15,550 mps_preprocess.py:115] Visiting: aten_view_copy_default_47, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,550 mps_preprocess.py:115] Visiting: aten_view_copy_default_48, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,550 mps_preprocess.py:115] Visiting: aten_clone_default_5, aten.clone.default\n",
      "[INFO 2024-05-29 13:37:15,550 mps_preprocess.py:115] Visiting: aten_permute_copy_default_10, aten.permute_copy.default\n",
      "[INFO 2024-05-29 13:37:15,550 mps_preprocess.py:115] Visiting: aten_permute_copy_default_11, aten.permute_copy.default\n",
      "[INFO 2024-05-29 13:37:15,551 mps_preprocess.py:115] Visiting: aten_view_copy_default_52, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,551 mps_preprocess.py:115] Visiting: aten_expand_copy_default_14, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:15,551 mps_preprocess.py:115] Visiting: aten_view_copy_default_49, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,551 mps_preprocess.py:115] Visiting: aten_expand_copy_default_17, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:15,552 mps_preprocess.py:115] Visiting: aten_view_copy_default_53, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,552 mps_preprocess.py:115] Visiting: aten_index_put_default_4, aten.index_put.default\n",
      "[INFO 2024-05-29 13:37:15,552 mps_preprocess.py:115] Visiting: aten_view_copy_default_57, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,553 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_8, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 13:37:15,553 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_9, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 13:37:15,554 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_22, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:15,554 mps_preprocess.py:115] Visiting: aten_expand_copy_default_12, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:15,554 mps_preprocess.py:115] Visiting: aten_clone_default_4, aten.clone.default\n",
      "[INFO 2024-05-29 13:37:15,555 mps_preprocess.py:115] Visiting: aten_view_copy_default_51, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,555 mps_preprocess.py:115] Visiting: aten_permute_copy_default_13, aten.permute_copy.default\n",
      "[INFO 2024-05-29 13:37:15,555 mps_preprocess.py:115] Visiting: aten_expand_copy_default_15, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:15,556 mps_preprocess.py:115] Visiting: aten_view_copy_default_54, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,556 mps_preprocess.py:115] Visiting: aten_bmm_default_4, aten.bmm.default\n",
      "[INFO 2024-05-29 13:37:15,556 mps_preprocess.py:115] Visiting: aten_view_copy_default_55, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,556 mps_preprocess.py:115] Visiting: aten_mul_tensor_45, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:15,557 mps_preprocess.py:115] Visiting: aten_add_tensor_17, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:15,557 mps_preprocess.py:115] Visiting: aten__softmax_default_2, aten._softmax.default\n",
      "[INFO 2024-05-29 13:37:15,557 mps_preprocess.py:115] Visiting: aten_expand_copy_default_16, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:15,557 mps_preprocess.py:115] Visiting: aten_view_copy_default_56, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,558 mps_preprocess.py:115] Visiting: aten_bmm_default_5, aten.bmm.default\n",
      "[INFO 2024-05-29 13:37:15,558 mps_preprocess.py:115] Visiting: aten_view_copy_default_58, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,558 mps_preprocess.py:115] Visiting: aten_permute_copy_default_14, aten.permute_copy.default\n",
      "[INFO 2024-05-29 13:37:15,559 mps_preprocess.py:115] Visiting: aten_view_copy_default_59, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:15,559 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_27, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:15,773 mps_preprocess.py:115] Visiting: aten_add_tensor_13, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:15,773 mps_preprocess.py:115] Visiting: aten_mul_tensor_34, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:15,774 mps_preprocess.py:115] Visiting: aten_mean_dim_4, aten.mean.dim\n",
      "[INFO 2024-05-29 13:37:15,774 mps_preprocess.py:115] Visiting: aten_add_tensor_14, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:15,774 mps_preprocess.py:115] Visiting: aten_rsqrt_default_4, aten.rsqrt.default\n",
      "[INFO 2024-05-29 13:37:15,775 mps_preprocess.py:115] Visiting: aten_mul_tensor_35, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:15,775 mps_preprocess.py:115] Visiting: aten_mul_tensor_36, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:15,775 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_20, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:15,775 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_21, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:15,776 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_22, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:15,853 mps_preprocess.py:115] Visiting: aten_sigmoid_default_1, aten.sigmoid.default\n",
      "[INFO 2024-05-29 13:37:15,853 mps_preprocess.py:115] Visiting: aten_mul_tensor_32, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:15,854 mps_preprocess.py:115] Visiting: aten_mul_tensor_33, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:15,935 mps_preprocess.py:115] Visiting: aten_add_tensor_11, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:15,936 mps_preprocess.py:115] Visiting: aten_mul_tensor_29, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:15,936 mps_preprocess.py:115] Visiting: aten_mean_dim_3, aten.mean.dim\n",
      "[INFO 2024-05-29 13:37:15,936 mps_preprocess.py:115] Visiting: aten_add_tensor_12, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:15,936 mps_preprocess.py:115] Visiting: aten_rsqrt_default_3, aten.rsqrt.default\n",
      "[INFO 2024-05-29 13:37:15,936 mps_preprocess.py:115] Visiting: aten_mul_tensor_30, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:15,937 mps_preprocess.py:115] Visiting: aten_mul_tensor_31, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:15,937 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_18, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:15,937 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_19, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:16,026 mps_preprocess.py:115] Visiting: aten_view_copy_default_25, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,027 mps_preprocess.py:115] Visiting: aten_view_copy_default_26, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,027 mps_preprocess.py:115] Visiting: aten_view_copy_default_20, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,027 mps_preprocess.py:115] Visiting: aten_view_copy_default_21, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,027 mps_preprocess.py:115] Visiting: aten_view_copy_default_22, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,028 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_12, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:16,028 mps_preprocess.py:115] Visiting: aten_view_copy_default_23, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,028 mps_preprocess.py:115] Visiting: aten_view_copy_default_24, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,028 mps_preprocess.py:115] Visiting: aten_permute_copy_default_7, aten.permute_copy.default\n",
      "[INFO 2024-05-29 13:37:16,028 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_13, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:16,028 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_4, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 13:37:16,029 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_5, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 13:37:16,029 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_6, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 13:37:16,029 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_7, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 13:37:16,029 mps_preprocess.py:115] Visiting: aten_view_copy_default_30, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,029 mps_preprocess.py:115] Visiting: aten_index_tensor_3, aten.index.Tensor\n",
      "[INFO 2024-05-29 13:37:16,029 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_13, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:16,030 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_14, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:16,030 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_15, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:16,030 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_16, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:16,030 mps_preprocess.py:115] Visiting: aten_index_put_default_3, aten.index_put.default\n",
      "[INFO 2024-05-29 13:37:16,030 mps_preprocess.py:115] Visiting: aten__to_copy_default_1, aten._to_copy.default\n",
      "[INFO 2024-05-29 13:37:16,030 mps_preprocess.py:115] Visiting: aten_mul_tensor_20, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:16,031 mps_preprocess.py:115] Visiting: aten_mul_tensor_22, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:16,031 mps_preprocess.py:115] Visiting: aten_mul_tensor_21, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:16,031 mps_preprocess.py:115] Visiting: aten_mul_tensor_23, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:16,031 mps_preprocess.py:115] Visiting: aten_mul_tensor_24, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:16,031 mps_preprocess.py:115] Visiting: aten_mul_tensor_26, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:16,031 mps_preprocess.py:115] Visiting: aten_mul_tensor_25, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:16,032 mps_preprocess.py:115] Visiting: aten_mul_tensor_27, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:16,032 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_6, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 13:37:16,032 mps_preprocess.py:115] Visiting: aten_sub_tensor_2, aten.sub.Tensor\n",
      "[INFO 2024-05-29 13:37:16,033 mps_preprocess.py:115] Visiting: aten_add_tensor_8, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:16,033 mps_preprocess.py:115] Visiting: aten_sub_tensor_3, aten.sub.Tensor\n",
      "[INFO 2024-05-29 13:37:16,033 mps_preprocess.py:115] Visiting: aten_add_tensor_9, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:16,033 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_7, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 13:37:16,034 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_8, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:16,034 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_9, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:16,034 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_10, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:16,034 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_11, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:16,035 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_15, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:16,035 mps_preprocess.py:115] Visiting: aten_cat_default_2, aten.cat.default\n",
      "[INFO 2024-05-29 13:37:16,035 mps_preprocess.py:115] Visiting: aten_cat_default_3, aten.cat.default\n",
      "[INFO 2024-05-29 13:37:16,035 mps_preprocess.py:115] Visiting: aten_expand_copy_default_7, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:16,035 mps_preprocess.py:115] Visiting: aten_view_copy_default_27, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,036 mps_preprocess.py:115] Visiting: aten_view_copy_default_28, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,036 mps_preprocess.py:115] Visiting: aten_clone_default_3, aten.clone.default\n",
      "[INFO 2024-05-29 13:37:16,036 mps_preprocess.py:115] Visiting: aten_permute_copy_default_5, aten.permute_copy.default\n",
      "[INFO 2024-05-29 13:37:16,036 mps_preprocess.py:115] Visiting: aten_permute_copy_default_6, aten.permute_copy.default\n",
      "[INFO 2024-05-29 13:37:16,037 mps_preprocess.py:115] Visiting: aten_view_copy_default_32, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,037 mps_preprocess.py:115] Visiting: aten_expand_copy_default_8, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:16,037 mps_preprocess.py:115] Visiting: aten_view_copy_default_29, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,037 mps_preprocess.py:115] Visiting: aten_expand_copy_default_11, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:16,037 mps_preprocess.py:115] Visiting: aten_view_copy_default_33, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,037 mps_preprocess.py:115] Visiting: aten_index_put_default_2, aten.index_put.default\n",
      "[INFO 2024-05-29 13:37:16,038 mps_preprocess.py:115] Visiting: aten_view_copy_default_37, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,038 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_4, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 13:37:16,039 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_5, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 13:37:16,039 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_14, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:16,039 mps_preprocess.py:115] Visiting: aten_expand_copy_default_6, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:16,040 mps_preprocess.py:115] Visiting: aten_clone_default_2, aten.clone.default\n",
      "[INFO 2024-05-29 13:37:16,040 mps_preprocess.py:115] Visiting: aten_view_copy_default_31, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,040 mps_preprocess.py:115] Visiting: aten_permute_copy_default_8, aten.permute_copy.default\n",
      "[INFO 2024-05-29 13:37:16,040 mps_preprocess.py:115] Visiting: aten_expand_copy_default_9, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:16,041 mps_preprocess.py:115] Visiting: aten_view_copy_default_34, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,041 mps_preprocess.py:115] Visiting: aten_bmm_default_2, aten.bmm.default\n",
      "[INFO 2024-05-29 13:37:16,041 mps_preprocess.py:115] Visiting: aten_view_copy_default_35, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,041 mps_preprocess.py:115] Visiting: aten_mul_tensor_28, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:16,042 mps_preprocess.py:115] Visiting: aten_add_tensor_10, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:16,042 mps_preprocess.py:115] Visiting: aten__softmax_default_1, aten._softmax.default\n",
      "[INFO 2024-05-29 13:37:16,042 mps_preprocess.py:115] Visiting: aten_expand_copy_default_10, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:16,043 mps_preprocess.py:115] Visiting: aten_view_copy_default_36, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,043 mps_preprocess.py:115] Visiting: aten_bmm_default_3, aten.bmm.default\n",
      "[INFO 2024-05-29 13:37:16,043 mps_preprocess.py:115] Visiting: aten_view_copy_default_38, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,043 mps_preprocess.py:115] Visiting: aten_permute_copy_default_9, aten.permute_copy.default\n",
      "[INFO 2024-05-29 13:37:16,044 mps_preprocess.py:115] Visiting: aten_view_copy_default_39, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,044 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_17, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:16,263 mps_preprocess.py:115] Visiting: aten_add_tensor_6, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:16,263 mps_preprocess.py:115] Visiting: aten_mul_tensor_17, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:16,264 mps_preprocess.py:115] Visiting: aten_mean_dim_2, aten.mean.dim\n",
      "[INFO 2024-05-29 13:37:16,264 mps_preprocess.py:115] Visiting: aten_add_tensor_7, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:16,264 mps_preprocess.py:115] Visiting: aten_rsqrt_default_2, aten.rsqrt.default\n",
      "[INFO 2024-05-29 13:37:16,264 mps_preprocess.py:115] Visiting: aten_mul_tensor_18, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:16,264 mps_preprocess.py:115] Visiting: aten_mul_tensor_19, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:16,265 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_10, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:16,265 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_11, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:16,265 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_12, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:16,345 mps_preprocess.py:115] Visiting: aten_sigmoid_default, aten.sigmoid.default\n",
      "[INFO 2024-05-29 13:37:16,346 mps_preprocess.py:115] Visiting: aten_mul_tensor_15, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:16,346 mps_preprocess.py:115] Visiting: aten_mul_tensor_16, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:16,419 mps_preprocess.py:115] Visiting: aten_add_tensor_4, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:16,420 mps_preprocess.py:115] Visiting: aten_mul_tensor_12, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:16,420 mps_preprocess.py:115] Visiting: aten_mean_dim_1, aten.mean.dim\n",
      "[INFO 2024-05-29 13:37:16,420 mps_preprocess.py:115] Visiting: aten_add_tensor_5, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:16,420 mps_preprocess.py:115] Visiting: aten_rsqrt_default_1, aten.rsqrt.default\n",
      "[INFO 2024-05-29 13:37:16,421 mps_preprocess.py:115] Visiting: aten_mul_tensor_13, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:16,421 mps_preprocess.py:115] Visiting: aten_mul_tensor_14, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:16,421 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_8, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:16,421 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_9, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:16,507 mps_preprocess.py:115] Visiting: aten_view_copy_default, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,508 mps_preprocess.py:115] Visiting: aten_view_copy_default_1, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,508 mps_preprocess.py:115] Visiting: aten_view_copy_default_2, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,508 mps_preprocess.py:115] Visiting: aten_index_tensor, aten.index.Tensor\n",
      "[INFO 2024-05-29 13:37:16,509 mps_preprocess.py:115] Visiting: aten_index_tensor_1, aten.index.Tensor\n",
      "[INFO 2024-05-29 13:37:16,509 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_4, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:16,509 mps_preprocess.py:115] Visiting: aten_view_copy_default_3, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,510 mps_preprocess.py:115] Visiting: aten_view_copy_default_4, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,510 mps_preprocess.py:115] Visiting: aten_permute_copy_default_2, aten.permute_copy.default\n",
      "[INFO 2024-05-29 13:37:16,510 mps_preprocess.py:115] Visiting: aten_view_copy_default_5, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,511 mps_preprocess.py:115] Visiting: aten_view_copy_default_6, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,511 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_5, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:16,511 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 13:37:16,511 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_1, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 13:37:16,511 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_2, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 13:37:16,512 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_3, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 13:37:16,512 mps_preprocess.py:115] Visiting: aten_view_copy_default_10, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,512 mps_preprocess.py:115] Visiting: aten_index_tensor_2, aten.index.Tensor\n",
      "[INFO 2024-05-29 13:37:16,512 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_3, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:16,512 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_4, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:16,513 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_5, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:16,513 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_6, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:16,513 mps_preprocess.py:115] Visiting: aten_index_put_default_1, aten.index_put.default\n",
      "[INFO 2024-05-29 13:37:16,513 mps_preprocess.py:115] Visiting: aten__to_copy_default, aten._to_copy.default\n",
      "[INFO 2024-05-29 13:37:16,513 mps_preprocess.py:115] Visiting: aten_mul_tensor_3, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:16,513 mps_preprocess.py:115] Visiting: aten_mul_tensor_5, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:16,514 mps_preprocess.py:115] Visiting: aten_mul_tensor_4, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:16,514 mps_preprocess.py:115] Visiting: aten_mul_tensor_6, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:16,514 mps_preprocess.py:115] Visiting: aten_mul_tensor_7, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:16,514 mps_preprocess.py:115] Visiting: aten_mul_tensor_9, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:16,514 mps_preprocess.py:115] Visiting: aten_mul_tensor_8, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:16,515 mps_preprocess.py:115] Visiting: aten_mul_tensor_10, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:16,515 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_2, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 13:37:16,515 mps_preprocess.py:115] Visiting: aten_sub_tensor, aten.sub.Tensor\n",
      "[INFO 2024-05-29 13:37:16,516 mps_preprocess.py:115] Visiting: aten_add_tensor_1, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:16,516 mps_preprocess.py:115] Visiting: aten_sub_tensor_1, aten.sub.Tensor\n",
      "[INFO 2024-05-29 13:37:16,516 mps_preprocess.py:115] Visiting: aten_add_tensor_2, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:16,516 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_3, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 13:37:16,517 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:16,517 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_1, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:16,517 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_2, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:16,517 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_3, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:16,517 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_7, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:16,518 mps_preprocess.py:115] Visiting: aten_cat_default, aten.cat.default\n",
      "[INFO 2024-05-29 13:37:16,518 mps_preprocess.py:115] Visiting: aten_cat_default_1, aten.cat.default\n",
      "[INFO 2024-05-29 13:37:16,518 mps_preprocess.py:115] Visiting: aten_expand_copy_default_1, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:16,518 mps_preprocess.py:115] Visiting: aten_view_copy_default_7, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,518 mps_preprocess.py:115] Visiting: aten_view_copy_default_8, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,519 mps_preprocess.py:115] Visiting: aten_clone_default_1, aten.clone.default\n",
      "[INFO 2024-05-29 13:37:16,519 mps_preprocess.py:115] Visiting: aten_permute_copy_default, aten.permute_copy.default\n",
      "[INFO 2024-05-29 13:37:16,519 mps_preprocess.py:115] Visiting: aten_permute_copy_default_1, aten.permute_copy.default\n",
      "[INFO 2024-05-29 13:37:16,519 mps_preprocess.py:115] Visiting: aten_view_copy_default_12, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,520 mps_preprocess.py:115] Visiting: aten_expand_copy_default_2, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:16,520 mps_preprocess.py:115] Visiting: aten_view_copy_default_9, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,520 mps_preprocess.py:115] Visiting: aten_expand_copy_default_5, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:16,520 mps_preprocess.py:115] Visiting: aten_view_copy_default_13, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,521 mps_preprocess.py:115] Visiting: aten_index_put_default, aten.index_put.default\n",
      "[INFO 2024-05-29 13:37:16,521 mps_preprocess.py:115] Visiting: aten_view_copy_default_17, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,521 mps_preprocess.py:115] Visiting: aten_slice_scatter_default, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 13:37:16,521 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_1, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 13:37:16,522 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_6, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 13:37:16,522 mps_preprocess.py:115] Visiting: aten_expand_copy_default, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:16,522 mps_preprocess.py:115] Visiting: aten_clone_default, aten.clone.default\n",
      "[INFO 2024-05-29 13:37:16,523 mps_preprocess.py:115] Visiting: aten_view_copy_default_11, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,523 mps_preprocess.py:115] Visiting: aten_permute_copy_default_3, aten.permute_copy.default\n",
      "[INFO 2024-05-29 13:37:16,524 mps_preprocess.py:115] Visiting: aten_expand_copy_default_3, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:16,524 mps_preprocess.py:115] Visiting: aten_view_copy_default_14, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,524 mps_preprocess.py:115] Visiting: aten_bmm_default, aten.bmm.default\n",
      "[INFO 2024-05-29 13:37:16,525 mps_preprocess.py:115] Visiting: aten_view_copy_default_15, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,525 mps_preprocess.py:115] Visiting: aten_mul_tensor_11, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:16,525 mps_preprocess.py:115] Visiting: aten_add_tensor_3, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:16,526 mps_preprocess.py:115] Visiting: aten__softmax_default, aten._softmax.default\n",
      "[INFO 2024-05-29 13:37:16,526 mps_preprocess.py:115] Visiting: aten_expand_copy_default_4, aten.expand_copy.default\n",
      "[INFO 2024-05-29 13:37:16,526 mps_preprocess.py:115] Visiting: aten_view_copy_default_16, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,526 mps_preprocess.py:115] Visiting: aten_bmm_default_1, aten.bmm.default\n",
      "[INFO 2024-05-29 13:37:16,527 mps_preprocess.py:115] Visiting: aten_view_copy_default_18, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,527 mps_preprocess.py:115] Visiting: aten_permute_copy_default_4, aten.permute_copy.default\n",
      "[INFO 2024-05-29 13:37:16,527 mps_preprocess.py:115] Visiting: aten_view_copy_default_19, aten.view_copy.default\n",
      "[INFO 2024-05-29 13:37:16,527 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_7, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:16,744 mps_preprocess.py:115] Visiting: aten_embedding_default, aten.embedding.default\n",
      "[INFO 2024-05-29 13:37:16,745 mps_preprocess.py:115] Visiting: aten_mul_tensor, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:16,745 mps_preprocess.py:115] Visiting: aten_mean_dim, aten.mean.dim\n",
      "[INFO 2024-05-29 13:37:16,746 mps_preprocess.py:115] Visiting: aten_add_tensor, aten.add.Tensor\n",
      "[INFO 2024-05-29 13:37:16,746 mps_preprocess.py:115] Visiting: aten_rsqrt_default, aten.rsqrt.default\n",
      "[INFO 2024-05-29 13:37:16,746 mps_preprocess.py:115] Visiting: aten_mul_tensor_1, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:16,746 mps_preprocess.py:115] Visiting: aten_mul_tensor_2, aten.mul.Tensor\n",
      "[INFO 2024-05-29 13:37:16,747 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:16,747 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_1, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 13:37:16,747 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_2, aten.squeeze_copy.dims\n",
      "/Users/larryliu/miniconda3/envs/executorch/lib/python3.11/site-packages/executorch/exir/emit/_emitter.py:1474: UserWarning: Mutation on a buffer in the model is detected. ExecuTorch assumes buffers that are mutated in the graph have a meaningless initial state, only the shape and dtype will be serialized.\n",
      "  warnings.warn(\n",
      "[INFO 2024-05-29 13:37:20,230 builder.py:375] Required memory for activation in bytes: [0, 3686144]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<executorch.examples.models.llama2.builder.LlamaEdgeManager at 0x32e5d10d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder = model.export_to_edge()\n",
    "builder.edge_manager = builder.edge_manager.transform([ReplaceMMPass()])\n",
    "builder.to_backend(partitioners).to_executorch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2024-05-08 23:48:05,952 xnnpack_partitioner.py:555] Found 85 subgraphs to be partitioned.\n",
      "/Users/larryliu/miniconda3/envs/executorch/lib/python3.11/site-packages/executorch/exir/emit/_emitter.py:1474: UserWarning: Mutation on a buffer in the model is detected. ExecuTorch assumes buffers that are mutated in the graph have a meaningless initial state, only the shape and dtype will be serialized.\n",
      "  warnings.warn(\n",
      "[INFO 2024-05-08 23:48:23,675 builder.py:341] Required memory for activation in bytes: [0, 459548672]\n",
      "[INFO 2024-05-08 23:48:23,852 utils.py:113] Saved exported program to stories110M_int8_xnnpack.pte\n"
     ]
    }
   ],
   "source": [
    "from executorch.examples.models.llama2.lib.partitioner_lib import get_xnnpack_partitioner\n",
    "\n",
    "builder = model.export_to_edge(None).to_backend([get_xnnpack_partitioner()]).to_executorch().save_to_pte(\"stories110M_int8_xnnpack.pte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/larryliu/miniconda3/envs/executorch/lib/python3.11/site-packages/executorch/exir/emit/_emitter.py:1474: UserWarning: Mutation on a buffer in the model is detected. ExecuTorch assumes buffers that are mutated in the graph have a meaningless initial state, only the shape and dtype will be serialized.\n",
      "  warnings.warn(\n",
      "[INFO 2024-05-08 18:05:25,534 builder.py:341] Required memory for activation in bytes: [0, 418116608]\n"
     ]
    }
   ],
   "source": [
    "builder = model.export_to_edge(None).to_executorch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph():\n",
      "    %b_layers_0_attention_wq_weight : [num_users=1] = placeholder[target=b_layers_0_attention_wq_weight]\n",
      "    %b_layers_0_attention_wq_scales : [num_users=1] = placeholder[target=b_layers_0_attention_wq_scales]\n",
      "    %b_layers_0_attention_wk_weight : [num_users=1] = placeholder[target=b_layers_0_attention_wk_weight]\n",
      "    %b_layers_0_attention_wk_scales : [num_users=1] = placeholder[target=b_layers_0_attention_wk_scales]\n",
      "    %b_layers_0_attention_wv_weight : [num_users=1] = placeholder[target=b_layers_0_attention_wv_weight]\n",
      "    %b_layers_0_attention_wv_scales : [num_users=1] = placeholder[target=b_layers_0_attention_wv_scales]\n",
      "    %b_layers_0_attention_sdpa_kv_cache_k_cache : [num_users=2] = placeholder[target=b_layers_0_attention_sdpa_kv_cache_k_cache]\n",
      "    %b_layers_0_attention_sdpa_kv_cache_v_cache : [num_users=2] = placeholder[target=b_layers_0_attention_sdpa_kv_cache_v_cache]\n",
      "    %b_layers_0_attention_wo_weight : [num_users=1] = placeholder[target=b_layers_0_attention_wo_weight]\n",
      "    %b_layers_0_attention_wo_scales : [num_users=1] = placeholder[target=b_layers_0_attention_wo_scales]\n",
      "    %b_layers_0_feed_forward_w1_weight : [num_users=1] = placeholder[target=b_layers_0_feed_forward_w1_weight]\n",
      "    %b_layers_0_feed_forward_w1_scales : [num_users=1] = placeholder[target=b_layers_0_feed_forward_w1_scales]\n",
      "    %b_layers_0_feed_forward_w3_weight : [num_users=1] = placeholder[target=b_layers_0_feed_forward_w3_weight]\n",
      "    %b_layers_0_feed_forward_w3_scales : [num_users=1] = placeholder[target=b_layers_0_feed_forward_w3_scales]\n",
      "    %b_layers_0_feed_forward_w2_weight : [num_users=1] = placeholder[target=b_layers_0_feed_forward_w2_weight]\n",
      "    %b_layers_0_feed_forward_w2_scales : [num_users=1] = placeholder[target=b_layers_0_feed_forward_w2_scales]\n",
      "    %b_layers_1_attention_wq_weight : [num_users=1] = placeholder[target=b_layers_1_attention_wq_weight]\n",
      "    %b_layers_1_attention_wq_scales : [num_users=1] = placeholder[target=b_layers_1_attention_wq_scales]\n",
      "    %b_layers_1_attention_wk_weight : [num_users=1] = placeholder[target=b_layers_1_attention_wk_weight]\n",
      "    %b_layers_1_attention_wk_scales : [num_users=1] = placeholder[target=b_layers_1_attention_wk_scales]\n",
      "    %b_layers_1_attention_wv_weight : [num_users=1] = placeholder[target=b_layers_1_attention_wv_weight]\n",
      "    %b_layers_1_attention_wv_scales : [num_users=1] = placeholder[target=b_layers_1_attention_wv_scales]\n",
      "    %b_layers_1_attention_sdpa_kv_cache_k_cache : [num_users=2] = placeholder[target=b_layers_1_attention_sdpa_kv_cache_k_cache]\n",
      "    %b_layers_1_attention_sdpa_kv_cache_v_cache : [num_users=2] = placeholder[target=b_layers_1_attention_sdpa_kv_cache_v_cache]\n",
      "    %b_layers_1_attention_wo_weight : [num_users=1] = placeholder[target=b_layers_1_attention_wo_weight]\n",
      "    %b_layers_1_attention_wo_scales : [num_users=1] = placeholder[target=b_layers_1_attention_wo_scales]\n",
      "    %b_layers_1_feed_forward_w1_weight : [num_users=1] = placeholder[target=b_layers_1_feed_forward_w1_weight]\n",
      "    %b_layers_1_feed_forward_w1_scales : [num_users=1] = placeholder[target=b_layers_1_feed_forward_w1_scales]\n",
      "    %b_layers_1_feed_forward_w3_weight : [num_users=1] = placeholder[target=b_layers_1_feed_forward_w3_weight]\n",
      "    %b_layers_1_feed_forward_w3_scales : [num_users=1] = placeholder[target=b_layers_1_feed_forward_w3_scales]\n",
      "    %b_layers_1_feed_forward_w2_weight : [num_users=1] = placeholder[target=b_layers_1_feed_forward_w2_weight]\n",
      "    %b_layers_1_feed_forward_w2_scales : [num_users=1] = placeholder[target=b_layers_1_feed_forward_w2_scales]\n",
      "    %b_layers_2_attention_wq_weight : [num_users=1] = placeholder[target=b_layers_2_attention_wq_weight]\n",
      "    %b_layers_2_attention_wq_scales : [num_users=1] = placeholder[target=b_layers_2_attention_wq_scales]\n",
      "    %b_layers_2_attention_wk_weight : [num_users=1] = placeholder[target=b_layers_2_attention_wk_weight]\n",
      "    %b_layers_2_attention_wk_scales : [num_users=1] = placeholder[target=b_layers_2_attention_wk_scales]\n",
      "    %b_layers_2_attention_wv_weight : [num_users=1] = placeholder[target=b_layers_2_attention_wv_weight]\n",
      "    %b_layers_2_attention_wv_scales : [num_users=1] = placeholder[target=b_layers_2_attention_wv_scales]\n",
      "    %b_layers_2_attention_sdpa_kv_cache_k_cache : [num_users=2] = placeholder[target=b_layers_2_attention_sdpa_kv_cache_k_cache]\n",
      "    %b_layers_2_attention_sdpa_kv_cache_v_cache : [num_users=2] = placeholder[target=b_layers_2_attention_sdpa_kv_cache_v_cache]\n",
      "    %b_layers_2_attention_wo_weight : [num_users=1] = placeholder[target=b_layers_2_attention_wo_weight]\n",
      "    %b_layers_2_attention_wo_scales : [num_users=1] = placeholder[target=b_layers_2_attention_wo_scales]\n",
      "    %b_layers_2_feed_forward_w1_weight : [num_users=1] = placeholder[target=b_layers_2_feed_forward_w1_weight]\n",
      "    %b_layers_2_feed_forward_w1_scales : [num_users=1] = placeholder[target=b_layers_2_feed_forward_w1_scales]\n",
      "    %b_layers_2_feed_forward_w3_weight : [num_users=1] = placeholder[target=b_layers_2_feed_forward_w3_weight]\n",
      "    %b_layers_2_feed_forward_w3_scales : [num_users=1] = placeholder[target=b_layers_2_feed_forward_w3_scales]\n",
      "    %b_layers_2_feed_forward_w2_weight : [num_users=1] = placeholder[target=b_layers_2_feed_forward_w2_weight]\n",
      "    %b_layers_2_feed_forward_w2_scales : [num_users=1] = placeholder[target=b_layers_2_feed_forward_w2_scales]\n",
      "    %b_layers_3_attention_wq_weight : [num_users=1] = placeholder[target=b_layers_3_attention_wq_weight]\n",
      "    %b_layers_3_attention_wq_scales : [num_users=1] = placeholder[target=b_layers_3_attention_wq_scales]\n",
      "    %b_layers_3_attention_wk_weight : [num_users=1] = placeholder[target=b_layers_3_attention_wk_weight]\n",
      "    %b_layers_3_attention_wk_scales : [num_users=1] = placeholder[target=b_layers_3_attention_wk_scales]\n",
      "    %b_layers_3_attention_wv_weight : [num_users=1] = placeholder[target=b_layers_3_attention_wv_weight]\n",
      "    %b_layers_3_attention_wv_scales : [num_users=1] = placeholder[target=b_layers_3_attention_wv_scales]\n",
      "    %b_layers_3_attention_sdpa_kv_cache_k_cache : [num_users=2] = placeholder[target=b_layers_3_attention_sdpa_kv_cache_k_cache]\n",
      "    %b_layers_3_attention_sdpa_kv_cache_v_cache : [num_users=2] = placeholder[target=b_layers_3_attention_sdpa_kv_cache_v_cache]\n",
      "    %b_layers_3_attention_wo_weight : [num_users=1] = placeholder[target=b_layers_3_attention_wo_weight]\n",
      "    %b_layers_3_attention_wo_scales : [num_users=1] = placeholder[target=b_layers_3_attention_wo_scales]\n",
      "    %b_layers_3_feed_forward_w1_weight : [num_users=1] = placeholder[target=b_layers_3_feed_forward_w1_weight]\n",
      "    %b_layers_3_feed_forward_w1_scales : [num_users=1] = placeholder[target=b_layers_3_feed_forward_w1_scales]\n",
      "    %b_layers_3_feed_forward_w3_weight : [num_users=1] = placeholder[target=b_layers_3_feed_forward_w3_weight]\n",
      "    %b_layers_3_feed_forward_w3_scales : [num_users=1] = placeholder[target=b_layers_3_feed_forward_w3_scales]\n",
      "    %b_layers_3_feed_forward_w2_weight : [num_users=1] = placeholder[target=b_layers_3_feed_forward_w2_weight]\n",
      "    %b_layers_3_feed_forward_w2_scales : [num_users=1] = placeholder[target=b_layers_3_feed_forward_w2_scales]\n",
      "    %b_layers_4_attention_wq_weight : [num_users=1] = placeholder[target=b_layers_4_attention_wq_weight]\n",
      "    %b_layers_4_attention_wq_scales : [num_users=1] = placeholder[target=b_layers_4_attention_wq_scales]\n",
      "    %b_layers_4_attention_wk_weight : [num_users=1] = placeholder[target=b_layers_4_attention_wk_weight]\n",
      "    %b_layers_4_attention_wk_scales : [num_users=1] = placeholder[target=b_layers_4_attention_wk_scales]\n",
      "    %b_layers_4_attention_wv_weight : [num_users=1] = placeholder[target=b_layers_4_attention_wv_weight]\n",
      "    %b_layers_4_attention_wv_scales : [num_users=1] = placeholder[target=b_layers_4_attention_wv_scales]\n",
      "    %b_layers_4_attention_sdpa_kv_cache_k_cache : [num_users=2] = placeholder[target=b_layers_4_attention_sdpa_kv_cache_k_cache]\n",
      "    %b_layers_4_attention_sdpa_kv_cache_v_cache : [num_users=2] = placeholder[target=b_layers_4_attention_sdpa_kv_cache_v_cache]\n",
      "    %b_layers_4_attention_wo_weight : [num_users=1] = placeholder[target=b_layers_4_attention_wo_weight]\n",
      "    %b_layers_4_attention_wo_scales : [num_users=1] = placeholder[target=b_layers_4_attention_wo_scales]\n",
      "    %b_layers_4_feed_forward_w1_weight : [num_users=1] = placeholder[target=b_layers_4_feed_forward_w1_weight]\n",
      "    %b_layers_4_feed_forward_w1_scales : [num_users=1] = placeholder[target=b_layers_4_feed_forward_w1_scales]\n",
      "    %b_layers_4_feed_forward_w3_weight : [num_users=1] = placeholder[target=b_layers_4_feed_forward_w3_weight]\n",
      "    %b_layers_4_feed_forward_w3_scales : [num_users=1] = placeholder[target=b_layers_4_feed_forward_w3_scales]\n",
      "    %b_layers_4_feed_forward_w2_weight : [num_users=1] = placeholder[target=b_layers_4_feed_forward_w2_weight]\n",
      "    %b_layers_4_feed_forward_w2_scales : [num_users=1] = placeholder[target=b_layers_4_feed_forward_w2_scales]\n",
      "    %b_layers_5_attention_wq_weight : [num_users=1] = placeholder[target=b_layers_5_attention_wq_weight]\n",
      "    %b_layers_5_attention_wq_scales : [num_users=1] = placeholder[target=b_layers_5_attention_wq_scales]\n",
      "    %b_layers_5_attention_wk_weight : [num_users=1] = placeholder[target=b_layers_5_attention_wk_weight]\n",
      "    %b_layers_5_attention_wk_scales : [num_users=1] = placeholder[target=b_layers_5_attention_wk_scales]\n",
      "    %b_layers_5_attention_wv_weight : [num_users=1] = placeholder[target=b_layers_5_attention_wv_weight]\n",
      "    %b_layers_5_attention_wv_scales : [num_users=1] = placeholder[target=b_layers_5_attention_wv_scales]\n",
      "    %b_layers_5_attention_sdpa_kv_cache_k_cache : [num_users=2] = placeholder[target=b_layers_5_attention_sdpa_kv_cache_k_cache]\n",
      "    %b_layers_5_attention_sdpa_kv_cache_v_cache : [num_users=2] = placeholder[target=b_layers_5_attention_sdpa_kv_cache_v_cache]\n",
      "    %b_layers_5_attention_wo_weight : [num_users=1] = placeholder[target=b_layers_5_attention_wo_weight]\n",
      "    %b_layers_5_attention_wo_scales : [num_users=1] = placeholder[target=b_layers_5_attention_wo_scales]\n",
      "    %b_layers_5_feed_forward_w1_weight : [num_users=1] = placeholder[target=b_layers_5_feed_forward_w1_weight]\n",
      "    %b_layers_5_feed_forward_w1_scales : [num_users=1] = placeholder[target=b_layers_5_feed_forward_w1_scales]\n",
      "    %b_layers_5_feed_forward_w3_weight : [num_users=1] = placeholder[target=b_layers_5_feed_forward_w3_weight]\n",
      "    %b_layers_5_feed_forward_w3_scales : [num_users=1] = placeholder[target=b_layers_5_feed_forward_w3_scales]\n",
      "    %b_layers_5_feed_forward_w2_weight : [num_users=1] = placeholder[target=b_layers_5_feed_forward_w2_weight]\n",
      "    %b_layers_5_feed_forward_w2_scales : [num_users=1] = placeholder[target=b_layers_5_feed_forward_w2_scales]\n",
      "    %b_layers_6_attention_wq_weight : [num_users=1] = placeholder[target=b_layers_6_attention_wq_weight]\n",
      "    %b_layers_6_attention_wq_scales : [num_users=1] = placeholder[target=b_layers_6_attention_wq_scales]\n",
      "    %b_layers_6_attention_wk_weight : [num_users=1] = placeholder[target=b_layers_6_attention_wk_weight]\n",
      "    %b_layers_6_attention_wk_scales : [num_users=1] = placeholder[target=b_layers_6_attention_wk_scales]\n",
      "    %b_layers_6_attention_wv_weight : [num_users=1] = placeholder[target=b_layers_6_attention_wv_weight]\n",
      "    %b_layers_6_attention_wv_scales : [num_users=1] = placeholder[target=b_layers_6_attention_wv_scales]\n",
      "    %b_layers_6_attention_sdpa_kv_cache_k_cache : [num_users=2] = placeholder[target=b_layers_6_attention_sdpa_kv_cache_k_cache]\n",
      "    %b_layers_6_attention_sdpa_kv_cache_v_cache : [num_users=2] = placeholder[target=b_layers_6_attention_sdpa_kv_cache_v_cache]\n",
      "    %b_layers_6_attention_wo_weight : [num_users=1] = placeholder[target=b_layers_6_attention_wo_weight]\n",
      "    %b_layers_6_attention_wo_scales : [num_users=1] = placeholder[target=b_layers_6_attention_wo_scales]\n",
      "    %b_layers_6_feed_forward_w1_weight : [num_users=1] = placeholder[target=b_layers_6_feed_forward_w1_weight]\n",
      "    %b_layers_6_feed_forward_w1_scales : [num_users=1] = placeholder[target=b_layers_6_feed_forward_w1_scales]\n",
      "    %b_layers_6_feed_forward_w3_weight : [num_users=1] = placeholder[target=b_layers_6_feed_forward_w3_weight]\n",
      "    %b_layers_6_feed_forward_w3_scales : [num_users=1] = placeholder[target=b_layers_6_feed_forward_w3_scales]\n",
      "    %b_layers_6_feed_forward_w2_weight : [num_users=1] = placeholder[target=b_layers_6_feed_forward_w2_weight]\n",
      "    %b_layers_6_feed_forward_w2_scales : [num_users=1] = placeholder[target=b_layers_6_feed_forward_w2_scales]\n",
      "    %b_layers_7_attention_wq_weight : [num_users=1] = placeholder[target=b_layers_7_attention_wq_weight]\n",
      "    %b_layers_7_attention_wq_scales : [num_users=1] = placeholder[target=b_layers_7_attention_wq_scales]\n",
      "    %b_layers_7_attention_wk_weight : [num_users=1] = placeholder[target=b_layers_7_attention_wk_weight]\n",
      "    %b_layers_7_attention_wk_scales : [num_users=1] = placeholder[target=b_layers_7_attention_wk_scales]\n",
      "    %b_layers_7_attention_wv_weight : [num_users=1] = placeholder[target=b_layers_7_attention_wv_weight]\n",
      "    %b_layers_7_attention_wv_scales : [num_users=1] = placeholder[target=b_layers_7_attention_wv_scales]\n",
      "    %b_layers_7_attention_sdpa_kv_cache_k_cache : [num_users=2] = placeholder[target=b_layers_7_attention_sdpa_kv_cache_k_cache]\n",
      "    %b_layers_7_attention_sdpa_kv_cache_v_cache : [num_users=2] = placeholder[target=b_layers_7_attention_sdpa_kv_cache_v_cache]\n",
      "    %b_layers_7_attention_wo_weight : [num_users=1] = placeholder[target=b_layers_7_attention_wo_weight]\n",
      "    %b_layers_7_attention_wo_scales : [num_users=1] = placeholder[target=b_layers_7_attention_wo_scales]\n",
      "    %b_layers_7_feed_forward_w1_weight : [num_users=1] = placeholder[target=b_layers_7_feed_forward_w1_weight]\n",
      "    %b_layers_7_feed_forward_w1_scales : [num_users=1] = placeholder[target=b_layers_7_feed_forward_w1_scales]\n",
      "    %b_layers_7_feed_forward_w3_weight : [num_users=1] = placeholder[target=b_layers_7_feed_forward_w3_weight]\n",
      "    %b_layers_7_feed_forward_w3_scales : [num_users=1] = placeholder[target=b_layers_7_feed_forward_w3_scales]\n",
      "    %b_layers_7_feed_forward_w2_weight : [num_users=1] = placeholder[target=b_layers_7_feed_forward_w2_weight]\n",
      "    %b_layers_7_feed_forward_w2_scales : [num_users=1] = placeholder[target=b_layers_7_feed_forward_w2_scales]\n",
      "    %b_layers_8_attention_wq_weight : [num_users=1] = placeholder[target=b_layers_8_attention_wq_weight]\n",
      "    %b_layers_8_attention_wq_scales : [num_users=1] = placeholder[target=b_layers_8_attention_wq_scales]\n",
      "    %b_layers_8_attention_wk_weight : [num_users=1] = placeholder[target=b_layers_8_attention_wk_weight]\n",
      "    %b_layers_8_attention_wk_scales : [num_users=1] = placeholder[target=b_layers_8_attention_wk_scales]\n",
      "    %b_layers_8_attention_wv_weight : [num_users=1] = placeholder[target=b_layers_8_attention_wv_weight]\n",
      "    %b_layers_8_attention_wv_scales : [num_users=1] = placeholder[target=b_layers_8_attention_wv_scales]\n",
      "    %b_layers_8_attention_sdpa_kv_cache_k_cache : [num_users=2] = placeholder[target=b_layers_8_attention_sdpa_kv_cache_k_cache]\n",
      "    %b_layers_8_attention_sdpa_kv_cache_v_cache : [num_users=2] = placeholder[target=b_layers_8_attention_sdpa_kv_cache_v_cache]\n",
      "    %b_layers_8_attention_wo_weight : [num_users=1] = placeholder[target=b_layers_8_attention_wo_weight]\n",
      "    %b_layers_8_attention_wo_scales : [num_users=1] = placeholder[target=b_layers_8_attention_wo_scales]\n",
      "    %b_layers_8_feed_forward_w1_weight : [num_users=1] = placeholder[target=b_layers_8_feed_forward_w1_weight]\n",
      "    %b_layers_8_feed_forward_w1_scales : [num_users=1] = placeholder[target=b_layers_8_feed_forward_w1_scales]\n",
      "    %b_layers_8_feed_forward_w3_weight : [num_users=1] = placeholder[target=b_layers_8_feed_forward_w3_weight]\n",
      "    %b_layers_8_feed_forward_w3_scales : [num_users=1] = placeholder[target=b_layers_8_feed_forward_w3_scales]\n",
      "    %b_layers_8_feed_forward_w2_weight : [num_users=1] = placeholder[target=b_layers_8_feed_forward_w2_weight]\n",
      "    %b_layers_8_feed_forward_w2_scales : [num_users=1] = placeholder[target=b_layers_8_feed_forward_w2_scales]\n",
      "    %b_layers_9_attention_wq_weight : [num_users=1] = placeholder[target=b_layers_9_attention_wq_weight]\n",
      "    %b_layers_9_attention_wq_scales : [num_users=1] = placeholder[target=b_layers_9_attention_wq_scales]\n",
      "    %b_layers_9_attention_wk_weight : [num_users=1] = placeholder[target=b_layers_9_attention_wk_weight]\n",
      "    %b_layers_9_attention_wk_scales : [num_users=1] = placeholder[target=b_layers_9_attention_wk_scales]\n",
      "    %b_layers_9_attention_wv_weight : [num_users=1] = placeholder[target=b_layers_9_attention_wv_weight]\n",
      "    %b_layers_9_attention_wv_scales : [num_users=1] = placeholder[target=b_layers_9_attention_wv_scales]\n",
      "    %b_layers_9_attention_sdpa_kv_cache_k_cache : [num_users=2] = placeholder[target=b_layers_9_attention_sdpa_kv_cache_k_cache]\n",
      "    %b_layers_9_attention_sdpa_kv_cache_v_cache : [num_users=2] = placeholder[target=b_layers_9_attention_sdpa_kv_cache_v_cache]\n",
      "    %b_layers_9_attention_wo_weight : [num_users=1] = placeholder[target=b_layers_9_attention_wo_weight]\n",
      "    %b_layers_9_attention_wo_scales : [num_users=1] = placeholder[target=b_layers_9_attention_wo_scales]\n",
      "    %b_layers_9_feed_forward_w1_weight : [num_users=1] = placeholder[target=b_layers_9_feed_forward_w1_weight]\n",
      "    %b_layers_9_feed_forward_w1_scales : [num_users=1] = placeholder[target=b_layers_9_feed_forward_w1_scales]\n",
      "    %b_layers_9_feed_forward_w3_weight : [num_users=1] = placeholder[target=b_layers_9_feed_forward_w3_weight]\n",
      "    %b_layers_9_feed_forward_w3_scales : [num_users=1] = placeholder[target=b_layers_9_feed_forward_w3_scales]\n",
      "    %b_layers_9_feed_forward_w2_weight : [num_users=1] = placeholder[target=b_layers_9_feed_forward_w2_weight]\n",
      "    %b_layers_9_feed_forward_w2_scales : [num_users=1] = placeholder[target=b_layers_9_feed_forward_w2_scales]\n",
      "    %b_layers_10_attention_wq_weight : [num_users=1] = placeholder[target=b_layers_10_attention_wq_weight]\n",
      "    %b_layers_10_attention_wq_scales : [num_users=1] = placeholder[target=b_layers_10_attention_wq_scales]\n",
      "    %b_layers_10_attention_wk_weight : [num_users=1] = placeholder[target=b_layers_10_attention_wk_weight]\n",
      "    %b_layers_10_attention_wk_scales : [num_users=1] = placeholder[target=b_layers_10_attention_wk_scales]\n",
      "    %b_layers_10_attention_wv_weight : [num_users=1] = placeholder[target=b_layers_10_attention_wv_weight]\n",
      "    %b_layers_10_attention_wv_scales : [num_users=1] = placeholder[target=b_layers_10_attention_wv_scales]\n",
      "    %b_layers_10_attention_sdpa_kv_cache_k_cache : [num_users=2] = placeholder[target=b_layers_10_attention_sdpa_kv_cache_k_cache]\n",
      "    %b_layers_10_attention_sdpa_kv_cache_v_cache : [num_users=2] = placeholder[target=b_layers_10_attention_sdpa_kv_cache_v_cache]\n",
      "    %b_layers_10_attention_wo_weight : [num_users=1] = placeholder[target=b_layers_10_attention_wo_weight]\n",
      "    %b_layers_10_attention_wo_scales : [num_users=1] = placeholder[target=b_layers_10_attention_wo_scales]\n",
      "    %b_layers_10_feed_forward_w1_weight : [num_users=1] = placeholder[target=b_layers_10_feed_forward_w1_weight]\n",
      "    %b_layers_10_feed_forward_w1_scales : [num_users=1] = placeholder[target=b_layers_10_feed_forward_w1_scales]\n",
      "    %b_layers_10_feed_forward_w3_weight : [num_users=1] = placeholder[target=b_layers_10_feed_forward_w3_weight]\n",
      "    %b_layers_10_feed_forward_w3_scales : [num_users=1] = placeholder[target=b_layers_10_feed_forward_w3_scales]\n",
      "    %b_layers_10_feed_forward_w2_weight : [num_users=1] = placeholder[target=b_layers_10_feed_forward_w2_weight]\n",
      "    %b_layers_10_feed_forward_w2_scales : [num_users=1] = placeholder[target=b_layers_10_feed_forward_w2_scales]\n",
      "    %b_layers_11_attention_wq_weight : [num_users=1] = placeholder[target=b_layers_11_attention_wq_weight]\n",
      "    %b_layers_11_attention_wq_scales : [num_users=1] = placeholder[target=b_layers_11_attention_wq_scales]\n",
      "    %b_layers_11_attention_wk_weight : [num_users=1] = placeholder[target=b_layers_11_attention_wk_weight]\n",
      "    %b_layers_11_attention_wk_scales : [num_users=1] = placeholder[target=b_layers_11_attention_wk_scales]\n",
      "    %b_layers_11_attention_wv_weight : [num_users=1] = placeholder[target=b_layers_11_attention_wv_weight]\n",
      "    %b_layers_11_attention_wv_scales : [num_users=1] = placeholder[target=b_layers_11_attention_wv_scales]\n",
      "    %b_layers_11_attention_sdpa_kv_cache_k_cache : [num_users=2] = placeholder[target=b_layers_11_attention_sdpa_kv_cache_k_cache]\n",
      "    %b_layers_11_attention_sdpa_kv_cache_v_cache : [num_users=2] = placeholder[target=b_layers_11_attention_sdpa_kv_cache_v_cache]\n",
      "    %b_layers_11_attention_wo_weight : [num_users=1] = placeholder[target=b_layers_11_attention_wo_weight]\n",
      "    %b_layers_11_attention_wo_scales : [num_users=1] = placeholder[target=b_layers_11_attention_wo_scales]\n",
      "    %b_layers_11_feed_forward_w1_weight : [num_users=1] = placeholder[target=b_layers_11_feed_forward_w1_weight]\n",
      "    %b_layers_11_feed_forward_w1_scales : [num_users=1] = placeholder[target=b_layers_11_feed_forward_w1_scales]\n",
      "    %b_layers_11_feed_forward_w3_weight : [num_users=1] = placeholder[target=b_layers_11_feed_forward_w3_weight]\n",
      "    %b_layers_11_feed_forward_w3_scales : [num_users=1] = placeholder[target=b_layers_11_feed_forward_w3_scales]\n",
      "    %b_layers_11_feed_forward_w2_weight : [num_users=1] = placeholder[target=b_layers_11_feed_forward_w2_weight]\n",
      "    %b_layers_11_feed_forward_w2_scales : [num_users=1] = placeholder[target=b_layers_11_feed_forward_w2_scales]\n",
      "    %b_output_weight : [num_users=1] = placeholder[target=b_output_weight]\n",
      "    %b_output_scales : [num_users=1] = placeholder[target=b_output_scales]\n",
      "    %tokens : [num_users=1] = placeholder[target=tokens]\n",
      "    %input_pos : [num_users=12] = placeholder[target=input_pos]\n",
      "    %lowered_module_0 : [num_users=1] = get_attr[target=lowered_module_0]\n",
      "    %executorch_call_delegate : [num_users=4] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_0, %tokens), kwargs = {})\n",
      "    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate, 0), kwargs = {})\n",
      "    %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate, 1), kwargs = {})\n",
      "    %getitem_2 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate, 2), kwargs = {})\n",
      "    %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate, 3), kwargs = {})\n",
      "    %alloc : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_1, %b_layers_0_attention_wq_weight, %b_layers_0_attention_wq_scales), kwargs = {out: %alloc})\n",
      "    %alloc_1 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_1 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_2, %b_layers_0_attention_wk_weight, %b_layers_0_attention_wk_scales), kwargs = {out: %alloc_1})\n",
      "    %alloc_2 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_2 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_3, %b_layers_0_attention_wv_weight, %b_layers_0_attention_wv_scales), kwargs = {out: %alloc_2})\n",
      "    %lowered_module_1 : [num_users=1] = get_attr[target=lowered_module_1]\n",
      "    %executorch_call_delegate_1 : [num_users=5] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_1, %b_layers_0_attention_sdpa_kv_cache_v_cache, %b_layers_0_attention_sdpa_kv_cache_k_cache, %llama_cpp__weight_int8pack_mm_default, %llama_cpp__weight_int8pack_mm_default_1, %llama_cpp__weight_int8pack_mm_default_2, %input_pos), kwargs = {})\n",
      "    %getitem_4 : [num_users=11] = call_function[target=operator.getitem](args = (%executorch_call_delegate_1, 0), kwargs = {})\n",
      "    %getitem_5 : [num_users=11] = call_function[target=operator.getitem](args = (%executorch_call_delegate_1, 1), kwargs = {})\n",
      "    %getitem_6 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_1, 2), kwargs = {})\n",
      "    %getitem_7 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_1, 3), kwargs = {})\n",
      "    %getitem_8 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_1, 4), kwargs = {})\n",
      "    %alloc_3 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_3 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_8, %b_layers_0_attention_wo_weight, %b_layers_0_attention_wo_scales), kwargs = {out: %alloc_3})\n",
      "    %lowered_module_2 : [num_users=1] = get_attr[target=lowered_module_2]\n",
      "    %executorch_call_delegate_2 : [num_users=3] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_2, %getitem, %llama_cpp__weight_int8pack_mm_default_3), kwargs = {})\n",
      "    %getitem_9 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_2, 0), kwargs = {})\n",
      "    %getitem_10 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_2, 1), kwargs = {})\n",
      "    %getitem_11 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_2, 2), kwargs = {})\n",
      "    %alloc_4 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 2048), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_4 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_10, %b_layers_0_feed_forward_w1_weight, %b_layers_0_feed_forward_w1_scales), kwargs = {out: %alloc_4})\n",
      "    %alloc_5 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 2048), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_5 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_11, %b_layers_0_feed_forward_w3_weight, %b_layers_0_feed_forward_w3_scales), kwargs = {out: %alloc_5})\n",
      "    %lowered_module_3 : [num_users=1] = get_attr[target=lowered_module_3]\n",
      "    %executorch_call_delegate_3 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_3, %llama_cpp__weight_int8pack_mm_default_4, %llama_cpp__weight_int8pack_mm_default_5), kwargs = {})\n",
      "    %getitem_12 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_3, 0), kwargs = {})\n",
      "    %alloc_6 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_6 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_12, %b_layers_0_feed_forward_w2_weight, %b_layers_0_feed_forward_w2_scales), kwargs = {out: %alloc_6})\n",
      "    %lowered_module_4 : [num_users=1] = get_attr[target=lowered_module_4]\n",
      "    %executorch_call_delegate_4 : [num_users=4] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_4, %getitem_9, %llama_cpp__weight_int8pack_mm_default_6), kwargs = {})\n",
      "    %getitem_13 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_4, 0), kwargs = {})\n",
      "    %getitem_14 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_4, 1), kwargs = {})\n",
      "    %getitem_15 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_4, 2), kwargs = {})\n",
      "    %getitem_16 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_4, 3), kwargs = {})\n",
      "    %alloc_7 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_7 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_14, %b_layers_1_attention_wq_weight, %b_layers_1_attention_wq_scales), kwargs = {out: %alloc_7})\n",
      "    %alloc_8 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_8 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_15, %b_layers_1_attention_wk_weight, %b_layers_1_attention_wk_scales), kwargs = {out: %alloc_8})\n",
      "    %alloc_9 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_9 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_16, %b_layers_1_attention_wv_weight, %b_layers_1_attention_wv_scales), kwargs = {out: %alloc_9})\n",
      "    %lowered_module_5 : [num_users=1] = get_attr[target=lowered_module_5]\n",
      "    %executorch_call_delegate_5 : [num_users=3] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_5, %b_layers_1_attention_sdpa_kv_cache_v_cache, %b_layers_1_attention_sdpa_kv_cache_k_cache, %getitem_4, %getitem_5, %llama_cpp__weight_int8pack_mm_default_7, %llama_cpp__weight_int8pack_mm_default_8, %llama_cpp__weight_int8pack_mm_default_9, %input_pos), kwargs = {})\n",
      "    %getitem_17 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_5, 0), kwargs = {})\n",
      "    %getitem_18 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_5, 1), kwargs = {})\n",
      "    %getitem_19 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_5, 2), kwargs = {})\n",
      "    %alloc_10 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_10 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_19, %b_layers_1_attention_wo_weight, %b_layers_1_attention_wo_scales), kwargs = {out: %alloc_10})\n",
      "    %lowered_module_6 : [num_users=1] = get_attr[target=lowered_module_6]\n",
      "    %executorch_call_delegate_6 : [num_users=3] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_6, %getitem_13, %llama_cpp__weight_int8pack_mm_default_10), kwargs = {})\n",
      "    %getitem_20 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_6, 0), kwargs = {})\n",
      "    %getitem_21 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_6, 1), kwargs = {})\n",
      "    %getitem_22 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_6, 2), kwargs = {})\n",
      "    %alloc_11 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 2048), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_11 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_21, %b_layers_1_feed_forward_w1_weight, %b_layers_1_feed_forward_w1_scales), kwargs = {out: %alloc_11})\n",
      "    %alloc_12 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 2048), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_12 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_22, %b_layers_1_feed_forward_w3_weight, %b_layers_1_feed_forward_w3_scales), kwargs = {out: %alloc_12})\n",
      "    %lowered_module_7 : [num_users=1] = get_attr[target=lowered_module_7]\n",
      "    %executorch_call_delegate_7 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_7, %llama_cpp__weight_int8pack_mm_default_11, %llama_cpp__weight_int8pack_mm_default_12), kwargs = {})\n",
      "    %getitem_23 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_7, 0), kwargs = {})\n",
      "    %alloc_13 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_13 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_23, %b_layers_1_feed_forward_w2_weight, %b_layers_1_feed_forward_w2_scales), kwargs = {out: %alloc_13})\n",
      "    %lowered_module_8 : [num_users=1] = get_attr[target=lowered_module_8]\n",
      "    %executorch_call_delegate_8 : [num_users=4] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_8, %getitem_20, %llama_cpp__weight_int8pack_mm_default_13), kwargs = {})\n",
      "    %getitem_24 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_8, 0), kwargs = {})\n",
      "    %getitem_25 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_8, 1), kwargs = {})\n",
      "    %getitem_26 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_8, 2), kwargs = {})\n",
      "    %getitem_27 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_8, 3), kwargs = {})\n",
      "    %alloc_14 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_14 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_25, %b_layers_2_attention_wq_weight, %b_layers_2_attention_wq_scales), kwargs = {out: %alloc_14})\n",
      "    %alloc_15 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_15 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_26, %b_layers_2_attention_wk_weight, %b_layers_2_attention_wk_scales), kwargs = {out: %alloc_15})\n",
      "    %alloc_16 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_16 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_27, %b_layers_2_attention_wv_weight, %b_layers_2_attention_wv_scales), kwargs = {out: %alloc_16})\n",
      "    %lowered_module_9 : [num_users=1] = get_attr[target=lowered_module_9]\n",
      "    %executorch_call_delegate_9 : [num_users=3] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_9, %b_layers_2_attention_sdpa_kv_cache_v_cache, %b_layers_2_attention_sdpa_kv_cache_k_cache, %getitem_4, %getitem_5, %llama_cpp__weight_int8pack_mm_default_14, %llama_cpp__weight_int8pack_mm_default_15, %llama_cpp__weight_int8pack_mm_default_16, %input_pos), kwargs = {})\n",
      "    %getitem_28 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_9, 0), kwargs = {})\n",
      "    %getitem_29 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_9, 1), kwargs = {})\n",
      "    %getitem_30 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_9, 2), kwargs = {})\n",
      "    %alloc_17 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_17 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_30, %b_layers_2_attention_wo_weight, %b_layers_2_attention_wo_scales), kwargs = {out: %alloc_17})\n",
      "    %lowered_module_10 : [num_users=1] = get_attr[target=lowered_module_10]\n",
      "    %executorch_call_delegate_10 : [num_users=3] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_10, %getitem_24, %llama_cpp__weight_int8pack_mm_default_17), kwargs = {})\n",
      "    %getitem_31 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_10, 0), kwargs = {})\n",
      "    %getitem_32 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_10, 1), kwargs = {})\n",
      "    %getitem_33 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_10, 2), kwargs = {})\n",
      "    %alloc_18 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 2048), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_18 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_32, %b_layers_2_feed_forward_w1_weight, %b_layers_2_feed_forward_w1_scales), kwargs = {out: %alloc_18})\n",
      "    %alloc_19 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 2048), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_19 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_33, %b_layers_2_feed_forward_w3_weight, %b_layers_2_feed_forward_w3_scales), kwargs = {out: %alloc_19})\n",
      "    %lowered_module_11 : [num_users=1] = get_attr[target=lowered_module_11]\n",
      "    %executorch_call_delegate_11 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_11, %llama_cpp__weight_int8pack_mm_default_18, %llama_cpp__weight_int8pack_mm_default_19), kwargs = {})\n",
      "    %getitem_34 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_11, 0), kwargs = {})\n",
      "    %alloc_20 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_20 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_34, %b_layers_2_feed_forward_w2_weight, %b_layers_2_feed_forward_w2_scales), kwargs = {out: %alloc_20})\n",
      "    %lowered_module_12 : [num_users=1] = get_attr[target=lowered_module_12]\n",
      "    %executorch_call_delegate_12 : [num_users=4] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_12, %getitem_31, %llama_cpp__weight_int8pack_mm_default_20), kwargs = {})\n",
      "    %getitem_35 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_12, 0), kwargs = {})\n",
      "    %getitem_36 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_12, 1), kwargs = {})\n",
      "    %getitem_37 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_12, 2), kwargs = {})\n",
      "    %getitem_38 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_12, 3), kwargs = {})\n",
      "    %alloc_21 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_21 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_36, %b_layers_3_attention_wq_weight, %b_layers_3_attention_wq_scales), kwargs = {out: %alloc_21})\n",
      "    %alloc_22 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_22 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_37, %b_layers_3_attention_wk_weight, %b_layers_3_attention_wk_scales), kwargs = {out: %alloc_22})\n",
      "    %alloc_23 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_23 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_38, %b_layers_3_attention_wv_weight, %b_layers_3_attention_wv_scales), kwargs = {out: %alloc_23})\n",
      "    %lowered_module_13 : [num_users=1] = get_attr[target=lowered_module_13]\n",
      "    %executorch_call_delegate_13 : [num_users=3] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_13, %b_layers_3_attention_sdpa_kv_cache_v_cache, %b_layers_3_attention_sdpa_kv_cache_k_cache, %getitem_4, %getitem_5, %llama_cpp__weight_int8pack_mm_default_21, %llama_cpp__weight_int8pack_mm_default_22, %llama_cpp__weight_int8pack_mm_default_23, %input_pos), kwargs = {})\n",
      "    %getitem_39 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_13, 0), kwargs = {})\n",
      "    %getitem_40 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_13, 1), kwargs = {})\n",
      "    %getitem_41 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_13, 2), kwargs = {})\n",
      "    %alloc_24 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_24 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_41, %b_layers_3_attention_wo_weight, %b_layers_3_attention_wo_scales), kwargs = {out: %alloc_24})\n",
      "    %lowered_module_14 : [num_users=1] = get_attr[target=lowered_module_14]\n",
      "    %executorch_call_delegate_14 : [num_users=3] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_14, %getitem_35, %llama_cpp__weight_int8pack_mm_default_24), kwargs = {})\n",
      "    %getitem_42 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_14, 0), kwargs = {})\n",
      "    %getitem_43 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_14, 1), kwargs = {})\n",
      "    %getitem_44 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_14, 2), kwargs = {})\n",
      "    %alloc_25 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 2048), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_25 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_43, %b_layers_3_feed_forward_w1_weight, %b_layers_3_feed_forward_w1_scales), kwargs = {out: %alloc_25})\n",
      "    %alloc_26 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 2048), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_26 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_44, %b_layers_3_feed_forward_w3_weight, %b_layers_3_feed_forward_w3_scales), kwargs = {out: %alloc_26})\n",
      "    %lowered_module_15 : [num_users=1] = get_attr[target=lowered_module_15]\n",
      "    %executorch_call_delegate_15 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_15, %llama_cpp__weight_int8pack_mm_default_25, %llama_cpp__weight_int8pack_mm_default_26), kwargs = {})\n",
      "    %getitem_45 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_15, 0), kwargs = {})\n",
      "    %alloc_27 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_27 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_45, %b_layers_3_feed_forward_w2_weight, %b_layers_3_feed_forward_w2_scales), kwargs = {out: %alloc_27})\n",
      "    %lowered_module_16 : [num_users=1] = get_attr[target=lowered_module_16]\n",
      "    %executorch_call_delegate_16 : [num_users=4] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_16, %getitem_42, %llama_cpp__weight_int8pack_mm_default_27), kwargs = {})\n",
      "    %getitem_46 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_16, 0), kwargs = {})\n",
      "    %getitem_47 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_16, 1), kwargs = {})\n",
      "    %getitem_48 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_16, 2), kwargs = {})\n",
      "    %getitem_49 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_16, 3), kwargs = {})\n",
      "    %alloc_28 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_28 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_47, %b_layers_4_attention_wq_weight, %b_layers_4_attention_wq_scales), kwargs = {out: %alloc_28})\n",
      "    %alloc_29 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_29 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_48, %b_layers_4_attention_wk_weight, %b_layers_4_attention_wk_scales), kwargs = {out: %alloc_29})\n",
      "    %alloc_30 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_30 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_49, %b_layers_4_attention_wv_weight, %b_layers_4_attention_wv_scales), kwargs = {out: %alloc_30})\n",
      "    %lowered_module_17 : [num_users=1] = get_attr[target=lowered_module_17]\n",
      "    %executorch_call_delegate_17 : [num_users=3] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_17, %b_layers_4_attention_sdpa_kv_cache_v_cache, %b_layers_4_attention_sdpa_kv_cache_k_cache, %getitem_4, %getitem_5, %llama_cpp__weight_int8pack_mm_default_28, %llama_cpp__weight_int8pack_mm_default_29, %llama_cpp__weight_int8pack_mm_default_30, %input_pos), kwargs = {})\n",
      "    %getitem_50 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_17, 0), kwargs = {})\n",
      "    %getitem_51 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_17, 1), kwargs = {})\n",
      "    %getitem_52 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_17, 2), kwargs = {})\n",
      "    %alloc_31 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_31 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_52, %b_layers_4_attention_wo_weight, %b_layers_4_attention_wo_scales), kwargs = {out: %alloc_31})\n",
      "    %lowered_module_18 : [num_users=1] = get_attr[target=lowered_module_18]\n",
      "    %executorch_call_delegate_18 : [num_users=3] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_18, %getitem_46, %llama_cpp__weight_int8pack_mm_default_31), kwargs = {})\n",
      "    %getitem_53 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_18, 0), kwargs = {})\n",
      "    %getitem_54 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_18, 1), kwargs = {})\n",
      "    %getitem_55 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_18, 2), kwargs = {})\n",
      "    %alloc_32 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 2048), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_32 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_54, %b_layers_4_feed_forward_w1_weight, %b_layers_4_feed_forward_w1_scales), kwargs = {out: %alloc_32})\n",
      "    %alloc_33 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 2048), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_33 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_55, %b_layers_4_feed_forward_w3_weight, %b_layers_4_feed_forward_w3_scales), kwargs = {out: %alloc_33})\n",
      "    %lowered_module_19 : [num_users=1] = get_attr[target=lowered_module_19]\n",
      "    %executorch_call_delegate_19 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_19, %llama_cpp__weight_int8pack_mm_default_32, %llama_cpp__weight_int8pack_mm_default_33), kwargs = {})\n",
      "    %getitem_56 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_19, 0), kwargs = {})\n",
      "    %alloc_34 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_34 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_56, %b_layers_4_feed_forward_w2_weight, %b_layers_4_feed_forward_w2_scales), kwargs = {out: %alloc_34})\n",
      "    %lowered_module_20 : [num_users=1] = get_attr[target=lowered_module_20]\n",
      "    %executorch_call_delegate_20 : [num_users=4] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_20, %getitem_53, %llama_cpp__weight_int8pack_mm_default_34), kwargs = {})\n",
      "    %getitem_57 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_20, 0), kwargs = {})\n",
      "    %getitem_58 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_20, 1), kwargs = {})\n",
      "    %getitem_59 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_20, 2), kwargs = {})\n",
      "    %getitem_60 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_20, 3), kwargs = {})\n",
      "    %alloc_35 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_35 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_58, %b_layers_5_attention_wq_weight, %b_layers_5_attention_wq_scales), kwargs = {out: %alloc_35})\n",
      "    %alloc_36 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_36 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_59, %b_layers_5_attention_wk_weight, %b_layers_5_attention_wk_scales), kwargs = {out: %alloc_36})\n",
      "    %alloc_37 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_37 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_60, %b_layers_5_attention_wv_weight, %b_layers_5_attention_wv_scales), kwargs = {out: %alloc_37})\n",
      "    %lowered_module_21 : [num_users=1] = get_attr[target=lowered_module_21]\n",
      "    %executorch_call_delegate_21 : [num_users=3] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_21, %b_layers_5_attention_sdpa_kv_cache_v_cache, %b_layers_5_attention_sdpa_kv_cache_k_cache, %getitem_4, %getitem_5, %llama_cpp__weight_int8pack_mm_default_35, %llama_cpp__weight_int8pack_mm_default_36, %llama_cpp__weight_int8pack_mm_default_37, %input_pos), kwargs = {})\n",
      "    %getitem_61 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_21, 0), kwargs = {})\n",
      "    %getitem_62 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_21, 1), kwargs = {})\n",
      "    %getitem_63 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_21, 2), kwargs = {})\n",
      "    %alloc_38 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_38 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_63, %b_layers_5_attention_wo_weight, %b_layers_5_attention_wo_scales), kwargs = {out: %alloc_38})\n",
      "    %lowered_module_22 : [num_users=1] = get_attr[target=lowered_module_22]\n",
      "    %executorch_call_delegate_22 : [num_users=3] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_22, %getitem_57, %llama_cpp__weight_int8pack_mm_default_38), kwargs = {})\n",
      "    %getitem_64 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_22, 0), kwargs = {})\n",
      "    %getitem_65 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_22, 1), kwargs = {})\n",
      "    %getitem_66 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_22, 2), kwargs = {})\n",
      "    %alloc_39 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 2048), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_39 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_65, %b_layers_5_feed_forward_w1_weight, %b_layers_5_feed_forward_w1_scales), kwargs = {out: %alloc_39})\n",
      "    %alloc_40 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 2048), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_40 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_66, %b_layers_5_feed_forward_w3_weight, %b_layers_5_feed_forward_w3_scales), kwargs = {out: %alloc_40})\n",
      "    %lowered_module_23 : [num_users=1] = get_attr[target=lowered_module_23]\n",
      "    %executorch_call_delegate_23 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_23, %llama_cpp__weight_int8pack_mm_default_39, %llama_cpp__weight_int8pack_mm_default_40), kwargs = {})\n",
      "    %getitem_67 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_23, 0), kwargs = {})\n",
      "    %alloc_41 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_41 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_67, %b_layers_5_feed_forward_w2_weight, %b_layers_5_feed_forward_w2_scales), kwargs = {out: %alloc_41})\n",
      "    %lowered_module_24 : [num_users=1] = get_attr[target=lowered_module_24]\n",
      "    %executorch_call_delegate_24 : [num_users=4] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_24, %getitem_64, %llama_cpp__weight_int8pack_mm_default_41), kwargs = {})\n",
      "    %getitem_68 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_24, 0), kwargs = {})\n",
      "    %getitem_69 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_24, 1), kwargs = {})\n",
      "    %getitem_70 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_24, 2), kwargs = {})\n",
      "    %getitem_71 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_24, 3), kwargs = {})\n",
      "    %alloc_42 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_42 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_69, %b_layers_6_attention_wq_weight, %b_layers_6_attention_wq_scales), kwargs = {out: %alloc_42})\n",
      "    %alloc_43 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_43 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_70, %b_layers_6_attention_wk_weight, %b_layers_6_attention_wk_scales), kwargs = {out: %alloc_43})\n",
      "    %alloc_44 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_44 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_71, %b_layers_6_attention_wv_weight, %b_layers_6_attention_wv_scales), kwargs = {out: %alloc_44})\n",
      "    %lowered_module_25 : [num_users=1] = get_attr[target=lowered_module_25]\n",
      "    %executorch_call_delegate_25 : [num_users=3] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_25, %b_layers_6_attention_sdpa_kv_cache_v_cache, %b_layers_6_attention_sdpa_kv_cache_k_cache, %getitem_4, %getitem_5, %llama_cpp__weight_int8pack_mm_default_42, %llama_cpp__weight_int8pack_mm_default_43, %llama_cpp__weight_int8pack_mm_default_44, %input_pos), kwargs = {})\n",
      "    %getitem_72 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_25, 0), kwargs = {})\n",
      "    %getitem_73 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_25, 1), kwargs = {})\n",
      "    %getitem_74 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_25, 2), kwargs = {})\n",
      "    %alloc_45 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_45 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_74, %b_layers_6_attention_wo_weight, %b_layers_6_attention_wo_scales), kwargs = {out: %alloc_45})\n",
      "    %lowered_module_26 : [num_users=1] = get_attr[target=lowered_module_26]\n",
      "    %executorch_call_delegate_26 : [num_users=3] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_26, %getitem_68, %llama_cpp__weight_int8pack_mm_default_45), kwargs = {})\n",
      "    %getitem_75 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_26, 0), kwargs = {})\n",
      "    %getitem_76 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_26, 1), kwargs = {})\n",
      "    %getitem_77 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_26, 2), kwargs = {})\n",
      "    %alloc_46 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 2048), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_46 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_76, %b_layers_6_feed_forward_w1_weight, %b_layers_6_feed_forward_w1_scales), kwargs = {out: %alloc_46})\n",
      "    %alloc_47 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 2048), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_47 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_77, %b_layers_6_feed_forward_w3_weight, %b_layers_6_feed_forward_w3_scales), kwargs = {out: %alloc_47})\n",
      "    %lowered_module_27 : [num_users=1] = get_attr[target=lowered_module_27]\n",
      "    %executorch_call_delegate_27 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_27, %llama_cpp__weight_int8pack_mm_default_46, %llama_cpp__weight_int8pack_mm_default_47), kwargs = {})\n",
      "    %getitem_78 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_27, 0), kwargs = {})\n",
      "    %alloc_48 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_48 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_78, %b_layers_6_feed_forward_w2_weight, %b_layers_6_feed_forward_w2_scales), kwargs = {out: %alloc_48})\n",
      "    %lowered_module_28 : [num_users=1] = get_attr[target=lowered_module_28]\n",
      "    %executorch_call_delegate_28 : [num_users=4] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_28, %getitem_75, %llama_cpp__weight_int8pack_mm_default_48), kwargs = {})\n",
      "    %getitem_79 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_28, 0), kwargs = {})\n",
      "    %getitem_80 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_28, 1), kwargs = {})\n",
      "    %getitem_81 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_28, 2), kwargs = {})\n",
      "    %getitem_82 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_28, 3), kwargs = {})\n",
      "    %alloc_49 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_49 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_80, %b_layers_7_attention_wq_weight, %b_layers_7_attention_wq_scales), kwargs = {out: %alloc_49})\n",
      "    %alloc_50 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_50 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_81, %b_layers_7_attention_wk_weight, %b_layers_7_attention_wk_scales), kwargs = {out: %alloc_50})\n",
      "    %alloc_51 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_51 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_82, %b_layers_7_attention_wv_weight, %b_layers_7_attention_wv_scales), kwargs = {out: %alloc_51})\n",
      "    %lowered_module_29 : [num_users=1] = get_attr[target=lowered_module_29]\n",
      "    %executorch_call_delegate_29 : [num_users=3] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_29, %b_layers_7_attention_sdpa_kv_cache_v_cache, %b_layers_7_attention_sdpa_kv_cache_k_cache, %getitem_4, %getitem_5, %llama_cpp__weight_int8pack_mm_default_49, %llama_cpp__weight_int8pack_mm_default_50, %llama_cpp__weight_int8pack_mm_default_51, %input_pos), kwargs = {})\n",
      "    %getitem_83 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_29, 0), kwargs = {})\n",
      "    %getitem_84 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_29, 1), kwargs = {})\n",
      "    %getitem_85 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_29, 2), kwargs = {})\n",
      "    %alloc_52 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_52 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_85, %b_layers_7_attention_wo_weight, %b_layers_7_attention_wo_scales), kwargs = {out: %alloc_52})\n",
      "    %lowered_module_30 : [num_users=1] = get_attr[target=lowered_module_30]\n",
      "    %executorch_call_delegate_30 : [num_users=3] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_30, %getitem_79, %llama_cpp__weight_int8pack_mm_default_52), kwargs = {})\n",
      "    %getitem_86 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_30, 0), kwargs = {})\n",
      "    %getitem_87 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_30, 1), kwargs = {})\n",
      "    %getitem_88 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_30, 2), kwargs = {})\n",
      "    %alloc_53 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 2048), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_53 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_87, %b_layers_7_feed_forward_w1_weight, %b_layers_7_feed_forward_w1_scales), kwargs = {out: %alloc_53})\n",
      "    %alloc_54 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 2048), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_54 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_88, %b_layers_7_feed_forward_w3_weight, %b_layers_7_feed_forward_w3_scales), kwargs = {out: %alloc_54})\n",
      "    %lowered_module_31 : [num_users=1] = get_attr[target=lowered_module_31]\n",
      "    %executorch_call_delegate_31 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_31, %llama_cpp__weight_int8pack_mm_default_53, %llama_cpp__weight_int8pack_mm_default_54), kwargs = {})\n",
      "    %getitem_89 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_31, 0), kwargs = {})\n",
      "    %alloc_55 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_55 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_89, %b_layers_7_feed_forward_w2_weight, %b_layers_7_feed_forward_w2_scales), kwargs = {out: %alloc_55})\n",
      "    %lowered_module_32 : [num_users=1] = get_attr[target=lowered_module_32]\n",
      "    %executorch_call_delegate_32 : [num_users=4] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_32, %getitem_86, %llama_cpp__weight_int8pack_mm_default_55), kwargs = {})\n",
      "    %getitem_90 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_32, 0), kwargs = {})\n",
      "    %getitem_91 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_32, 1), kwargs = {})\n",
      "    %getitem_92 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_32, 2), kwargs = {})\n",
      "    %getitem_93 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_32, 3), kwargs = {})\n",
      "    %alloc_56 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_56 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_91, %b_layers_8_attention_wq_weight, %b_layers_8_attention_wq_scales), kwargs = {out: %alloc_56})\n",
      "    %alloc_57 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_57 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_92, %b_layers_8_attention_wk_weight, %b_layers_8_attention_wk_scales), kwargs = {out: %alloc_57})\n",
      "    %alloc_58 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_58 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_93, %b_layers_8_attention_wv_weight, %b_layers_8_attention_wv_scales), kwargs = {out: %alloc_58})\n",
      "    %lowered_module_33 : [num_users=1] = get_attr[target=lowered_module_33]\n",
      "    %executorch_call_delegate_33 : [num_users=3] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_33, %b_layers_8_attention_sdpa_kv_cache_v_cache, %b_layers_8_attention_sdpa_kv_cache_k_cache, %getitem_4, %getitem_5, %llama_cpp__weight_int8pack_mm_default_56, %llama_cpp__weight_int8pack_mm_default_57, %llama_cpp__weight_int8pack_mm_default_58, %input_pos), kwargs = {})\n",
      "    %getitem_94 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_33, 0), kwargs = {})\n",
      "    %getitem_95 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_33, 1), kwargs = {})\n",
      "    %getitem_96 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_33, 2), kwargs = {})\n",
      "    %alloc_59 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_59 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_96, %b_layers_8_attention_wo_weight, %b_layers_8_attention_wo_scales), kwargs = {out: %alloc_59})\n",
      "    %lowered_module_34 : [num_users=1] = get_attr[target=lowered_module_34]\n",
      "    %executorch_call_delegate_34 : [num_users=3] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_34, %getitem_90, %llama_cpp__weight_int8pack_mm_default_59), kwargs = {})\n",
      "    %getitem_97 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_34, 0), kwargs = {})\n",
      "    %getitem_98 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_34, 1), kwargs = {})\n",
      "    %getitem_99 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_34, 2), kwargs = {})\n",
      "    %alloc_60 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 2048), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_60 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_98, %b_layers_8_feed_forward_w1_weight, %b_layers_8_feed_forward_w1_scales), kwargs = {out: %alloc_60})\n",
      "    %alloc_61 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 2048), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_61 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_99, %b_layers_8_feed_forward_w3_weight, %b_layers_8_feed_forward_w3_scales), kwargs = {out: %alloc_61})\n",
      "    %lowered_module_35 : [num_users=1] = get_attr[target=lowered_module_35]\n",
      "    %executorch_call_delegate_35 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_35, %llama_cpp__weight_int8pack_mm_default_60, %llama_cpp__weight_int8pack_mm_default_61), kwargs = {})\n",
      "    %getitem_100 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_35, 0), kwargs = {})\n",
      "    %alloc_62 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_62 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_100, %b_layers_8_feed_forward_w2_weight, %b_layers_8_feed_forward_w2_scales), kwargs = {out: %alloc_62})\n",
      "    %lowered_module_36 : [num_users=1] = get_attr[target=lowered_module_36]\n",
      "    %executorch_call_delegate_36 : [num_users=4] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_36, %getitem_97, %llama_cpp__weight_int8pack_mm_default_62), kwargs = {})\n",
      "    %getitem_101 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_36, 0), kwargs = {})\n",
      "    %getitem_102 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_36, 1), kwargs = {})\n",
      "    %getitem_103 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_36, 2), kwargs = {})\n",
      "    %getitem_104 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_36, 3), kwargs = {})\n",
      "    %alloc_63 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_63 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_102, %b_layers_9_attention_wq_weight, %b_layers_9_attention_wq_scales), kwargs = {out: %alloc_63})\n",
      "    %alloc_64 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_64 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_103, %b_layers_9_attention_wk_weight, %b_layers_9_attention_wk_scales), kwargs = {out: %alloc_64})\n",
      "    %alloc_65 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_65 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_104, %b_layers_9_attention_wv_weight, %b_layers_9_attention_wv_scales), kwargs = {out: %alloc_65})\n",
      "    %lowered_module_37 : [num_users=1] = get_attr[target=lowered_module_37]\n",
      "    %executorch_call_delegate_37 : [num_users=3] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_37, %b_layers_9_attention_sdpa_kv_cache_v_cache, %b_layers_9_attention_sdpa_kv_cache_k_cache, %getitem_4, %getitem_5, %llama_cpp__weight_int8pack_mm_default_63, %llama_cpp__weight_int8pack_mm_default_64, %llama_cpp__weight_int8pack_mm_default_65, %input_pos), kwargs = {})\n",
      "    %getitem_105 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_37, 0), kwargs = {})\n",
      "    %getitem_106 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_37, 1), kwargs = {})\n",
      "    %getitem_107 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_37, 2), kwargs = {})\n",
      "    %alloc_66 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_66 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_107, %b_layers_9_attention_wo_weight, %b_layers_9_attention_wo_scales), kwargs = {out: %alloc_66})\n",
      "    %lowered_module_38 : [num_users=1] = get_attr[target=lowered_module_38]\n",
      "    %executorch_call_delegate_38 : [num_users=3] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_38, %getitem_101, %llama_cpp__weight_int8pack_mm_default_66), kwargs = {})\n",
      "    %getitem_108 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_38, 0), kwargs = {})\n",
      "    %getitem_109 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_38, 1), kwargs = {})\n",
      "    %getitem_110 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_38, 2), kwargs = {})\n",
      "    %alloc_67 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 2048), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_67 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_109, %b_layers_9_feed_forward_w1_weight, %b_layers_9_feed_forward_w1_scales), kwargs = {out: %alloc_67})\n",
      "    %alloc_68 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 2048), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_68 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_110, %b_layers_9_feed_forward_w3_weight, %b_layers_9_feed_forward_w3_scales), kwargs = {out: %alloc_68})\n",
      "    %lowered_module_39 : [num_users=1] = get_attr[target=lowered_module_39]\n",
      "    %executorch_call_delegate_39 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_39, %llama_cpp__weight_int8pack_mm_default_67, %llama_cpp__weight_int8pack_mm_default_68), kwargs = {})\n",
      "    %getitem_111 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_39, 0), kwargs = {})\n",
      "    %alloc_69 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_69 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_111, %b_layers_9_feed_forward_w2_weight, %b_layers_9_feed_forward_w2_scales), kwargs = {out: %alloc_69})\n",
      "    %lowered_module_40 : [num_users=1] = get_attr[target=lowered_module_40]\n",
      "    %executorch_call_delegate_40 : [num_users=4] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_40, %getitem_108, %llama_cpp__weight_int8pack_mm_default_69), kwargs = {})\n",
      "    %getitem_112 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_40, 0), kwargs = {})\n",
      "    %getitem_113 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_40, 1), kwargs = {})\n",
      "    %getitem_114 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_40, 2), kwargs = {})\n",
      "    %getitem_115 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_40, 3), kwargs = {})\n",
      "    %alloc_70 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_70 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_113, %b_layers_10_attention_wq_weight, %b_layers_10_attention_wq_scales), kwargs = {out: %alloc_70})\n",
      "    %alloc_71 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_71 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_114, %b_layers_10_attention_wk_weight, %b_layers_10_attention_wk_scales), kwargs = {out: %alloc_71})\n",
      "    %alloc_72 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_72 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_115, %b_layers_10_attention_wv_weight, %b_layers_10_attention_wv_scales), kwargs = {out: %alloc_72})\n",
      "    %lowered_module_41 : [num_users=1] = get_attr[target=lowered_module_41]\n",
      "    %executorch_call_delegate_41 : [num_users=3] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_41, %b_layers_10_attention_sdpa_kv_cache_v_cache, %b_layers_10_attention_sdpa_kv_cache_k_cache, %getitem_4, %getitem_5, %llama_cpp__weight_int8pack_mm_default_70, %llama_cpp__weight_int8pack_mm_default_71, %llama_cpp__weight_int8pack_mm_default_72, %input_pos), kwargs = {})\n",
      "    %getitem_116 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_41, 0), kwargs = {})\n",
      "    %getitem_117 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_41, 1), kwargs = {})\n",
      "    %getitem_118 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_41, 2), kwargs = {})\n",
      "    %alloc_73 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_73 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_118, %b_layers_10_attention_wo_weight, %b_layers_10_attention_wo_scales), kwargs = {out: %alloc_73})\n",
      "    %lowered_module_42 : [num_users=1] = get_attr[target=lowered_module_42]\n",
      "    %executorch_call_delegate_42 : [num_users=3] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_42, %getitem_112, %llama_cpp__weight_int8pack_mm_default_73), kwargs = {})\n",
      "    %getitem_119 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_42, 0), kwargs = {})\n",
      "    %getitem_120 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_42, 1), kwargs = {})\n",
      "    %getitem_121 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_42, 2), kwargs = {})\n",
      "    %alloc_74 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 2048), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_74 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_120, %b_layers_10_feed_forward_w1_weight, %b_layers_10_feed_forward_w1_scales), kwargs = {out: %alloc_74})\n",
      "    %alloc_75 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 2048), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_75 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_121, %b_layers_10_feed_forward_w3_weight, %b_layers_10_feed_forward_w3_scales), kwargs = {out: %alloc_75})\n",
      "    %lowered_module_43 : [num_users=1] = get_attr[target=lowered_module_43]\n",
      "    %executorch_call_delegate_43 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_43, %llama_cpp__weight_int8pack_mm_default_74, %llama_cpp__weight_int8pack_mm_default_75), kwargs = {})\n",
      "    %getitem_122 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_43, 0), kwargs = {})\n",
      "    %alloc_76 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_76 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_122, %b_layers_10_feed_forward_w2_weight, %b_layers_10_feed_forward_w2_scales), kwargs = {out: %alloc_76})\n",
      "    %lowered_module_44 : [num_users=1] = get_attr[target=lowered_module_44]\n",
      "    %executorch_call_delegate_44 : [num_users=4] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_44, %getitem_119, %llama_cpp__weight_int8pack_mm_default_76), kwargs = {})\n",
      "    %getitem_123 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_44, 0), kwargs = {})\n",
      "    %getitem_124 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_44, 1), kwargs = {})\n",
      "    %getitem_125 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_44, 2), kwargs = {})\n",
      "    %getitem_126 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_44, 3), kwargs = {})\n",
      "    %alloc_77 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_77 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_124, %b_layers_11_attention_wq_weight, %b_layers_11_attention_wq_scales), kwargs = {out: %alloc_77})\n",
      "    %alloc_78 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_78 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_125, %b_layers_11_attention_wk_weight, %b_layers_11_attention_wk_scales), kwargs = {out: %alloc_78})\n",
      "    %alloc_79 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_79 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_126, %b_layers_11_attention_wv_weight, %b_layers_11_attention_wv_scales), kwargs = {out: %alloc_79})\n",
      "    %lowered_module_45 : [num_users=1] = get_attr[target=lowered_module_45]\n",
      "    %executorch_call_delegate_45 : [num_users=3] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_45, %b_layers_11_attention_sdpa_kv_cache_v_cache, %b_layers_11_attention_sdpa_kv_cache_k_cache, %getitem_4, %getitem_5, %llama_cpp__weight_int8pack_mm_default_77, %llama_cpp__weight_int8pack_mm_default_78, %llama_cpp__weight_int8pack_mm_default_79, %input_pos), kwargs = {})\n",
      "    %getitem_127 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_45, 0), kwargs = {})\n",
      "    %getitem_128 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_45, 1), kwargs = {})\n",
      "    %getitem_129 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_45, 2), kwargs = {})\n",
      "    %alloc_80 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_80 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_129, %b_layers_11_attention_wo_weight, %b_layers_11_attention_wo_scales), kwargs = {out: %alloc_80})\n",
      "    %lowered_module_46 : [num_users=1] = get_attr[target=lowered_module_46]\n",
      "    %executorch_call_delegate_46 : [num_users=3] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_46, %getitem_123, %llama_cpp__weight_int8pack_mm_default_80), kwargs = {})\n",
      "    %getitem_130 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_46, 0), kwargs = {})\n",
      "    %getitem_131 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_46, 1), kwargs = {})\n",
      "    %getitem_132 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_46, 2), kwargs = {})\n",
      "    %alloc_81 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 2048), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_81 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_131, %b_layers_11_feed_forward_w1_weight, %b_layers_11_feed_forward_w1_scales), kwargs = {out: %alloc_81})\n",
      "    %alloc_82 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 2048), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_82 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_132, %b_layers_11_feed_forward_w3_weight, %b_layers_11_feed_forward_w3_scales), kwargs = {out: %alloc_82})\n",
      "    %lowered_module_47 : [num_users=1] = get_attr[target=lowered_module_47]\n",
      "    %executorch_call_delegate_47 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_47, %llama_cpp__weight_int8pack_mm_default_81, %llama_cpp__weight_int8pack_mm_default_82), kwargs = {})\n",
      "    %getitem_133 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_47, 0), kwargs = {})\n",
      "    %alloc_83 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_83 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_133, %b_layers_11_feed_forward_w2_weight, %b_layers_11_feed_forward_w2_scales), kwargs = {out: %alloc_83})\n",
      "    %lowered_module_48 : [num_users=1] = get_attr[target=lowered_module_48]\n",
      "    %executorch_call_delegate_48 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_48, %getitem_130, %llama_cpp__weight_int8pack_mm_default_83), kwargs = {})\n",
      "    %getitem_134 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_48, 0), kwargs = {})\n",
      "    %alloc_84 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 32000), torch.float32),), kwargs = {})\n",
      "    %llama_cpp__weight_int8pack_mm_default_84 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_134, %b_output_weight, %b_output_scales), kwargs = {out: %alloc_84})\n",
      "    %copy_ : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_0_attention_sdpa_kv_cache_k_cache, %getitem_7), kwargs = {})\n",
      "    %copy__1 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_0_attention_sdpa_kv_cache_v_cache, %getitem_6), kwargs = {})\n",
      "    %copy__2 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_1_attention_sdpa_kv_cache_k_cache, %getitem_18), kwargs = {})\n",
      "    %copy__3 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_1_attention_sdpa_kv_cache_v_cache, %getitem_17), kwargs = {})\n",
      "    %copy__4 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_2_attention_sdpa_kv_cache_k_cache, %getitem_29), kwargs = {})\n",
      "    %copy__5 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_2_attention_sdpa_kv_cache_v_cache, %getitem_28), kwargs = {})\n",
      "    %copy__6 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_3_attention_sdpa_kv_cache_k_cache, %getitem_40), kwargs = {})\n",
      "    %copy__7 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_3_attention_sdpa_kv_cache_v_cache, %getitem_39), kwargs = {})\n",
      "    %copy__8 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_4_attention_sdpa_kv_cache_k_cache, %getitem_51), kwargs = {})\n",
      "    %copy__9 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_4_attention_sdpa_kv_cache_v_cache, %getitem_50), kwargs = {})\n",
      "    %copy__10 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_5_attention_sdpa_kv_cache_k_cache, %getitem_62), kwargs = {})\n",
      "    %copy__11 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_5_attention_sdpa_kv_cache_v_cache, %getitem_61), kwargs = {})\n",
      "    %copy__12 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_6_attention_sdpa_kv_cache_k_cache, %getitem_73), kwargs = {})\n",
      "    %copy__13 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_6_attention_sdpa_kv_cache_v_cache, %getitem_72), kwargs = {})\n",
      "    %copy__14 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_7_attention_sdpa_kv_cache_k_cache, %getitem_84), kwargs = {})\n",
      "    %copy__15 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_7_attention_sdpa_kv_cache_v_cache, %getitem_83), kwargs = {})\n",
      "    %copy__16 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_8_attention_sdpa_kv_cache_k_cache, %getitem_95), kwargs = {})\n",
      "    %copy__17 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_8_attention_sdpa_kv_cache_v_cache, %getitem_94), kwargs = {})\n",
      "    %copy__18 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_9_attention_sdpa_kv_cache_k_cache, %getitem_106), kwargs = {})\n",
      "    %copy__19 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_9_attention_sdpa_kv_cache_v_cache, %getitem_105), kwargs = {})\n",
      "    %copy__20 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_10_attention_sdpa_kv_cache_k_cache, %getitem_117), kwargs = {})\n",
      "    %copy__21 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_10_attention_sdpa_kv_cache_v_cache, %getitem_116), kwargs = {})\n",
      "    %copy__22 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_11_attention_sdpa_kv_cache_k_cache, %getitem_128), kwargs = {})\n",
      "    %copy__23 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_11_attention_sdpa_kv_cache_v_cache, %getitem_127), kwargs = {})\n",
      "    return (llama_cpp__weight_int8pack_mm_default_84,)\n"
     ]
    }
   ],
   "source": [
    "print(model.edge_manager._edge_programs['forward'].graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph():\n",
      "  %b_layers_0_attention_wq_weight : [num_users=1] = placeholder[target=b_layers_0_attention_wq_weight]\n",
      "  %b_layers_0_attention_wq_scales : [num_users=1] = placeholder[target=b_layers_0_attention_wq_scales]\n",
      "  %b_layers_0_attention_wk_weight : [num_users=1] = placeholder[target=b_layers_0_attention_wk_weight]\n",
      "  %b_layers_0_attention_wk_scales : [num_users=1] = placeholder[target=b_layers_0_attention_wk_scales]\n",
      "  %b_layers_0_attention_wv_weight : [num_users=1] = placeholder[target=b_layers_0_attention_wv_weight]\n",
      "  %b_layers_0_attention_wv_scales : [num_users=1] = placeholder[target=b_layers_0_attention_wv_scales]\n",
      "  %b_layers_0_attention_sdpa_kv_cache_k_cache : [num_users=2] = placeholder[target=b_layers_0_attention_sdpa_kv_cache_k_cache]\n",
      "  %b_layers_0_attention_sdpa_kv_cache_v_cache : [num_users=2] = placeholder[target=b_layers_0_attention_sdpa_kv_cache_v_cache]\n",
      "  %b_layers_0_attention_wo_weight : [num_users=1] = placeholder[target=b_layers_0_attention_wo_weight]\n",
      "  %b_layers_0_attention_wo_scales : [num_users=1] = placeholder[target=b_layers_0_attention_wo_scales]\n",
      "  %b_layers_0_feed_forward_w1_weight : [num_users=1] = placeholder[target=b_layers_0_feed_forward_w1_weight]\n",
      "  %b_layers_0_feed_forward_w1_scales : [num_users=1] = placeholder[target=b_layers_0_feed_forward_w1_scales]\n",
      "  %b_layers_0_feed_forward_w3_weight : [num_users=1] = placeholder[target=b_layers_0_feed_forward_w3_weight]\n",
      "  %b_layers_0_feed_forward_w3_scales : [num_users=1] = placeholder[target=b_layers_0_feed_forward_w3_scales]\n",
      "  %b_layers_0_feed_forward_w2_weight : [num_users=1] = placeholder[target=b_layers_0_feed_forward_w2_weight]\n",
      "  %b_layers_0_feed_forward_w2_scales : [num_users=1] = placeholder[target=b_layers_0_feed_forward_w2_scales]\n",
      "  %b_layers_1_attention_wq_weight : [num_users=1] = placeholder[target=b_layers_1_attention_wq_weight]\n",
      "  %b_layers_1_attention_wq_scales : [num_users=1] = placeholder[target=b_layers_1_attention_wq_scales]\n",
      "  %b_layers_1_attention_wk_weight : [num_users=1] = placeholder[target=b_layers_1_attention_wk_weight]\n",
      "  %b_layers_1_attention_wk_scales : [num_users=1] = placeholder[target=b_layers_1_attention_wk_scales]\n",
      "  %b_layers_1_attention_wv_weight : [num_users=1] = placeholder[target=b_layers_1_attention_wv_weight]\n",
      "  %b_layers_1_attention_wv_scales : [num_users=1] = placeholder[target=b_layers_1_attention_wv_scales]\n",
      "  %b_layers_1_attention_sdpa_kv_cache_k_cache : [num_users=2] = placeholder[target=b_layers_1_attention_sdpa_kv_cache_k_cache]\n",
      "  %b_layers_1_attention_sdpa_kv_cache_v_cache : [num_users=2] = placeholder[target=b_layers_1_attention_sdpa_kv_cache_v_cache]\n",
      "  %b_layers_1_attention_wo_weight : [num_users=1] = placeholder[target=b_layers_1_attention_wo_weight]\n",
      "  %b_layers_1_attention_wo_scales : [num_users=1] = placeholder[target=b_layers_1_attention_wo_scales]\n",
      "  %b_layers_1_feed_forward_w1_weight : [num_users=1] = placeholder[target=b_layers_1_feed_forward_w1_weight]\n",
      "  %b_layers_1_feed_forward_w1_scales : [num_users=1] = placeholder[target=b_layers_1_feed_forward_w1_scales]\n",
      "  %b_layers_1_feed_forward_w3_weight : [num_users=1] = placeholder[target=b_layers_1_feed_forward_w3_weight]\n",
      "  %b_layers_1_feed_forward_w3_scales : [num_users=1] = placeholder[target=b_layers_1_feed_forward_w3_scales]\n",
      "  %b_layers_1_feed_forward_w2_weight : [num_users=1] = placeholder[target=b_layers_1_feed_forward_w2_weight]\n",
      "  %b_layers_1_feed_forward_w2_scales : [num_users=1] = placeholder[target=b_layers_1_feed_forward_w2_scales]\n",
      "  %b_layers_2_attention_wq_weight : [num_users=1] = placeholder[target=b_layers_2_attention_wq_weight]\n",
      "  %b_layers_2_attention_wq_scales : [num_users=1] = placeholder[target=b_layers_2_attention_wq_scales]\n",
      "  %b_layers_2_attention_wk_weight : [num_users=1] = placeholder[target=b_layers_2_attention_wk_weight]\n",
      "  %b_layers_2_attention_wk_scales : [num_users=1] = placeholder[target=b_layers_2_attention_wk_scales]\n",
      "  %b_layers_2_attention_wv_weight : [num_users=1] = placeholder[target=b_layers_2_attention_wv_weight]\n",
      "  %b_layers_2_attention_wv_scales : [num_users=1] = placeholder[target=b_layers_2_attention_wv_scales]\n",
      "  %b_layers_2_attention_sdpa_kv_cache_k_cache : [num_users=2] = placeholder[target=b_layers_2_attention_sdpa_kv_cache_k_cache]\n",
      "  %b_layers_2_attention_sdpa_kv_cache_v_cache : [num_users=2] = placeholder[target=b_layers_2_attention_sdpa_kv_cache_v_cache]\n",
      "  %b_layers_2_attention_wo_weight : [num_users=1] = placeholder[target=b_layers_2_attention_wo_weight]\n",
      "  %b_layers_2_attention_wo_scales : [num_users=1] = placeholder[target=b_layers_2_attention_wo_scales]\n",
      "  %b_layers_2_feed_forward_w1_weight : [num_users=1] = placeholder[target=b_layers_2_feed_forward_w1_weight]\n",
      "  %b_layers_2_feed_forward_w1_scales : [num_users=1] = placeholder[target=b_layers_2_feed_forward_w1_scales]\n",
      "  %b_layers_2_feed_forward_w3_weight : [num_users=1] = placeholder[target=b_layers_2_feed_forward_w3_weight]\n",
      "  %b_layers_2_feed_forward_w3_scales : [num_users=1] = placeholder[target=b_layers_2_feed_forward_w3_scales]\n",
      "  %b_layers_2_feed_forward_w2_weight : [num_users=1] = placeholder[target=b_layers_2_feed_forward_w2_weight]\n",
      "  %b_layers_2_feed_forward_w2_scales : [num_users=1] = placeholder[target=b_layers_2_feed_forward_w2_scales]\n",
      "  %b_layers_3_attention_wq_weight : [num_users=1] = placeholder[target=b_layers_3_attention_wq_weight]\n",
      "  %b_layers_3_attention_wq_scales : [num_users=1] = placeholder[target=b_layers_3_attention_wq_scales]\n",
      "  %b_layers_3_attention_wk_weight : [num_users=1] = placeholder[target=b_layers_3_attention_wk_weight]\n",
      "  %b_layers_3_attention_wk_scales : [num_users=1] = placeholder[target=b_layers_3_attention_wk_scales]\n",
      "  %b_layers_3_attention_wv_weight : [num_users=1] = placeholder[target=b_layers_3_attention_wv_weight]\n",
      "  %b_layers_3_attention_wv_scales : [num_users=1] = placeholder[target=b_layers_3_attention_wv_scales]\n",
      "  %b_layers_3_attention_sdpa_kv_cache_k_cache : [num_users=2] = placeholder[target=b_layers_3_attention_sdpa_kv_cache_k_cache]\n",
      "  %b_layers_3_attention_sdpa_kv_cache_v_cache : [num_users=2] = placeholder[target=b_layers_3_attention_sdpa_kv_cache_v_cache]\n",
      "  %b_layers_3_attention_wo_weight : [num_users=1] = placeholder[target=b_layers_3_attention_wo_weight]\n",
      "  %b_layers_3_attention_wo_scales : [num_users=1] = placeholder[target=b_layers_3_attention_wo_scales]\n",
      "  %b_layers_3_feed_forward_w1_weight : [num_users=1] = placeholder[target=b_layers_3_feed_forward_w1_weight]\n",
      "  %b_layers_3_feed_forward_w1_scales : [num_users=1] = placeholder[target=b_layers_3_feed_forward_w1_scales]\n",
      "  %b_layers_3_feed_forward_w3_weight : [num_users=1] = placeholder[target=b_layers_3_feed_forward_w3_weight]\n",
      "  %b_layers_3_feed_forward_w3_scales : [num_users=1] = placeholder[target=b_layers_3_feed_forward_w3_scales]\n",
      "  %b_layers_3_feed_forward_w2_weight : [num_users=1] = placeholder[target=b_layers_3_feed_forward_w2_weight]\n",
      "  %b_layers_3_feed_forward_w2_scales : [num_users=1] = placeholder[target=b_layers_3_feed_forward_w2_scales]\n",
      "  %b_layers_4_attention_wq_weight : [num_users=1] = placeholder[target=b_layers_4_attention_wq_weight]\n",
      "  %b_layers_4_attention_wq_scales : [num_users=1] = placeholder[target=b_layers_4_attention_wq_scales]\n",
      "  %b_layers_4_attention_wk_weight : [num_users=1] = placeholder[target=b_layers_4_attention_wk_weight]\n",
      "  %b_layers_4_attention_wk_scales : [num_users=1] = placeholder[target=b_layers_4_attention_wk_scales]\n",
      "  %b_layers_4_attention_wv_weight : [num_users=1] = placeholder[target=b_layers_4_attention_wv_weight]\n",
      "  %b_layers_4_attention_wv_scales : [num_users=1] = placeholder[target=b_layers_4_attention_wv_scales]\n",
      "  %b_layers_4_attention_sdpa_kv_cache_k_cache : [num_users=2] = placeholder[target=b_layers_4_attention_sdpa_kv_cache_k_cache]\n",
      "  %b_layers_4_attention_sdpa_kv_cache_v_cache : [num_users=2] = placeholder[target=b_layers_4_attention_sdpa_kv_cache_v_cache]\n",
      "  %b_layers_4_attention_wo_weight : [num_users=1] = placeholder[target=b_layers_4_attention_wo_weight]\n",
      "  %b_layers_4_attention_wo_scales : [num_users=1] = placeholder[target=b_layers_4_attention_wo_scales]\n",
      "  %b_layers_4_feed_forward_w1_weight : [num_users=1] = placeholder[target=b_layers_4_feed_forward_w1_weight]\n",
      "  %b_layers_4_feed_forward_w1_scales : [num_users=1] = placeholder[target=b_layers_4_feed_forward_w1_scales]\n",
      "  %b_layers_4_feed_forward_w3_weight : [num_users=1] = placeholder[target=b_layers_4_feed_forward_w3_weight]\n",
      "  %b_layers_4_feed_forward_w3_scales : [num_users=1] = placeholder[target=b_layers_4_feed_forward_w3_scales]\n",
      "  %b_layers_4_feed_forward_w2_weight : [num_users=1] = placeholder[target=b_layers_4_feed_forward_w2_weight]\n",
      "  %b_layers_4_feed_forward_w2_scales : [num_users=1] = placeholder[target=b_layers_4_feed_forward_w2_scales]\n",
      "  %b_layers_5_attention_wq_weight : [num_users=1] = placeholder[target=b_layers_5_attention_wq_weight]\n",
      "  %b_layers_5_attention_wq_scales : [num_users=1] = placeholder[target=b_layers_5_attention_wq_scales]\n",
      "  %b_layers_5_attention_wk_weight : [num_users=1] = placeholder[target=b_layers_5_attention_wk_weight]\n",
      "  %b_layers_5_attention_wk_scales : [num_users=1] = placeholder[target=b_layers_5_attention_wk_scales]\n",
      "  %b_layers_5_attention_wv_weight : [num_users=1] = placeholder[target=b_layers_5_attention_wv_weight]\n",
      "  %b_layers_5_attention_wv_scales : [num_users=1] = placeholder[target=b_layers_5_attention_wv_scales]\n",
      "  %b_layers_5_attention_sdpa_kv_cache_k_cache : [num_users=2] = placeholder[target=b_layers_5_attention_sdpa_kv_cache_k_cache]\n",
      "  %b_layers_5_attention_sdpa_kv_cache_v_cache : [num_users=2] = placeholder[target=b_layers_5_attention_sdpa_kv_cache_v_cache]\n",
      "  %b_layers_5_attention_wo_weight : [num_users=1] = placeholder[target=b_layers_5_attention_wo_weight]\n",
      "  %b_layers_5_attention_wo_scales : [num_users=1] = placeholder[target=b_layers_5_attention_wo_scales]\n",
      "  %b_layers_5_feed_forward_w1_weight : [num_users=1] = placeholder[target=b_layers_5_feed_forward_w1_weight]\n",
      "  %b_layers_5_feed_forward_w1_scales : [num_users=1] = placeholder[target=b_layers_5_feed_forward_w1_scales]\n",
      "  %b_layers_5_feed_forward_w3_weight : [num_users=1] = placeholder[target=b_layers_5_feed_forward_w3_weight]\n",
      "  %b_layers_5_feed_forward_w3_scales : [num_users=1] = placeholder[target=b_layers_5_feed_forward_w3_scales]\n",
      "  %b_layers_5_feed_forward_w2_weight : [num_users=1] = placeholder[target=b_layers_5_feed_forward_w2_weight]\n",
      "  %b_layers_5_feed_forward_w2_scales : [num_users=1] = placeholder[target=b_layers_5_feed_forward_w2_scales]\n",
      "  %b_output_weight : [num_users=1] = placeholder[target=b_output_weight]\n",
      "  %b_output_scales : [num_users=1] = placeholder[target=b_output_scales]\n",
      "  %tokens : [num_users=1] = placeholder[target=tokens]\n",
      "  %input_pos : [num_users=6] = placeholder[target=input_pos]\n",
      "  %lowered_module_0 : [num_users=1] = get_attr[target=lowered_module_0]\n",
      "    backend_id: MPSBackend\n",
      "    lowered graph():\n",
      "      %p_tok_embeddings_weight : [num_users=1] = placeholder[target=p_tok_embeddings_weight]\n",
      "      %p_layers_0_attention_norm_weight : [num_users=1] = placeholder[target=p_layers_0_attention_norm_weight]\n",
      "      %_lifted_tensor_constant106 : [num_users=1] = placeholder[target=_lifted_tensor_constant106]\n",
      "      %tokens : [num_users=1] = placeholder[target=tokens]\n",
      "      %aten_embedding_default : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.embedding.default](args = (%p_tok_embeddings_weight, %tokens), kwargs = {})\n",
      "      %aten_mul_tensor : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_embedding_default, %aten_embedding_default), kwargs = {})\n",
      "      %aten_mean_dim : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mean.dim](args = (%aten_mul_tensor, [-1], True), kwargs = {})\n",
      "      %aten_add_tensor : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mean_dim, %_lifted_tensor_constant106), kwargs = {})\n",
      "      %aten_rsqrt_default : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.rsqrt.default](args = (%aten_add_tensor,), kwargs = {})\n",
      "      %aten_mul_tensor_1 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_embedding_default, %aten_rsqrt_default), kwargs = {})\n",
      "      %aten_mul_tensor_2 : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_1, %p_layers_0_attention_norm_weight), kwargs = {})\n",
      "      %aten_squeeze_copy_dims : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_2, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_1 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_2, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_2 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_2, [0]), kwargs = {})\n",
      "      return (aten_embedding_default, aten_squeeze_copy_dims, aten_squeeze_copy_dims_1, aten_squeeze_copy_dims_2)\n",
      "  %executorch_call_delegate : [num_users=4] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_0, %tokens), kwargs = {})\n",
      "  %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate, 0), kwargs = {})\n",
      "  %getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate, 1), kwargs = {})\n",
      "  %getitem_2 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate, 2), kwargs = {})\n",
      "  %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate, 3), kwargs = {})\n",
      "  %alloc : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 288), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_1, %b_layers_0_attention_wq_weight, %b_layers_0_attention_wq_scales), kwargs = {out: %alloc})\n",
      "  %alloc_1 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 288), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_1 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_2, %b_layers_0_attention_wk_weight, %b_layers_0_attention_wk_scales), kwargs = {out: %alloc_1})\n",
      "  %alloc_2 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 288), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_2 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_3, %b_layers_0_attention_wv_weight, %b_layers_0_attention_wv_scales), kwargs = {out: %alloc_2})\n",
      "  %lowered_module_1 : [num_users=1] = get_attr[target=lowered_module_1]\n",
      "    backend_id: MPSBackend\n",
      "    lowered graph():\n",
      "      %b_freqs_cos : [num_users=1] = placeholder[target=b_freqs_cos]\n",
      "      %b_freqs_sin : [num_users=1] = placeholder[target=b_freqs_sin]\n",
      "      %b_layers_0_attention_mask : [num_users=1] = placeholder[target=b_layers_0_attention_mask]\n",
      "      %_lifted_tensor_constant107 : [num_users=1] = placeholder[target=_lifted_tensor_constant107]\n",
      "      %b_layers_0_attention_sdpa_kv_cache_v_cache : [num_users=3] = placeholder[target=b_layers_0_attention_sdpa_kv_cache_v_cache]\n",
      "      %b_layers_0_attention_sdpa_kv_cache_k_cache : [num_users=3] = placeholder[target=b_layers_0_attention_sdpa_kv_cache_k_cache]\n",
      "      %llama_cpp__weight_int8pack_mm_default : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default]\n",
      "      %llama_cpp__weight_int8pack_mm_default_1 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_1]\n",
      "      %llama_cpp__weight_int8pack_mm_default_2 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_2]\n",
      "      %input_pos : [num_users=5] = placeholder[target=input_pos]\n",
      "      %aten_view_copy_default : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%llama_cpp__weight_int8pack_mm_default, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_1 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%llama_cpp__weight_int8pack_mm_default_1, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_2 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%llama_cpp__weight_int8pack_mm_default_2, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_index_tensor : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.index.Tensor](args = (%b_freqs_cos, [%input_pos]), kwargs = {})\n",
      "      %aten_index_tensor_1 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.index.Tensor](args = (%b_freqs_sin, [%input_pos]), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_4 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%b_layers_0_attention_mask, 0), kwargs = {})\n",
      "      %aten_view_copy_default_3 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_view_copy_default, [1, 1, 6, -1, 2]), kwargs = {})\n",
      "      %aten_view_copy_default_4 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_view_copy_default_1, [1, 1, 6, -1, 2]), kwargs = {})\n",
      "      %aten_permute_copy_default_2 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten_view_copy_default_2, [0, 2, 1, 3]), kwargs = {})\n",
      "      %aten_view_copy_default_5 : [num_users=4] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_index_tensor, [1, 1, 1, 24]), kwargs = {})\n",
      "      %aten_view_copy_default_6 : [num_users=4] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_index_tensor_1, [1, 1, 1, 24]), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_5 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_unsqueeze_copy_default_4, 1), kwargs = {})\n",
      "      %aten_slice_copy_tensor : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_3, 4, 0, 1), kwargs = {})\n",
      "      %aten_slice_copy_tensor_1 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_3, 4, 1, 2), kwargs = {})\n",
      "      %aten_slice_copy_tensor_2 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_4, 4, 0, 1), kwargs = {})\n",
      "      %aten_slice_copy_tensor_3 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_4, 4, 1, 2), kwargs = {})\n",
      "      %aten_view_copy_default_10 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_permute_copy_default_2, [6, 1, 48]), kwargs = {})\n",
      "      %aten_index_tensor_2 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.index.Tensor](args = (%aten_unsqueeze_copy_default_5, [None, None, %input_pos]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_3 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_4 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_1, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_5 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_2, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_6 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_3, [4]), kwargs = {})\n",
      "      %aten_index_put_default_1 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.index_put.default](args = (%b_layers_0_attention_sdpa_kv_cache_v_cache, [None, None, %input_pos], %aten_view_copy_default_10), kwargs = {})\n",
      "      %aten__to_copy_default : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%aten_index_tensor_2,), kwargs = {dtype: torch.float32})\n",
      "      %aten_mul_tensor_3 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_3, %aten_view_copy_default_5), kwargs = {})\n",
      "      %aten_mul_tensor_5 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_3, %aten_view_copy_default_6), kwargs = {})\n",
      "      %aten_mul_tensor_4 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_4, %aten_view_copy_default_6), kwargs = {})\n",
      "      %aten_mul_tensor_6 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_4, %aten_view_copy_default_5), kwargs = {})\n",
      "      %aten_mul_tensor_7 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_5, %aten_view_copy_default_5), kwargs = {})\n",
      "      %aten_mul_tensor_9 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_5, %aten_view_copy_default_6), kwargs = {})\n",
      "      %aten_mul_tensor_8 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_6, %aten_view_copy_default_6), kwargs = {})\n",
      "      %aten_mul_tensor_10 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_6, %aten_view_copy_default_5), kwargs = {})\n",
      "      %aten_slice_scatter_default_2 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_scatter.default](args = (%b_layers_0_attention_sdpa_kv_cache_v_cache, %aten_index_put_default_1, 1, 0, 9223372036854775807), kwargs = {})\n",
      "      %aten_sub_tensor : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sub.Tensor](args = (%aten_mul_tensor_3, %aten_mul_tensor_4), kwargs = {})\n",
      "      %aten_add_tensor_1 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mul_tensor_5, %aten_mul_tensor_6), kwargs = {})\n",
      "      %aten_sub_tensor_1 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sub.Tensor](args = (%aten_mul_tensor_7, %aten_mul_tensor_8), kwargs = {})\n",
      "      %aten_add_tensor_2 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mul_tensor_9, %aten_mul_tensor_10), kwargs = {})\n",
      "      %aten_slice_scatter_default_3 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_scatter.default](args = (%b_layers_0_attention_sdpa_kv_cache_v_cache, %aten_slice_scatter_default_2, 0, 0, 9223372036854775807), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_sub_tensor, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_1 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_add_tensor_1, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_2 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_sub_tensor_1, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_3 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_add_tensor_2, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_7 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_slice_scatter_default_3, 2), kwargs = {})\n",
      "      %aten_cat_default : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.cat.default](args = ([%aten_unsqueeze_copy_default, %aten_unsqueeze_copy_default_1], -1), kwargs = {})\n",
      "      %aten_cat_default_1 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.cat.default](args = ([%aten_unsqueeze_copy_default_2, %aten_unsqueeze_copy_default_3], -1), kwargs = {})\n",
      "      %aten_expand_copy_default_1 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten_unsqueeze_copy_default_7, [1, 6, 1, 128, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_7 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_cat_default, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_8 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_cat_default_1, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_clone_default_1 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.clone.default](args = (%aten_expand_copy_default_1,), kwargs = {memory_format: torch.contiguous_format})\n",
      "      %aten_permute_copy_default : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten_view_copy_default_7, [0, 2, 1, 3]), kwargs = {})\n",
      "      %aten_permute_copy_default_1 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten_view_copy_default_8, [0, 2, 1, 3]), kwargs = {})\n",
      "      %aten_view_copy_default_12 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_clone_default_1, [1, 6, 128, 48]), kwargs = {})\n",
      "      %aten_expand_copy_default_2 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten_permute_copy_default, [1, 6, 1, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_9 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_permute_copy_default_1, [6, 1, 48]), kwargs = {})\n",
      "      %aten_expand_copy_default_5 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten_view_copy_default_12, [1, 6, 128, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_13 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_expand_copy_default_2, [6, 1, 48]), kwargs = {})\n",
      "      %aten_index_put_default : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.index_put.default](args = (%b_layers_0_attention_sdpa_kv_cache_k_cache, [None, None, %input_pos], %aten_view_copy_default_9), kwargs = {})\n",
      "      %aten_view_copy_default_17 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_expand_copy_default_5, [6, 128, 48]), kwargs = {})\n",
      "      %aten_slice_scatter_default : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_scatter.default](args = (%b_layers_0_attention_sdpa_kv_cache_k_cache, %aten_index_put_default, 1, 0, 9223372036854775807), kwargs = {})\n",
      "      %aten_slice_scatter_default_1 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_scatter.default](args = (%b_layers_0_attention_sdpa_kv_cache_k_cache, %aten_slice_scatter_default, 0, 0, 9223372036854775807), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_6 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_slice_scatter_default_1, 2), kwargs = {})\n",
      "      %aten_expand_copy_default : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten_unsqueeze_copy_default_6, [1, 6, 1, 128, 48]), kwargs = {})\n",
      "      %aten_clone_default : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.clone.default](args = (%aten_expand_copy_default,), kwargs = {memory_format: torch.contiguous_format})\n",
      "      %aten_view_copy_default_11 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_clone_default, [1, 6, 128, 48]), kwargs = {})\n",
      "      %aten_permute_copy_default_3 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten_view_copy_default_11, [0, 1, 3, 2]), kwargs = {})\n",
      "      %aten_expand_copy_default_3 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten_permute_copy_default_3, [1, 6, 48, 128]), kwargs = {})\n",
      "      %aten_view_copy_default_14 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_expand_copy_default_3, [6, 48, 128]), kwargs = {})\n",
      "      %aten_bmm_default : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.bmm.default](args = (%aten_view_copy_default_13, %aten_view_copy_default_14), kwargs = {})\n",
      "      %aten_view_copy_default_15 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_bmm_default, [1, 6, 1, 128]), kwargs = {})\n",
      "      %aten_mul_tensor_11 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_view_copy_default_15, %_lifted_tensor_constant107), kwargs = {})\n",
      "      %aten_add_tensor_3 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mul_tensor_11, %aten__to_copy_default), kwargs = {})\n",
      "      %aten__softmax_default : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._softmax.default](args = (%aten_add_tensor_3, -1, False), kwargs = {})\n",
      "      %aten_expand_copy_default_4 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten__softmax_default, [1, 6, 1, 128]), kwargs = {})\n",
      "      %aten_view_copy_default_16 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_expand_copy_default_4, [6, 1, 128]), kwargs = {})\n",
      "      %aten_bmm_default_1 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.bmm.default](args = (%aten_view_copy_default_16, %aten_view_copy_default_17), kwargs = {})\n",
      "      %aten_view_copy_default_18 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_bmm_default_1, [1, 6, 1, 48]), kwargs = {})\n",
      "      %aten_permute_copy_default_4 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten_view_copy_default_18, [0, 2, 1, 3]), kwargs = {})\n",
      "      %aten_view_copy_default_19 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_permute_copy_default_4, [1, 1, 288]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_7 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_view_copy_default_19, [0]), kwargs = {})\n",
      "      return (aten_index_tensor, aten_index_tensor_1, aten_slice_scatter_default_3, aten_slice_scatter_default_1, aten_squeeze_copy_dims_7)\n",
      "  %executorch_call_delegate_1 : [num_users=5] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_1, %b_layers_0_attention_sdpa_kv_cache_v_cache, %b_layers_0_attention_sdpa_kv_cache_k_cache, %llama_cpp__weight_int8pack_mm_default, %llama_cpp__weight_int8pack_mm_default_1, %llama_cpp__weight_int8pack_mm_default_2, %input_pos), kwargs = {})\n",
      "  %getitem_4 : [num_users=5] = call_function[target=operator.getitem](args = (%executorch_call_delegate_1, 0), kwargs = {})\n",
      "  %getitem_5 : [num_users=5] = call_function[target=operator.getitem](args = (%executorch_call_delegate_1, 1), kwargs = {})\n",
      "  %getitem_6 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_1, 2), kwargs = {})\n",
      "  %getitem_7 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_1, 3), kwargs = {})\n",
      "  %getitem_8 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_1, 4), kwargs = {})\n",
      "  %alloc_3 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 288), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_3 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_8, %b_layers_0_attention_wo_weight, %b_layers_0_attention_wo_scales), kwargs = {out: %alloc_3})\n",
      "  %lowered_module_2 : [num_users=1] = get_attr[target=lowered_module_2]\n",
      "    backend_id: MPSBackend\n",
      "    lowered graph():\n",
      "      %p_layers_0_ffn_norm_weight : [num_users=1] = placeholder[target=p_layers_0_ffn_norm_weight]\n",
      "      %_lifted_tensor_constant108 : [num_users=1] = placeholder[target=_lifted_tensor_constant108]\n",
      "      %aten_embedding_default : [num_users=1] = placeholder[target=aten_embedding_default]\n",
      "      %llama_cpp__weight_int8pack_mm_default_3 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_3]\n",
      "      %aten_add_tensor_4 : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_embedding_default, %llama_cpp__weight_int8pack_mm_default_3), kwargs = {})\n",
      "      %aten_mul_tensor_12 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_4, %aten_add_tensor_4), kwargs = {})\n",
      "      %aten_mean_dim_1 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mean.dim](args = (%aten_mul_tensor_12, [-1], True), kwargs = {})\n",
      "      %aten_add_tensor_5 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mean_dim_1, %_lifted_tensor_constant108), kwargs = {})\n",
      "      %aten_rsqrt_default_1 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.rsqrt.default](args = (%aten_add_tensor_5,), kwargs = {})\n",
      "      %aten_mul_tensor_13 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_4, %aten_rsqrt_default_1), kwargs = {})\n",
      "      %aten_mul_tensor_14 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_13, %p_layers_0_ffn_norm_weight), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_8 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_14, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_9 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_14, [0]), kwargs = {})\n",
      "      return (aten_add_tensor_4, aten_squeeze_copy_dims_8, aten_squeeze_copy_dims_9)\n",
      "  %executorch_call_delegate_2 : [num_users=3] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_2, %getitem, %llama_cpp__weight_int8pack_mm_default_3), kwargs = {})\n",
      "  %getitem_9 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_2, 0), kwargs = {})\n",
      "  %getitem_10 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_2, 1), kwargs = {})\n",
      "  %getitem_11 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_2, 2), kwargs = {})\n",
      "  %alloc_4 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_4 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_10, %b_layers_0_feed_forward_w1_weight, %b_layers_0_feed_forward_w1_scales), kwargs = {out: %alloc_4})\n",
      "  %alloc_5 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_5 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_11, %b_layers_0_feed_forward_w3_weight, %b_layers_0_feed_forward_w3_scales), kwargs = {out: %alloc_5})\n",
      "  %lowered_module_3 : [num_users=1] = get_attr[target=lowered_module_3]\n",
      "    backend_id: MPSBackend\n",
      "    lowered graph():\n",
      "      %llama_cpp__weight_int8pack_mm_default_4 : [num_users=2] = placeholder[target=llama_cpp__weight_int8pack_mm_default_4]\n",
      "      %llama_cpp__weight_int8pack_mm_default_5 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_5]\n",
      "      %aten_sigmoid_default : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sigmoid.default](args = (%llama_cpp__weight_int8pack_mm_default_4,), kwargs = {})\n",
      "      %aten_mul_tensor_15 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%llama_cpp__weight_int8pack_mm_default_4, %aten_sigmoid_default), kwargs = {})\n",
      "      %aten_mul_tensor_16 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_15, %llama_cpp__weight_int8pack_mm_default_5), kwargs = {})\n",
      "      return (aten_mul_tensor_16,)\n",
      "  %executorch_call_delegate_3 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_3, %llama_cpp__weight_int8pack_mm_default_4, %llama_cpp__weight_int8pack_mm_default_5), kwargs = {})\n",
      "  %getitem_12 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_3, 0), kwargs = {})\n",
      "  %alloc_6 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 288), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_6 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_12, %b_layers_0_feed_forward_w2_weight, %b_layers_0_feed_forward_w2_scales), kwargs = {out: %alloc_6})\n",
      "  %lowered_module_4 : [num_users=1] = get_attr[target=lowered_module_4]\n",
      "    backend_id: MPSBackend\n",
      "    lowered graph():\n",
      "      %p_layers_1_attention_norm_weight : [num_users=1] = placeholder[target=p_layers_1_attention_norm_weight]\n",
      "      %_lifted_tensor_constant109 : [num_users=1] = placeholder[target=_lifted_tensor_constant109]\n",
      "      %aten_add_tensor_4 : [num_users=1] = placeholder[target=aten_add_tensor_4]\n",
      "      %llama_cpp__weight_int8pack_mm_default_6 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_6]\n",
      "      %aten_add_tensor_6 : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_add_tensor_4, %llama_cpp__weight_int8pack_mm_default_6), kwargs = {})\n",
      "      %aten_mul_tensor_17 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_6, %aten_add_tensor_6), kwargs = {})\n",
      "      %aten_mean_dim_2 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mean.dim](args = (%aten_mul_tensor_17, [-1], True), kwargs = {})\n",
      "      %aten_add_tensor_7 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mean_dim_2, %_lifted_tensor_constant109), kwargs = {})\n",
      "      %aten_rsqrt_default_2 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.rsqrt.default](args = (%aten_add_tensor_7,), kwargs = {})\n",
      "      %aten_mul_tensor_18 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_6, %aten_rsqrt_default_2), kwargs = {})\n",
      "      %aten_mul_tensor_19 : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_18, %p_layers_1_attention_norm_weight), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_10 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_19, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_11 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_19, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_12 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_19, [0]), kwargs = {})\n",
      "      return (aten_add_tensor_6, aten_squeeze_copy_dims_10, aten_squeeze_copy_dims_11, aten_squeeze_copy_dims_12)\n",
      "  %executorch_call_delegate_4 : [num_users=4] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_4, %getitem_9, %llama_cpp__weight_int8pack_mm_default_6), kwargs = {})\n",
      "  %getitem_13 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_4, 0), kwargs = {})\n",
      "  %getitem_14 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_4, 1), kwargs = {})\n",
      "  %getitem_15 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_4, 2), kwargs = {})\n",
      "  %getitem_16 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_4, 3), kwargs = {})\n",
      "  %alloc_7 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 288), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_7 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_14, %b_layers_1_attention_wq_weight, %b_layers_1_attention_wq_scales), kwargs = {out: %alloc_7})\n",
      "  %alloc_8 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 288), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_8 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_15, %b_layers_1_attention_wk_weight, %b_layers_1_attention_wk_scales), kwargs = {out: %alloc_8})\n",
      "  %alloc_9 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 288), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_9 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_16, %b_layers_1_attention_wv_weight, %b_layers_1_attention_wv_scales), kwargs = {out: %alloc_9})\n",
      "  %lowered_module_5 : [num_users=1] = get_attr[target=lowered_module_5]\n",
      "    backend_id: MPSBackend\n",
      "    lowered graph():\n",
      "      %b_layers_1_attention_mask : [num_users=1] = placeholder[target=b_layers_1_attention_mask]\n",
      "      %_lifted_tensor_constant110 : [num_users=1] = placeholder[target=_lifted_tensor_constant110]\n",
      "      %b_layers_1_attention_sdpa_kv_cache_v_cache : [num_users=3] = placeholder[target=b_layers_1_attention_sdpa_kv_cache_v_cache]\n",
      "      %b_layers_1_attention_sdpa_kv_cache_k_cache : [num_users=3] = placeholder[target=b_layers_1_attention_sdpa_kv_cache_k_cache]\n",
      "      %aten_index_tensor : [num_users=1] = placeholder[target=aten_index_tensor]\n",
      "      %aten_index_tensor_1 : [num_users=1] = placeholder[target=aten_index_tensor_1]\n",
      "      %llama_cpp__weight_int8pack_mm_default_7 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_7]\n",
      "      %llama_cpp__weight_int8pack_mm_default_8 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_8]\n",
      "      %llama_cpp__weight_int8pack_mm_default_9 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_9]\n",
      "      %input_pos : [num_users=3] = placeholder[target=input_pos]\n",
      "      %aten_view_copy_default_25 : [num_users=4] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_index_tensor, [1, 1, 1, 24]), kwargs = {})\n",
      "      %aten_view_copy_default_26 : [num_users=4] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_index_tensor_1, [1, 1, 1, 24]), kwargs = {})\n",
      "      %aten_view_copy_default_20 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%llama_cpp__weight_int8pack_mm_default_7, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_21 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%llama_cpp__weight_int8pack_mm_default_8, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_22 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%llama_cpp__weight_int8pack_mm_default_9, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_12 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%b_layers_1_attention_mask, 0), kwargs = {})\n",
      "      %aten_view_copy_default_23 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_view_copy_default_20, [1, 1, 6, -1, 2]), kwargs = {})\n",
      "      %aten_view_copy_default_24 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_view_copy_default_21, [1, 1, 6, -1, 2]), kwargs = {})\n",
      "      %aten_permute_copy_default_7 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten_view_copy_default_22, [0, 2, 1, 3]), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_13 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_unsqueeze_copy_default_12, 1), kwargs = {})\n",
      "      %aten_slice_copy_tensor_4 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_23, 4, 0, 1), kwargs = {})\n",
      "      %aten_slice_copy_tensor_5 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_23, 4, 1, 2), kwargs = {})\n",
      "      %aten_slice_copy_tensor_6 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_24, 4, 0, 1), kwargs = {})\n",
      "      %aten_slice_copy_tensor_7 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_24, 4, 1, 2), kwargs = {})\n",
      "      %aten_view_copy_default_30 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_permute_copy_default_7, [6, 1, 48]), kwargs = {})\n",
      "      %aten_index_tensor_3 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.index.Tensor](args = (%aten_unsqueeze_copy_default_13, [None, None, %input_pos]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_13 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_4, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_14 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_5, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_15 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_6, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_16 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_7, [4]), kwargs = {})\n",
      "      %aten_index_put_default_3 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.index_put.default](args = (%b_layers_1_attention_sdpa_kv_cache_v_cache, [None, None, %input_pos], %aten_view_copy_default_30), kwargs = {})\n",
      "      %aten__to_copy_default_1 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%aten_index_tensor_3,), kwargs = {dtype: torch.float32})\n",
      "      %aten_mul_tensor_20 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_13, %aten_view_copy_default_25), kwargs = {})\n",
      "      %aten_mul_tensor_22 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_13, %aten_view_copy_default_26), kwargs = {})\n",
      "      %aten_mul_tensor_21 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_14, %aten_view_copy_default_26), kwargs = {})\n",
      "      %aten_mul_tensor_23 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_14, %aten_view_copy_default_25), kwargs = {})\n",
      "      %aten_mul_tensor_24 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_15, %aten_view_copy_default_25), kwargs = {})\n",
      "      %aten_mul_tensor_26 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_15, %aten_view_copy_default_26), kwargs = {})\n",
      "      %aten_mul_tensor_25 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_16, %aten_view_copy_default_26), kwargs = {})\n",
      "      %aten_mul_tensor_27 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_16, %aten_view_copy_default_25), kwargs = {})\n",
      "      %aten_slice_scatter_default_6 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_scatter.default](args = (%b_layers_1_attention_sdpa_kv_cache_v_cache, %aten_index_put_default_3, 1, 0, 9223372036854775807), kwargs = {})\n",
      "      %aten_sub_tensor_2 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sub.Tensor](args = (%aten_mul_tensor_20, %aten_mul_tensor_21), kwargs = {})\n",
      "      %aten_add_tensor_8 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mul_tensor_22, %aten_mul_tensor_23), kwargs = {})\n",
      "      %aten_sub_tensor_3 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sub.Tensor](args = (%aten_mul_tensor_24, %aten_mul_tensor_25), kwargs = {})\n",
      "      %aten_add_tensor_9 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mul_tensor_26, %aten_mul_tensor_27), kwargs = {})\n",
      "      %aten_slice_scatter_default_7 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_scatter.default](args = (%b_layers_1_attention_sdpa_kv_cache_v_cache, %aten_slice_scatter_default_6, 0, 0, 9223372036854775807), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_8 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_sub_tensor_2, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_9 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_add_tensor_8, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_10 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_sub_tensor_3, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_11 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_add_tensor_9, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_15 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_slice_scatter_default_7, 2), kwargs = {})\n",
      "      %aten_cat_default_2 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.cat.default](args = ([%aten_unsqueeze_copy_default_8, %aten_unsqueeze_copy_default_9], -1), kwargs = {})\n",
      "      %aten_cat_default_3 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.cat.default](args = ([%aten_unsqueeze_copy_default_10, %aten_unsqueeze_copy_default_11], -1), kwargs = {})\n",
      "      %aten_expand_copy_default_7 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten_unsqueeze_copy_default_15, [1, 6, 1, 128, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_27 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_cat_default_2, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_28 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_cat_default_3, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_clone_default_3 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.clone.default](args = (%aten_expand_copy_default_7,), kwargs = {memory_format: torch.contiguous_format})\n",
      "      %aten_permute_copy_default_5 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten_view_copy_default_27, [0, 2, 1, 3]), kwargs = {})\n",
      "      %aten_permute_copy_default_6 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten_view_copy_default_28, [0, 2, 1, 3]), kwargs = {})\n",
      "      %aten_view_copy_default_32 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_clone_default_3, [1, 6, 128, 48]), kwargs = {})\n",
      "      %aten_expand_copy_default_8 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten_permute_copy_default_5, [1, 6, 1, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_29 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_permute_copy_default_6, [6, 1, 48]), kwargs = {})\n",
      "      %aten_expand_copy_default_11 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten_view_copy_default_32, [1, 6, 128, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_33 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_expand_copy_default_8, [6, 1, 48]), kwargs = {})\n",
      "      %aten_index_put_default_2 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.index_put.default](args = (%b_layers_1_attention_sdpa_kv_cache_k_cache, [None, None, %input_pos], %aten_view_copy_default_29), kwargs = {})\n",
      "      %aten_view_copy_default_37 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_expand_copy_default_11, [6, 128, 48]), kwargs = {})\n",
      "      %aten_slice_scatter_default_4 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_scatter.default](args = (%b_layers_1_attention_sdpa_kv_cache_k_cache, %aten_index_put_default_2, 1, 0, 9223372036854775807), kwargs = {})\n",
      "      %aten_slice_scatter_default_5 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_scatter.default](args = (%b_layers_1_attention_sdpa_kv_cache_k_cache, %aten_slice_scatter_default_4, 0, 0, 9223372036854775807), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_14 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_slice_scatter_default_5, 2), kwargs = {})\n",
      "      %aten_expand_copy_default_6 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten_unsqueeze_copy_default_14, [1, 6, 1, 128, 48]), kwargs = {})\n",
      "      %aten_clone_default_2 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.clone.default](args = (%aten_expand_copy_default_6,), kwargs = {memory_format: torch.contiguous_format})\n",
      "      %aten_view_copy_default_31 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_clone_default_2, [1, 6, 128, 48]), kwargs = {})\n",
      "      %aten_permute_copy_default_8 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten_view_copy_default_31, [0, 1, 3, 2]), kwargs = {})\n",
      "      %aten_expand_copy_default_9 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten_permute_copy_default_8, [1, 6, 48, 128]), kwargs = {})\n",
      "      %aten_view_copy_default_34 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_expand_copy_default_9, [6, 48, 128]), kwargs = {})\n",
      "      %aten_bmm_default_2 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.bmm.default](args = (%aten_view_copy_default_33, %aten_view_copy_default_34), kwargs = {})\n",
      "      %aten_view_copy_default_35 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_bmm_default_2, [1, 6, 1, 128]), kwargs = {})\n",
      "      %aten_mul_tensor_28 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_view_copy_default_35, %_lifted_tensor_constant110), kwargs = {})\n",
      "      %aten_add_tensor_10 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mul_tensor_28, %aten__to_copy_default_1), kwargs = {})\n",
      "      %aten__softmax_default_1 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._softmax.default](args = (%aten_add_tensor_10, -1, False), kwargs = {})\n",
      "      %aten_expand_copy_default_10 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten__softmax_default_1, [1, 6, 1, 128]), kwargs = {})\n",
      "      %aten_view_copy_default_36 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_expand_copy_default_10, [6, 1, 128]), kwargs = {})\n",
      "      %aten_bmm_default_3 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.bmm.default](args = (%aten_view_copy_default_36, %aten_view_copy_default_37), kwargs = {})\n",
      "      %aten_view_copy_default_38 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_bmm_default_3, [1, 6, 1, 48]), kwargs = {})\n",
      "      %aten_permute_copy_default_9 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten_view_copy_default_38, [0, 2, 1, 3]), kwargs = {})\n",
      "      %aten_view_copy_default_39 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_permute_copy_default_9, [1, 1, 288]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_17 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_view_copy_default_39, [0]), kwargs = {})\n",
      "      return (aten_slice_scatter_default_7, aten_slice_scatter_default_5, aten_squeeze_copy_dims_17)\n",
      "  %executorch_call_delegate_5 : [num_users=3] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_5, %b_layers_1_attention_sdpa_kv_cache_v_cache, %b_layers_1_attention_sdpa_kv_cache_k_cache, %getitem_4, %getitem_5, %llama_cpp__weight_int8pack_mm_default_7, %llama_cpp__weight_int8pack_mm_default_8, %llama_cpp__weight_int8pack_mm_default_9, %input_pos), kwargs = {})\n",
      "  %getitem_17 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_5, 0), kwargs = {})\n",
      "  %getitem_18 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_5, 1), kwargs = {})\n",
      "  %getitem_19 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_5, 2), kwargs = {})\n",
      "  %alloc_10 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 288), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_10 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_19, %b_layers_1_attention_wo_weight, %b_layers_1_attention_wo_scales), kwargs = {out: %alloc_10})\n",
      "  %lowered_module_6 : [num_users=1] = get_attr[target=lowered_module_6]\n",
      "    backend_id: MPSBackend\n",
      "    lowered graph():\n",
      "      %p_layers_1_ffn_norm_weight : [num_users=1] = placeholder[target=p_layers_1_ffn_norm_weight]\n",
      "      %_lifted_tensor_constant111 : [num_users=1] = placeholder[target=_lifted_tensor_constant111]\n",
      "      %aten_add_tensor_6 : [num_users=1] = placeholder[target=aten_add_tensor_6]\n",
      "      %llama_cpp__weight_int8pack_mm_default_10 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_10]\n",
      "      %aten_add_tensor_11 : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_add_tensor_6, %llama_cpp__weight_int8pack_mm_default_10), kwargs = {})\n",
      "      %aten_mul_tensor_29 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_11, %aten_add_tensor_11), kwargs = {})\n",
      "      %aten_mean_dim_3 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mean.dim](args = (%aten_mul_tensor_29, [-1], True), kwargs = {})\n",
      "      %aten_add_tensor_12 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mean_dim_3, %_lifted_tensor_constant111), kwargs = {})\n",
      "      %aten_rsqrt_default_3 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.rsqrt.default](args = (%aten_add_tensor_12,), kwargs = {})\n",
      "      %aten_mul_tensor_30 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_11, %aten_rsqrt_default_3), kwargs = {})\n",
      "      %aten_mul_tensor_31 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_30, %p_layers_1_ffn_norm_weight), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_18 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_31, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_19 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_31, [0]), kwargs = {})\n",
      "      return (aten_add_tensor_11, aten_squeeze_copy_dims_18, aten_squeeze_copy_dims_19)\n",
      "  %executorch_call_delegate_6 : [num_users=3] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_6, %getitem_13, %llama_cpp__weight_int8pack_mm_default_10), kwargs = {})\n",
      "  %getitem_20 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_6, 0), kwargs = {})\n",
      "  %getitem_21 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_6, 1), kwargs = {})\n",
      "  %getitem_22 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_6, 2), kwargs = {})\n",
      "  %alloc_11 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_11 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_21, %b_layers_1_feed_forward_w1_weight, %b_layers_1_feed_forward_w1_scales), kwargs = {out: %alloc_11})\n",
      "  %alloc_12 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_12 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_22, %b_layers_1_feed_forward_w3_weight, %b_layers_1_feed_forward_w3_scales), kwargs = {out: %alloc_12})\n",
      "  %lowered_module_7 : [num_users=1] = get_attr[target=lowered_module_7]\n",
      "    backend_id: MPSBackend\n",
      "    lowered graph():\n",
      "      %llama_cpp__weight_int8pack_mm_default_11 : [num_users=2] = placeholder[target=llama_cpp__weight_int8pack_mm_default_11]\n",
      "      %llama_cpp__weight_int8pack_mm_default_12 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_12]\n",
      "      %aten_sigmoid_default_1 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sigmoid.default](args = (%llama_cpp__weight_int8pack_mm_default_11,), kwargs = {})\n",
      "      %aten_mul_tensor_32 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%llama_cpp__weight_int8pack_mm_default_11, %aten_sigmoid_default_1), kwargs = {})\n",
      "      %aten_mul_tensor_33 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_32, %llama_cpp__weight_int8pack_mm_default_12), kwargs = {})\n",
      "      return (aten_mul_tensor_33,)\n",
      "  %executorch_call_delegate_7 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_7, %llama_cpp__weight_int8pack_mm_default_11, %llama_cpp__weight_int8pack_mm_default_12), kwargs = {})\n",
      "  %getitem_23 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_7, 0), kwargs = {})\n",
      "  %alloc_13 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 288), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_13 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_23, %b_layers_1_feed_forward_w2_weight, %b_layers_1_feed_forward_w2_scales), kwargs = {out: %alloc_13})\n",
      "  %lowered_module_8 : [num_users=1] = get_attr[target=lowered_module_8]\n",
      "    backend_id: MPSBackend\n",
      "    lowered graph():\n",
      "      %p_layers_2_attention_norm_weight : [num_users=1] = placeholder[target=p_layers_2_attention_norm_weight]\n",
      "      %_lifted_tensor_constant112 : [num_users=1] = placeholder[target=_lifted_tensor_constant112]\n",
      "      %aten_add_tensor_11 : [num_users=1] = placeholder[target=aten_add_tensor_11]\n",
      "      %llama_cpp__weight_int8pack_mm_default_13 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_13]\n",
      "      %aten_add_tensor_13 : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_add_tensor_11, %llama_cpp__weight_int8pack_mm_default_13), kwargs = {})\n",
      "      %aten_mul_tensor_34 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_13, %aten_add_tensor_13), kwargs = {})\n",
      "      %aten_mean_dim_4 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mean.dim](args = (%aten_mul_tensor_34, [-1], True), kwargs = {})\n",
      "      %aten_add_tensor_14 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mean_dim_4, %_lifted_tensor_constant112), kwargs = {})\n",
      "      %aten_rsqrt_default_4 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.rsqrt.default](args = (%aten_add_tensor_14,), kwargs = {})\n",
      "      %aten_mul_tensor_35 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_13, %aten_rsqrt_default_4), kwargs = {})\n",
      "      %aten_mul_tensor_36 : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_35, %p_layers_2_attention_norm_weight), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_20 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_36, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_21 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_36, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_22 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_36, [0]), kwargs = {})\n",
      "      return (aten_add_tensor_13, aten_squeeze_copy_dims_20, aten_squeeze_copy_dims_21, aten_squeeze_copy_dims_22)\n",
      "  %executorch_call_delegate_8 : [num_users=4] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_8, %getitem_20, %llama_cpp__weight_int8pack_mm_default_13), kwargs = {})\n",
      "  %getitem_24 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_8, 0), kwargs = {})\n",
      "  %getitem_25 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_8, 1), kwargs = {})\n",
      "  %getitem_26 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_8, 2), kwargs = {})\n",
      "  %getitem_27 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_8, 3), kwargs = {})\n",
      "  %alloc_14 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 288), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_14 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_25, %b_layers_2_attention_wq_weight, %b_layers_2_attention_wq_scales), kwargs = {out: %alloc_14})\n",
      "  %alloc_15 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 288), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_15 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_26, %b_layers_2_attention_wk_weight, %b_layers_2_attention_wk_scales), kwargs = {out: %alloc_15})\n",
      "  %alloc_16 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 288), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_16 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_27, %b_layers_2_attention_wv_weight, %b_layers_2_attention_wv_scales), kwargs = {out: %alloc_16})\n",
      "  %lowered_module_9 : [num_users=1] = get_attr[target=lowered_module_9]\n",
      "    backend_id: MPSBackend\n",
      "    lowered graph():\n",
      "      %b_layers_2_attention_mask : [num_users=1] = placeholder[target=b_layers_2_attention_mask]\n",
      "      %_lifted_tensor_constant113 : [num_users=1] = placeholder[target=_lifted_tensor_constant113]\n",
      "      %b_layers_2_attention_sdpa_kv_cache_v_cache : [num_users=3] = placeholder[target=b_layers_2_attention_sdpa_kv_cache_v_cache]\n",
      "      %b_layers_2_attention_sdpa_kv_cache_k_cache : [num_users=3] = placeholder[target=b_layers_2_attention_sdpa_kv_cache_k_cache]\n",
      "      %aten_index_tensor : [num_users=1] = placeholder[target=aten_index_tensor]\n",
      "      %aten_index_tensor_1 : [num_users=1] = placeholder[target=aten_index_tensor_1]\n",
      "      %llama_cpp__weight_int8pack_mm_default_14 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_14]\n",
      "      %llama_cpp__weight_int8pack_mm_default_15 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_15]\n",
      "      %llama_cpp__weight_int8pack_mm_default_16 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_16]\n",
      "      %input_pos : [num_users=3] = placeholder[target=input_pos]\n",
      "      %aten_view_copy_default_45 : [num_users=4] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_index_tensor, [1, 1, 1, 24]), kwargs = {})\n",
      "      %aten_view_copy_default_46 : [num_users=4] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_index_tensor_1, [1, 1, 1, 24]), kwargs = {})\n",
      "      %aten_view_copy_default_40 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%llama_cpp__weight_int8pack_mm_default_14, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_41 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%llama_cpp__weight_int8pack_mm_default_15, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_42 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%llama_cpp__weight_int8pack_mm_default_16, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_20 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%b_layers_2_attention_mask, 0), kwargs = {})\n",
      "      %aten_view_copy_default_43 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_view_copy_default_40, [1, 1, 6, -1, 2]), kwargs = {})\n",
      "      %aten_view_copy_default_44 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_view_copy_default_41, [1, 1, 6, -1, 2]), kwargs = {})\n",
      "      %aten_permute_copy_default_12 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten_view_copy_default_42, [0, 2, 1, 3]), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_21 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_unsqueeze_copy_default_20, 1), kwargs = {})\n",
      "      %aten_slice_copy_tensor_8 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_43, 4, 0, 1), kwargs = {})\n",
      "      %aten_slice_copy_tensor_9 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_43, 4, 1, 2), kwargs = {})\n",
      "      %aten_slice_copy_tensor_10 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_44, 4, 0, 1), kwargs = {})\n",
      "      %aten_slice_copy_tensor_11 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_44, 4, 1, 2), kwargs = {})\n",
      "      %aten_view_copy_default_50 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_permute_copy_default_12, [6, 1, 48]), kwargs = {})\n",
      "      %aten_index_tensor_4 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.index.Tensor](args = (%aten_unsqueeze_copy_default_21, [None, None, %input_pos]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_23 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_8, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_24 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_9, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_25 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_10, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_26 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_11, [4]), kwargs = {})\n",
      "      %aten_index_put_default_5 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.index_put.default](args = (%b_layers_2_attention_sdpa_kv_cache_v_cache, [None, None, %input_pos], %aten_view_copy_default_50), kwargs = {})\n",
      "      %aten__to_copy_default_2 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%aten_index_tensor_4,), kwargs = {dtype: torch.float32})\n",
      "      %aten_mul_tensor_37 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_23, %aten_view_copy_default_45), kwargs = {})\n",
      "      %aten_mul_tensor_39 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_23, %aten_view_copy_default_46), kwargs = {})\n",
      "      %aten_mul_tensor_38 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_24, %aten_view_copy_default_46), kwargs = {})\n",
      "      %aten_mul_tensor_40 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_24, %aten_view_copy_default_45), kwargs = {})\n",
      "      %aten_mul_tensor_41 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_25, %aten_view_copy_default_45), kwargs = {})\n",
      "      %aten_mul_tensor_43 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_25, %aten_view_copy_default_46), kwargs = {})\n",
      "      %aten_mul_tensor_42 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_26, %aten_view_copy_default_46), kwargs = {})\n",
      "      %aten_mul_tensor_44 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_26, %aten_view_copy_default_45), kwargs = {})\n",
      "      %aten_slice_scatter_default_10 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_scatter.default](args = (%b_layers_2_attention_sdpa_kv_cache_v_cache, %aten_index_put_default_5, 1, 0, 9223372036854775807), kwargs = {})\n",
      "      %aten_sub_tensor_4 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sub.Tensor](args = (%aten_mul_tensor_37, %aten_mul_tensor_38), kwargs = {})\n",
      "      %aten_add_tensor_15 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mul_tensor_39, %aten_mul_tensor_40), kwargs = {})\n",
      "      %aten_sub_tensor_5 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sub.Tensor](args = (%aten_mul_tensor_41, %aten_mul_tensor_42), kwargs = {})\n",
      "      %aten_add_tensor_16 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mul_tensor_43, %aten_mul_tensor_44), kwargs = {})\n",
      "      %aten_slice_scatter_default_11 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_scatter.default](args = (%b_layers_2_attention_sdpa_kv_cache_v_cache, %aten_slice_scatter_default_10, 0, 0, 9223372036854775807), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_16 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_sub_tensor_4, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_17 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_add_tensor_15, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_18 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_sub_tensor_5, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_19 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_add_tensor_16, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_23 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_slice_scatter_default_11, 2), kwargs = {})\n",
      "      %aten_cat_default_4 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.cat.default](args = ([%aten_unsqueeze_copy_default_16, %aten_unsqueeze_copy_default_17], -1), kwargs = {})\n",
      "      %aten_cat_default_5 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.cat.default](args = ([%aten_unsqueeze_copy_default_18, %aten_unsqueeze_copy_default_19], -1), kwargs = {})\n",
      "      %aten_expand_copy_default_13 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten_unsqueeze_copy_default_23, [1, 6, 1, 128, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_47 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_cat_default_4, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_48 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_cat_default_5, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_clone_default_5 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.clone.default](args = (%aten_expand_copy_default_13,), kwargs = {memory_format: torch.contiguous_format})\n",
      "      %aten_permute_copy_default_10 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten_view_copy_default_47, [0, 2, 1, 3]), kwargs = {})\n",
      "      %aten_permute_copy_default_11 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten_view_copy_default_48, [0, 2, 1, 3]), kwargs = {})\n",
      "      %aten_view_copy_default_52 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_clone_default_5, [1, 6, 128, 48]), kwargs = {})\n",
      "      %aten_expand_copy_default_14 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten_permute_copy_default_10, [1, 6, 1, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_49 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_permute_copy_default_11, [6, 1, 48]), kwargs = {})\n",
      "      %aten_expand_copy_default_17 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten_view_copy_default_52, [1, 6, 128, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_53 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_expand_copy_default_14, [6, 1, 48]), kwargs = {})\n",
      "      %aten_index_put_default_4 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.index_put.default](args = (%b_layers_2_attention_sdpa_kv_cache_k_cache, [None, None, %input_pos], %aten_view_copy_default_49), kwargs = {})\n",
      "      %aten_view_copy_default_57 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_expand_copy_default_17, [6, 128, 48]), kwargs = {})\n",
      "      %aten_slice_scatter_default_8 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_scatter.default](args = (%b_layers_2_attention_sdpa_kv_cache_k_cache, %aten_index_put_default_4, 1, 0, 9223372036854775807), kwargs = {})\n",
      "      %aten_slice_scatter_default_9 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_scatter.default](args = (%b_layers_2_attention_sdpa_kv_cache_k_cache, %aten_slice_scatter_default_8, 0, 0, 9223372036854775807), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_22 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_slice_scatter_default_9, 2), kwargs = {})\n",
      "      %aten_expand_copy_default_12 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten_unsqueeze_copy_default_22, [1, 6, 1, 128, 48]), kwargs = {})\n",
      "      %aten_clone_default_4 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.clone.default](args = (%aten_expand_copy_default_12,), kwargs = {memory_format: torch.contiguous_format})\n",
      "      %aten_view_copy_default_51 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_clone_default_4, [1, 6, 128, 48]), kwargs = {})\n",
      "      %aten_permute_copy_default_13 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten_view_copy_default_51, [0, 1, 3, 2]), kwargs = {})\n",
      "      %aten_expand_copy_default_15 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten_permute_copy_default_13, [1, 6, 48, 128]), kwargs = {})\n",
      "      %aten_view_copy_default_54 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_expand_copy_default_15, [6, 48, 128]), kwargs = {})\n",
      "      %aten_bmm_default_4 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.bmm.default](args = (%aten_view_copy_default_53, %aten_view_copy_default_54), kwargs = {})\n",
      "      %aten_view_copy_default_55 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_bmm_default_4, [1, 6, 1, 128]), kwargs = {})\n",
      "      %aten_mul_tensor_45 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_view_copy_default_55, %_lifted_tensor_constant113), kwargs = {})\n",
      "      %aten_add_tensor_17 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mul_tensor_45, %aten__to_copy_default_2), kwargs = {})\n",
      "      %aten__softmax_default_2 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._softmax.default](args = (%aten_add_tensor_17, -1, False), kwargs = {})\n",
      "      %aten_expand_copy_default_16 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten__softmax_default_2, [1, 6, 1, 128]), kwargs = {})\n",
      "      %aten_view_copy_default_56 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_expand_copy_default_16, [6, 1, 128]), kwargs = {})\n",
      "      %aten_bmm_default_5 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.bmm.default](args = (%aten_view_copy_default_56, %aten_view_copy_default_57), kwargs = {})\n",
      "      %aten_view_copy_default_58 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_bmm_default_5, [1, 6, 1, 48]), kwargs = {})\n",
      "      %aten_permute_copy_default_14 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten_view_copy_default_58, [0, 2, 1, 3]), kwargs = {})\n",
      "      %aten_view_copy_default_59 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_permute_copy_default_14, [1, 1, 288]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_27 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_view_copy_default_59, [0]), kwargs = {})\n",
      "      return (aten_slice_scatter_default_11, aten_slice_scatter_default_9, aten_squeeze_copy_dims_27)\n",
      "  %executorch_call_delegate_9 : [num_users=3] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_9, %b_layers_2_attention_sdpa_kv_cache_v_cache, %b_layers_2_attention_sdpa_kv_cache_k_cache, %getitem_4, %getitem_5, %llama_cpp__weight_int8pack_mm_default_14, %llama_cpp__weight_int8pack_mm_default_15, %llama_cpp__weight_int8pack_mm_default_16, %input_pos), kwargs = {})\n",
      "  %getitem_28 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_9, 0), kwargs = {})\n",
      "  %getitem_29 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_9, 1), kwargs = {})\n",
      "  %getitem_30 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_9, 2), kwargs = {})\n",
      "  %alloc_17 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 288), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_17 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_30, %b_layers_2_attention_wo_weight, %b_layers_2_attention_wo_scales), kwargs = {out: %alloc_17})\n",
      "  %lowered_module_10 : [num_users=1] = get_attr[target=lowered_module_10]\n",
      "    backend_id: MPSBackend\n",
      "    lowered graph():\n",
      "      %p_layers_2_ffn_norm_weight : [num_users=1] = placeholder[target=p_layers_2_ffn_norm_weight]\n",
      "      %_lifted_tensor_constant114 : [num_users=1] = placeholder[target=_lifted_tensor_constant114]\n",
      "      %aten_add_tensor_13 : [num_users=1] = placeholder[target=aten_add_tensor_13]\n",
      "      %llama_cpp__weight_int8pack_mm_default_17 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_17]\n",
      "      %aten_add_tensor_18 : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_add_tensor_13, %llama_cpp__weight_int8pack_mm_default_17), kwargs = {})\n",
      "      %aten_mul_tensor_46 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_18, %aten_add_tensor_18), kwargs = {})\n",
      "      %aten_mean_dim_5 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mean.dim](args = (%aten_mul_tensor_46, [-1], True), kwargs = {})\n",
      "      %aten_add_tensor_19 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mean_dim_5, %_lifted_tensor_constant114), kwargs = {})\n",
      "      %aten_rsqrt_default_5 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.rsqrt.default](args = (%aten_add_tensor_19,), kwargs = {})\n",
      "      %aten_mul_tensor_47 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_18, %aten_rsqrt_default_5), kwargs = {})\n",
      "      %aten_mul_tensor_48 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_47, %p_layers_2_ffn_norm_weight), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_28 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_48, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_29 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_48, [0]), kwargs = {})\n",
      "      return (aten_add_tensor_18, aten_squeeze_copy_dims_28, aten_squeeze_copy_dims_29)\n",
      "  %executorch_call_delegate_10 : [num_users=3] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_10, %getitem_24, %llama_cpp__weight_int8pack_mm_default_17), kwargs = {})\n",
      "  %getitem_31 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_10, 0), kwargs = {})\n",
      "  %getitem_32 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_10, 1), kwargs = {})\n",
      "  %getitem_33 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_10, 2), kwargs = {})\n",
      "  %alloc_18 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_18 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_32, %b_layers_2_feed_forward_w1_weight, %b_layers_2_feed_forward_w1_scales), kwargs = {out: %alloc_18})\n",
      "  %alloc_19 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_19 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_33, %b_layers_2_feed_forward_w3_weight, %b_layers_2_feed_forward_w3_scales), kwargs = {out: %alloc_19})\n",
      "  %lowered_module_11 : [num_users=1] = get_attr[target=lowered_module_11]\n",
      "    backend_id: MPSBackend\n",
      "    lowered graph():\n",
      "      %llama_cpp__weight_int8pack_mm_default_18 : [num_users=2] = placeholder[target=llama_cpp__weight_int8pack_mm_default_18]\n",
      "      %llama_cpp__weight_int8pack_mm_default_19 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_19]\n",
      "      %aten_sigmoid_default_2 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sigmoid.default](args = (%llama_cpp__weight_int8pack_mm_default_18,), kwargs = {})\n",
      "      %aten_mul_tensor_49 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%llama_cpp__weight_int8pack_mm_default_18, %aten_sigmoid_default_2), kwargs = {})\n",
      "      %aten_mul_tensor_50 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_49, %llama_cpp__weight_int8pack_mm_default_19), kwargs = {})\n",
      "      return (aten_mul_tensor_50,)\n",
      "  %executorch_call_delegate_11 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_11, %llama_cpp__weight_int8pack_mm_default_18, %llama_cpp__weight_int8pack_mm_default_19), kwargs = {})\n",
      "  %getitem_34 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_11, 0), kwargs = {})\n",
      "  %alloc_20 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 288), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_20 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_34, %b_layers_2_feed_forward_w2_weight, %b_layers_2_feed_forward_w2_scales), kwargs = {out: %alloc_20})\n",
      "  %lowered_module_12 : [num_users=1] = get_attr[target=lowered_module_12]\n",
      "    backend_id: MPSBackend\n",
      "    lowered graph():\n",
      "      %p_layers_3_attention_norm_weight : [num_users=1] = placeholder[target=p_layers_3_attention_norm_weight]\n",
      "      %_lifted_tensor_constant115 : [num_users=1] = placeholder[target=_lifted_tensor_constant115]\n",
      "      %aten_add_tensor_18 : [num_users=1] = placeholder[target=aten_add_tensor_18]\n",
      "      %llama_cpp__weight_int8pack_mm_default_20 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_20]\n",
      "      %aten_add_tensor_20 : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_add_tensor_18, %llama_cpp__weight_int8pack_mm_default_20), kwargs = {})\n",
      "      %aten_mul_tensor_51 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_20, %aten_add_tensor_20), kwargs = {})\n",
      "      %aten_mean_dim_6 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mean.dim](args = (%aten_mul_tensor_51, [-1], True), kwargs = {})\n",
      "      %aten_add_tensor_21 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mean_dim_6, %_lifted_tensor_constant115), kwargs = {})\n",
      "      %aten_rsqrt_default_6 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.rsqrt.default](args = (%aten_add_tensor_21,), kwargs = {})\n",
      "      %aten_mul_tensor_52 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_20, %aten_rsqrt_default_6), kwargs = {})\n",
      "      %aten_mul_tensor_53 : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_52, %p_layers_3_attention_norm_weight), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_30 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_53, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_31 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_53, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_32 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_53, [0]), kwargs = {})\n",
      "      return (aten_add_tensor_20, aten_squeeze_copy_dims_30, aten_squeeze_copy_dims_31, aten_squeeze_copy_dims_32)\n",
      "  %executorch_call_delegate_12 : [num_users=4] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_12, %getitem_31, %llama_cpp__weight_int8pack_mm_default_20), kwargs = {})\n",
      "  %getitem_35 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_12, 0), kwargs = {})\n",
      "  %getitem_36 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_12, 1), kwargs = {})\n",
      "  %getitem_37 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_12, 2), kwargs = {})\n",
      "  %getitem_38 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_12, 3), kwargs = {})\n",
      "  %alloc_21 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 288), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_21 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_36, %b_layers_3_attention_wq_weight, %b_layers_3_attention_wq_scales), kwargs = {out: %alloc_21})\n",
      "  %alloc_22 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 288), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_22 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_37, %b_layers_3_attention_wk_weight, %b_layers_3_attention_wk_scales), kwargs = {out: %alloc_22})\n",
      "  %alloc_23 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 288), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_23 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_38, %b_layers_3_attention_wv_weight, %b_layers_3_attention_wv_scales), kwargs = {out: %alloc_23})\n",
      "  %lowered_module_13 : [num_users=1] = get_attr[target=lowered_module_13]\n",
      "    backend_id: MPSBackend\n",
      "    lowered graph():\n",
      "      %b_layers_3_attention_mask : [num_users=1] = placeholder[target=b_layers_3_attention_mask]\n",
      "      %_lifted_tensor_constant116 : [num_users=1] = placeholder[target=_lifted_tensor_constant116]\n",
      "      %b_layers_3_attention_sdpa_kv_cache_v_cache : [num_users=3] = placeholder[target=b_layers_3_attention_sdpa_kv_cache_v_cache]\n",
      "      %b_layers_3_attention_sdpa_kv_cache_k_cache : [num_users=3] = placeholder[target=b_layers_3_attention_sdpa_kv_cache_k_cache]\n",
      "      %aten_index_tensor : [num_users=1] = placeholder[target=aten_index_tensor]\n",
      "      %aten_index_tensor_1 : [num_users=1] = placeholder[target=aten_index_tensor_1]\n",
      "      %llama_cpp__weight_int8pack_mm_default_21 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_21]\n",
      "      %llama_cpp__weight_int8pack_mm_default_22 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_22]\n",
      "      %llama_cpp__weight_int8pack_mm_default_23 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_23]\n",
      "      %input_pos : [num_users=3] = placeholder[target=input_pos]\n",
      "      %aten_view_copy_default_65 : [num_users=4] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_index_tensor, [1, 1, 1, 24]), kwargs = {})\n",
      "      %aten_view_copy_default_66 : [num_users=4] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_index_tensor_1, [1, 1, 1, 24]), kwargs = {})\n",
      "      %aten_view_copy_default_60 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%llama_cpp__weight_int8pack_mm_default_21, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_61 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%llama_cpp__weight_int8pack_mm_default_22, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_62 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%llama_cpp__weight_int8pack_mm_default_23, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_28 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%b_layers_3_attention_mask, 0), kwargs = {})\n",
      "      %aten_view_copy_default_63 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_view_copy_default_60, [1, 1, 6, -1, 2]), kwargs = {})\n",
      "      %aten_view_copy_default_64 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_view_copy_default_61, [1, 1, 6, -1, 2]), kwargs = {})\n",
      "      %aten_permute_copy_default_17 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten_view_copy_default_62, [0, 2, 1, 3]), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_29 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_unsqueeze_copy_default_28, 1), kwargs = {})\n",
      "      %aten_slice_copy_tensor_12 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_63, 4, 0, 1), kwargs = {})\n",
      "      %aten_slice_copy_tensor_13 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_63, 4, 1, 2), kwargs = {})\n",
      "      %aten_slice_copy_tensor_14 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_64, 4, 0, 1), kwargs = {})\n",
      "      %aten_slice_copy_tensor_15 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_64, 4, 1, 2), kwargs = {})\n",
      "      %aten_view_copy_default_70 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_permute_copy_default_17, [6, 1, 48]), kwargs = {})\n",
      "      %aten_index_tensor_5 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.index.Tensor](args = (%aten_unsqueeze_copy_default_29, [None, None, %input_pos]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_33 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_12, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_34 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_13, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_35 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_14, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_36 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_15, [4]), kwargs = {})\n",
      "      %aten_index_put_default_7 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.index_put.default](args = (%b_layers_3_attention_sdpa_kv_cache_v_cache, [None, None, %input_pos], %aten_view_copy_default_70), kwargs = {})\n",
      "      %aten__to_copy_default_3 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%aten_index_tensor_5,), kwargs = {dtype: torch.float32})\n",
      "      %aten_mul_tensor_54 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_33, %aten_view_copy_default_65), kwargs = {})\n",
      "      %aten_mul_tensor_56 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_33, %aten_view_copy_default_66), kwargs = {})\n",
      "      %aten_mul_tensor_55 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_34, %aten_view_copy_default_66), kwargs = {})\n",
      "      %aten_mul_tensor_57 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_34, %aten_view_copy_default_65), kwargs = {})\n",
      "      %aten_mul_tensor_58 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_35, %aten_view_copy_default_65), kwargs = {})\n",
      "      %aten_mul_tensor_60 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_35, %aten_view_copy_default_66), kwargs = {})\n",
      "      %aten_mul_tensor_59 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_36, %aten_view_copy_default_66), kwargs = {})\n",
      "      %aten_mul_tensor_61 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_36, %aten_view_copy_default_65), kwargs = {})\n",
      "      %aten_slice_scatter_default_14 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_scatter.default](args = (%b_layers_3_attention_sdpa_kv_cache_v_cache, %aten_index_put_default_7, 1, 0, 9223372036854775807), kwargs = {})\n",
      "      %aten_sub_tensor_6 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sub.Tensor](args = (%aten_mul_tensor_54, %aten_mul_tensor_55), kwargs = {})\n",
      "      %aten_add_tensor_22 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mul_tensor_56, %aten_mul_tensor_57), kwargs = {})\n",
      "      %aten_sub_tensor_7 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sub.Tensor](args = (%aten_mul_tensor_58, %aten_mul_tensor_59), kwargs = {})\n",
      "      %aten_add_tensor_23 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mul_tensor_60, %aten_mul_tensor_61), kwargs = {})\n",
      "      %aten_slice_scatter_default_15 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_scatter.default](args = (%b_layers_3_attention_sdpa_kv_cache_v_cache, %aten_slice_scatter_default_14, 0, 0, 9223372036854775807), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_24 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_sub_tensor_6, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_25 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_add_tensor_22, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_26 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_sub_tensor_7, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_27 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_add_tensor_23, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_31 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_slice_scatter_default_15, 2), kwargs = {})\n",
      "      %aten_cat_default_6 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.cat.default](args = ([%aten_unsqueeze_copy_default_24, %aten_unsqueeze_copy_default_25], -1), kwargs = {})\n",
      "      %aten_cat_default_7 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.cat.default](args = ([%aten_unsqueeze_copy_default_26, %aten_unsqueeze_copy_default_27], -1), kwargs = {})\n",
      "      %aten_expand_copy_default_19 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten_unsqueeze_copy_default_31, [1, 6, 1, 128, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_67 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_cat_default_6, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_68 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_cat_default_7, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_clone_default_7 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.clone.default](args = (%aten_expand_copy_default_19,), kwargs = {memory_format: torch.contiguous_format})\n",
      "      %aten_permute_copy_default_15 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten_view_copy_default_67, [0, 2, 1, 3]), kwargs = {})\n",
      "      %aten_permute_copy_default_16 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten_view_copy_default_68, [0, 2, 1, 3]), kwargs = {})\n",
      "      %aten_view_copy_default_72 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_clone_default_7, [1, 6, 128, 48]), kwargs = {})\n",
      "      %aten_expand_copy_default_20 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten_permute_copy_default_15, [1, 6, 1, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_69 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_permute_copy_default_16, [6, 1, 48]), kwargs = {})\n",
      "      %aten_expand_copy_default_23 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten_view_copy_default_72, [1, 6, 128, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_73 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_expand_copy_default_20, [6, 1, 48]), kwargs = {})\n",
      "      %aten_index_put_default_6 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.index_put.default](args = (%b_layers_3_attention_sdpa_kv_cache_k_cache, [None, None, %input_pos], %aten_view_copy_default_69), kwargs = {})\n",
      "      %aten_view_copy_default_77 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_expand_copy_default_23, [6, 128, 48]), kwargs = {})\n",
      "      %aten_slice_scatter_default_12 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_scatter.default](args = (%b_layers_3_attention_sdpa_kv_cache_k_cache, %aten_index_put_default_6, 1, 0, 9223372036854775807), kwargs = {})\n",
      "      %aten_slice_scatter_default_13 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_scatter.default](args = (%b_layers_3_attention_sdpa_kv_cache_k_cache, %aten_slice_scatter_default_12, 0, 0, 9223372036854775807), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_30 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_slice_scatter_default_13, 2), kwargs = {})\n",
      "      %aten_expand_copy_default_18 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten_unsqueeze_copy_default_30, [1, 6, 1, 128, 48]), kwargs = {})\n",
      "      %aten_clone_default_6 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.clone.default](args = (%aten_expand_copy_default_18,), kwargs = {memory_format: torch.contiguous_format})\n",
      "      %aten_view_copy_default_71 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_clone_default_6, [1, 6, 128, 48]), kwargs = {})\n",
      "      %aten_permute_copy_default_18 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten_view_copy_default_71, [0, 1, 3, 2]), kwargs = {})\n",
      "      %aten_expand_copy_default_21 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten_permute_copy_default_18, [1, 6, 48, 128]), kwargs = {})\n",
      "      %aten_view_copy_default_74 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_expand_copy_default_21, [6, 48, 128]), kwargs = {})\n",
      "      %aten_bmm_default_6 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.bmm.default](args = (%aten_view_copy_default_73, %aten_view_copy_default_74), kwargs = {})\n",
      "      %aten_view_copy_default_75 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_bmm_default_6, [1, 6, 1, 128]), kwargs = {})\n",
      "      %aten_mul_tensor_62 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_view_copy_default_75, %_lifted_tensor_constant116), kwargs = {})\n",
      "      %aten_add_tensor_24 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mul_tensor_62, %aten__to_copy_default_3), kwargs = {})\n",
      "      %aten__softmax_default_3 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._softmax.default](args = (%aten_add_tensor_24, -1, False), kwargs = {})\n",
      "      %aten_expand_copy_default_22 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten__softmax_default_3, [1, 6, 1, 128]), kwargs = {})\n",
      "      %aten_view_copy_default_76 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_expand_copy_default_22, [6, 1, 128]), kwargs = {})\n",
      "      %aten_bmm_default_7 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.bmm.default](args = (%aten_view_copy_default_76, %aten_view_copy_default_77), kwargs = {})\n",
      "      %aten_view_copy_default_78 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_bmm_default_7, [1, 6, 1, 48]), kwargs = {})\n",
      "      %aten_permute_copy_default_19 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten_view_copy_default_78, [0, 2, 1, 3]), kwargs = {})\n",
      "      %aten_view_copy_default_79 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_permute_copy_default_19, [1, 1, 288]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_37 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_view_copy_default_79, [0]), kwargs = {})\n",
      "      return (aten_slice_scatter_default_15, aten_slice_scatter_default_13, aten_squeeze_copy_dims_37)\n",
      "  %executorch_call_delegate_13 : [num_users=3] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_13, %b_layers_3_attention_sdpa_kv_cache_v_cache, %b_layers_3_attention_sdpa_kv_cache_k_cache, %getitem_4, %getitem_5, %llama_cpp__weight_int8pack_mm_default_21, %llama_cpp__weight_int8pack_mm_default_22, %llama_cpp__weight_int8pack_mm_default_23, %input_pos), kwargs = {})\n",
      "  %getitem_39 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_13, 0), kwargs = {})\n",
      "  %getitem_40 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_13, 1), kwargs = {})\n",
      "  %getitem_41 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_13, 2), kwargs = {})\n",
      "  %alloc_24 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 288), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_24 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_41, %b_layers_3_attention_wo_weight, %b_layers_3_attention_wo_scales), kwargs = {out: %alloc_24})\n",
      "  %lowered_module_14 : [num_users=1] = get_attr[target=lowered_module_14]\n",
      "    backend_id: MPSBackend\n",
      "    lowered graph():\n",
      "      %p_layers_3_ffn_norm_weight : [num_users=1] = placeholder[target=p_layers_3_ffn_norm_weight]\n",
      "      %_lifted_tensor_constant117 : [num_users=1] = placeholder[target=_lifted_tensor_constant117]\n",
      "      %aten_add_tensor_20 : [num_users=1] = placeholder[target=aten_add_tensor_20]\n",
      "      %llama_cpp__weight_int8pack_mm_default_24 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_24]\n",
      "      %aten_add_tensor_25 : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_add_tensor_20, %llama_cpp__weight_int8pack_mm_default_24), kwargs = {})\n",
      "      %aten_mul_tensor_63 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_25, %aten_add_tensor_25), kwargs = {})\n",
      "      %aten_mean_dim_7 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mean.dim](args = (%aten_mul_tensor_63, [-1], True), kwargs = {})\n",
      "      %aten_add_tensor_26 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mean_dim_7, %_lifted_tensor_constant117), kwargs = {})\n",
      "      %aten_rsqrt_default_7 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.rsqrt.default](args = (%aten_add_tensor_26,), kwargs = {})\n",
      "      %aten_mul_tensor_64 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_25, %aten_rsqrt_default_7), kwargs = {})\n",
      "      %aten_mul_tensor_65 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_64, %p_layers_3_ffn_norm_weight), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_38 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_65, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_39 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_65, [0]), kwargs = {})\n",
      "      return (aten_add_tensor_25, aten_squeeze_copy_dims_38, aten_squeeze_copy_dims_39)\n",
      "  %executorch_call_delegate_14 : [num_users=3] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_14, %getitem_35, %llama_cpp__weight_int8pack_mm_default_24), kwargs = {})\n",
      "  %getitem_42 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_14, 0), kwargs = {})\n",
      "  %getitem_43 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_14, 1), kwargs = {})\n",
      "  %getitem_44 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_14, 2), kwargs = {})\n",
      "  %alloc_25 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_25 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_43, %b_layers_3_feed_forward_w1_weight, %b_layers_3_feed_forward_w1_scales), kwargs = {out: %alloc_25})\n",
      "  %alloc_26 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_26 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_44, %b_layers_3_feed_forward_w3_weight, %b_layers_3_feed_forward_w3_scales), kwargs = {out: %alloc_26})\n",
      "  %lowered_module_15 : [num_users=1] = get_attr[target=lowered_module_15]\n",
      "    backend_id: MPSBackend\n",
      "    lowered graph():\n",
      "      %llama_cpp__weight_int8pack_mm_default_25 : [num_users=2] = placeholder[target=llama_cpp__weight_int8pack_mm_default_25]\n",
      "      %llama_cpp__weight_int8pack_mm_default_26 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_26]\n",
      "      %aten_sigmoid_default_3 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sigmoid.default](args = (%llama_cpp__weight_int8pack_mm_default_25,), kwargs = {})\n",
      "      %aten_mul_tensor_66 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%llama_cpp__weight_int8pack_mm_default_25, %aten_sigmoid_default_3), kwargs = {})\n",
      "      %aten_mul_tensor_67 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_66, %llama_cpp__weight_int8pack_mm_default_26), kwargs = {})\n",
      "      return (aten_mul_tensor_67,)\n",
      "  %executorch_call_delegate_15 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_15, %llama_cpp__weight_int8pack_mm_default_25, %llama_cpp__weight_int8pack_mm_default_26), kwargs = {})\n",
      "  %getitem_45 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_15, 0), kwargs = {})\n",
      "  %alloc_27 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 288), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_27 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_45, %b_layers_3_feed_forward_w2_weight, %b_layers_3_feed_forward_w2_scales), kwargs = {out: %alloc_27})\n",
      "  %lowered_module_16 : [num_users=1] = get_attr[target=lowered_module_16]\n",
      "    backend_id: MPSBackend\n",
      "    lowered graph():\n",
      "      %p_layers_4_attention_norm_weight : [num_users=1] = placeholder[target=p_layers_4_attention_norm_weight]\n",
      "      %_lifted_tensor_constant118 : [num_users=1] = placeholder[target=_lifted_tensor_constant118]\n",
      "      %aten_add_tensor_25 : [num_users=1] = placeholder[target=aten_add_tensor_25]\n",
      "      %llama_cpp__weight_int8pack_mm_default_27 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_27]\n",
      "      %aten_add_tensor_27 : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_add_tensor_25, %llama_cpp__weight_int8pack_mm_default_27), kwargs = {})\n",
      "      %aten_mul_tensor_68 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_27, %aten_add_tensor_27), kwargs = {})\n",
      "      %aten_mean_dim_8 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mean.dim](args = (%aten_mul_tensor_68, [-1], True), kwargs = {})\n",
      "      %aten_add_tensor_28 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mean_dim_8, %_lifted_tensor_constant118), kwargs = {})\n",
      "      %aten_rsqrt_default_8 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.rsqrt.default](args = (%aten_add_tensor_28,), kwargs = {})\n",
      "      %aten_mul_tensor_69 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_27, %aten_rsqrt_default_8), kwargs = {})\n",
      "      %aten_mul_tensor_70 : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_69, %p_layers_4_attention_norm_weight), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_40 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_70, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_41 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_70, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_42 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_70, [0]), kwargs = {})\n",
      "      return (aten_add_tensor_27, aten_squeeze_copy_dims_40, aten_squeeze_copy_dims_41, aten_squeeze_copy_dims_42)\n",
      "  %executorch_call_delegate_16 : [num_users=4] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_16, %getitem_42, %llama_cpp__weight_int8pack_mm_default_27), kwargs = {})\n",
      "  %getitem_46 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_16, 0), kwargs = {})\n",
      "  %getitem_47 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_16, 1), kwargs = {})\n",
      "  %getitem_48 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_16, 2), kwargs = {})\n",
      "  %getitem_49 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_16, 3), kwargs = {})\n",
      "  %alloc_28 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 288), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_28 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_47, %b_layers_4_attention_wq_weight, %b_layers_4_attention_wq_scales), kwargs = {out: %alloc_28})\n",
      "  %alloc_29 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 288), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_29 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_48, %b_layers_4_attention_wk_weight, %b_layers_4_attention_wk_scales), kwargs = {out: %alloc_29})\n",
      "  %alloc_30 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 288), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_30 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_49, %b_layers_4_attention_wv_weight, %b_layers_4_attention_wv_scales), kwargs = {out: %alloc_30})\n",
      "  %lowered_module_17 : [num_users=1] = get_attr[target=lowered_module_17]\n",
      "    backend_id: MPSBackend\n",
      "    lowered graph():\n",
      "      %b_layers_4_attention_mask : [num_users=1] = placeholder[target=b_layers_4_attention_mask]\n",
      "      %_lifted_tensor_constant119 : [num_users=1] = placeholder[target=_lifted_tensor_constant119]\n",
      "      %b_layers_4_attention_sdpa_kv_cache_v_cache : [num_users=3] = placeholder[target=b_layers_4_attention_sdpa_kv_cache_v_cache]\n",
      "      %b_layers_4_attention_sdpa_kv_cache_k_cache : [num_users=3] = placeholder[target=b_layers_4_attention_sdpa_kv_cache_k_cache]\n",
      "      %aten_index_tensor : [num_users=1] = placeholder[target=aten_index_tensor]\n",
      "      %aten_index_tensor_1 : [num_users=1] = placeholder[target=aten_index_tensor_1]\n",
      "      %llama_cpp__weight_int8pack_mm_default_28 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_28]\n",
      "      %llama_cpp__weight_int8pack_mm_default_29 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_29]\n",
      "      %llama_cpp__weight_int8pack_mm_default_30 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_30]\n",
      "      %input_pos : [num_users=3] = placeholder[target=input_pos]\n",
      "      %aten_view_copy_default_85 : [num_users=4] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_index_tensor, [1, 1, 1, 24]), kwargs = {})\n",
      "      %aten_view_copy_default_86 : [num_users=4] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_index_tensor_1, [1, 1, 1, 24]), kwargs = {})\n",
      "      %aten_view_copy_default_80 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%llama_cpp__weight_int8pack_mm_default_28, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_81 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%llama_cpp__weight_int8pack_mm_default_29, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_82 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%llama_cpp__weight_int8pack_mm_default_30, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_36 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%b_layers_4_attention_mask, 0), kwargs = {})\n",
      "      %aten_view_copy_default_83 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_view_copy_default_80, [1, 1, 6, -1, 2]), kwargs = {})\n",
      "      %aten_view_copy_default_84 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_view_copy_default_81, [1, 1, 6, -1, 2]), kwargs = {})\n",
      "      %aten_permute_copy_default_22 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten_view_copy_default_82, [0, 2, 1, 3]), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_37 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_unsqueeze_copy_default_36, 1), kwargs = {})\n",
      "      %aten_slice_copy_tensor_16 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_83, 4, 0, 1), kwargs = {})\n",
      "      %aten_slice_copy_tensor_17 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_83, 4, 1, 2), kwargs = {})\n",
      "      %aten_slice_copy_tensor_18 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_84, 4, 0, 1), kwargs = {})\n",
      "      %aten_slice_copy_tensor_19 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_84, 4, 1, 2), kwargs = {})\n",
      "      %aten_view_copy_default_90 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_permute_copy_default_22, [6, 1, 48]), kwargs = {})\n",
      "      %aten_index_tensor_6 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.index.Tensor](args = (%aten_unsqueeze_copy_default_37, [None, None, %input_pos]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_43 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_16, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_44 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_17, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_45 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_18, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_46 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_19, [4]), kwargs = {})\n",
      "      %aten_index_put_default_9 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.index_put.default](args = (%b_layers_4_attention_sdpa_kv_cache_v_cache, [None, None, %input_pos], %aten_view_copy_default_90), kwargs = {})\n",
      "      %aten__to_copy_default_4 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%aten_index_tensor_6,), kwargs = {dtype: torch.float32})\n",
      "      %aten_mul_tensor_71 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_43, %aten_view_copy_default_85), kwargs = {})\n",
      "      %aten_mul_tensor_73 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_43, %aten_view_copy_default_86), kwargs = {})\n",
      "      %aten_mul_tensor_72 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_44, %aten_view_copy_default_86), kwargs = {})\n",
      "      %aten_mul_tensor_74 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_44, %aten_view_copy_default_85), kwargs = {})\n",
      "      %aten_mul_tensor_75 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_45, %aten_view_copy_default_85), kwargs = {})\n",
      "      %aten_mul_tensor_77 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_45, %aten_view_copy_default_86), kwargs = {})\n",
      "      %aten_mul_tensor_76 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_46, %aten_view_copy_default_86), kwargs = {})\n",
      "      %aten_mul_tensor_78 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_46, %aten_view_copy_default_85), kwargs = {})\n",
      "      %aten_slice_scatter_default_18 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_scatter.default](args = (%b_layers_4_attention_sdpa_kv_cache_v_cache, %aten_index_put_default_9, 1, 0, 9223372036854775807), kwargs = {})\n",
      "      %aten_sub_tensor_8 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sub.Tensor](args = (%aten_mul_tensor_71, %aten_mul_tensor_72), kwargs = {})\n",
      "      %aten_add_tensor_29 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mul_tensor_73, %aten_mul_tensor_74), kwargs = {})\n",
      "      %aten_sub_tensor_9 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sub.Tensor](args = (%aten_mul_tensor_75, %aten_mul_tensor_76), kwargs = {})\n",
      "      %aten_add_tensor_30 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mul_tensor_77, %aten_mul_tensor_78), kwargs = {})\n",
      "      %aten_slice_scatter_default_19 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_scatter.default](args = (%b_layers_4_attention_sdpa_kv_cache_v_cache, %aten_slice_scatter_default_18, 0, 0, 9223372036854775807), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_32 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_sub_tensor_8, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_33 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_add_tensor_29, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_34 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_sub_tensor_9, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_35 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_add_tensor_30, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_39 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_slice_scatter_default_19, 2), kwargs = {})\n",
      "      %aten_cat_default_8 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.cat.default](args = ([%aten_unsqueeze_copy_default_32, %aten_unsqueeze_copy_default_33], -1), kwargs = {})\n",
      "      %aten_cat_default_9 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.cat.default](args = ([%aten_unsqueeze_copy_default_34, %aten_unsqueeze_copy_default_35], -1), kwargs = {})\n",
      "      %aten_expand_copy_default_25 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten_unsqueeze_copy_default_39, [1, 6, 1, 128, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_87 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_cat_default_8, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_88 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_cat_default_9, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_clone_default_9 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.clone.default](args = (%aten_expand_copy_default_25,), kwargs = {memory_format: torch.contiguous_format})\n",
      "      %aten_permute_copy_default_20 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten_view_copy_default_87, [0, 2, 1, 3]), kwargs = {})\n",
      "      %aten_permute_copy_default_21 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten_view_copy_default_88, [0, 2, 1, 3]), kwargs = {})\n",
      "      %aten_view_copy_default_92 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_clone_default_9, [1, 6, 128, 48]), kwargs = {})\n",
      "      %aten_expand_copy_default_26 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten_permute_copy_default_20, [1, 6, 1, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_89 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_permute_copy_default_21, [6, 1, 48]), kwargs = {})\n",
      "      %aten_expand_copy_default_29 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten_view_copy_default_92, [1, 6, 128, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_93 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_expand_copy_default_26, [6, 1, 48]), kwargs = {})\n",
      "      %aten_index_put_default_8 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.index_put.default](args = (%b_layers_4_attention_sdpa_kv_cache_k_cache, [None, None, %input_pos], %aten_view_copy_default_89), kwargs = {})\n",
      "      %aten_view_copy_default_97 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_expand_copy_default_29, [6, 128, 48]), kwargs = {})\n",
      "      %aten_slice_scatter_default_16 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_scatter.default](args = (%b_layers_4_attention_sdpa_kv_cache_k_cache, %aten_index_put_default_8, 1, 0, 9223372036854775807), kwargs = {})\n",
      "      %aten_slice_scatter_default_17 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_scatter.default](args = (%b_layers_4_attention_sdpa_kv_cache_k_cache, %aten_slice_scatter_default_16, 0, 0, 9223372036854775807), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_38 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_slice_scatter_default_17, 2), kwargs = {})\n",
      "      %aten_expand_copy_default_24 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten_unsqueeze_copy_default_38, [1, 6, 1, 128, 48]), kwargs = {})\n",
      "      %aten_clone_default_8 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.clone.default](args = (%aten_expand_copy_default_24,), kwargs = {memory_format: torch.contiguous_format})\n",
      "      %aten_view_copy_default_91 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_clone_default_8, [1, 6, 128, 48]), kwargs = {})\n",
      "      %aten_permute_copy_default_23 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten_view_copy_default_91, [0, 1, 3, 2]), kwargs = {})\n",
      "      %aten_expand_copy_default_27 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten_permute_copy_default_23, [1, 6, 48, 128]), kwargs = {})\n",
      "      %aten_view_copy_default_94 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_expand_copy_default_27, [6, 48, 128]), kwargs = {})\n",
      "      %aten_bmm_default_8 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.bmm.default](args = (%aten_view_copy_default_93, %aten_view_copy_default_94), kwargs = {})\n",
      "      %aten_view_copy_default_95 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_bmm_default_8, [1, 6, 1, 128]), kwargs = {})\n",
      "      %aten_mul_tensor_79 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_view_copy_default_95, %_lifted_tensor_constant119), kwargs = {})\n",
      "      %aten_add_tensor_31 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mul_tensor_79, %aten__to_copy_default_4), kwargs = {})\n",
      "      %aten__softmax_default_4 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._softmax.default](args = (%aten_add_tensor_31, -1, False), kwargs = {})\n",
      "      %aten_expand_copy_default_28 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten__softmax_default_4, [1, 6, 1, 128]), kwargs = {})\n",
      "      %aten_view_copy_default_96 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_expand_copy_default_28, [6, 1, 128]), kwargs = {})\n",
      "      %aten_bmm_default_9 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.bmm.default](args = (%aten_view_copy_default_96, %aten_view_copy_default_97), kwargs = {})\n",
      "      %aten_view_copy_default_98 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_bmm_default_9, [1, 6, 1, 48]), kwargs = {})\n",
      "      %aten_permute_copy_default_24 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten_view_copy_default_98, [0, 2, 1, 3]), kwargs = {})\n",
      "      %aten_view_copy_default_99 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_permute_copy_default_24, [1, 1, 288]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_47 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_view_copy_default_99, [0]), kwargs = {})\n",
      "      return (aten_slice_scatter_default_19, aten_slice_scatter_default_17, aten_squeeze_copy_dims_47)\n",
      "  %executorch_call_delegate_17 : [num_users=3] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_17, %b_layers_4_attention_sdpa_kv_cache_v_cache, %b_layers_4_attention_sdpa_kv_cache_k_cache, %getitem_4, %getitem_5, %llama_cpp__weight_int8pack_mm_default_28, %llama_cpp__weight_int8pack_mm_default_29, %llama_cpp__weight_int8pack_mm_default_30, %input_pos), kwargs = {})\n",
      "  %getitem_50 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_17, 0), kwargs = {})\n",
      "  %getitem_51 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_17, 1), kwargs = {})\n",
      "  %getitem_52 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_17, 2), kwargs = {})\n",
      "  %alloc_31 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 288), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_31 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_52, %b_layers_4_attention_wo_weight, %b_layers_4_attention_wo_scales), kwargs = {out: %alloc_31})\n",
      "  %lowered_module_18 : [num_users=1] = get_attr[target=lowered_module_18]\n",
      "    backend_id: MPSBackend\n",
      "    lowered graph():\n",
      "      %p_layers_4_ffn_norm_weight : [num_users=1] = placeholder[target=p_layers_4_ffn_norm_weight]\n",
      "      %_lifted_tensor_constant120 : [num_users=1] = placeholder[target=_lifted_tensor_constant120]\n",
      "      %aten_add_tensor_27 : [num_users=1] = placeholder[target=aten_add_tensor_27]\n",
      "      %llama_cpp__weight_int8pack_mm_default_31 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_31]\n",
      "      %aten_add_tensor_32 : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_add_tensor_27, %llama_cpp__weight_int8pack_mm_default_31), kwargs = {})\n",
      "      %aten_mul_tensor_80 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_32, %aten_add_tensor_32), kwargs = {})\n",
      "      %aten_mean_dim_9 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mean.dim](args = (%aten_mul_tensor_80, [-1], True), kwargs = {})\n",
      "      %aten_add_tensor_33 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mean_dim_9, %_lifted_tensor_constant120), kwargs = {})\n",
      "      %aten_rsqrt_default_9 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.rsqrt.default](args = (%aten_add_tensor_33,), kwargs = {})\n",
      "      %aten_mul_tensor_81 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_32, %aten_rsqrt_default_9), kwargs = {})\n",
      "      %aten_mul_tensor_82 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_81, %p_layers_4_ffn_norm_weight), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_48 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_82, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_49 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_82, [0]), kwargs = {})\n",
      "      return (aten_add_tensor_32, aten_squeeze_copy_dims_48, aten_squeeze_copy_dims_49)\n",
      "  %executorch_call_delegate_18 : [num_users=3] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_18, %getitem_46, %llama_cpp__weight_int8pack_mm_default_31), kwargs = {})\n",
      "  %getitem_53 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_18, 0), kwargs = {})\n",
      "  %getitem_54 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_18, 1), kwargs = {})\n",
      "  %getitem_55 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_18, 2), kwargs = {})\n",
      "  %alloc_32 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_32 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_54, %b_layers_4_feed_forward_w1_weight, %b_layers_4_feed_forward_w1_scales), kwargs = {out: %alloc_32})\n",
      "  %alloc_33 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_33 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_55, %b_layers_4_feed_forward_w3_weight, %b_layers_4_feed_forward_w3_scales), kwargs = {out: %alloc_33})\n",
      "  %lowered_module_19 : [num_users=1] = get_attr[target=lowered_module_19]\n",
      "    backend_id: MPSBackend\n",
      "    lowered graph():\n",
      "      %llama_cpp__weight_int8pack_mm_default_32 : [num_users=2] = placeholder[target=llama_cpp__weight_int8pack_mm_default_32]\n",
      "      %llama_cpp__weight_int8pack_mm_default_33 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_33]\n",
      "      %aten_sigmoid_default_4 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sigmoid.default](args = (%llama_cpp__weight_int8pack_mm_default_32,), kwargs = {})\n",
      "      %aten_mul_tensor_83 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%llama_cpp__weight_int8pack_mm_default_32, %aten_sigmoid_default_4), kwargs = {})\n",
      "      %aten_mul_tensor_84 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_83, %llama_cpp__weight_int8pack_mm_default_33), kwargs = {})\n",
      "      return (aten_mul_tensor_84,)\n",
      "  %executorch_call_delegate_19 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_19, %llama_cpp__weight_int8pack_mm_default_32, %llama_cpp__weight_int8pack_mm_default_33), kwargs = {})\n",
      "  %getitem_56 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_19, 0), kwargs = {})\n",
      "  %alloc_34 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 288), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_34 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_56, %b_layers_4_feed_forward_w2_weight, %b_layers_4_feed_forward_w2_scales), kwargs = {out: %alloc_34})\n",
      "  %lowered_module_20 : [num_users=1] = get_attr[target=lowered_module_20]\n",
      "    backend_id: MPSBackend\n",
      "    lowered graph():\n",
      "      %p_layers_5_attention_norm_weight : [num_users=1] = placeholder[target=p_layers_5_attention_norm_weight]\n",
      "      %_lifted_tensor_constant121 : [num_users=1] = placeholder[target=_lifted_tensor_constant121]\n",
      "      %aten_add_tensor_32 : [num_users=1] = placeholder[target=aten_add_tensor_32]\n",
      "      %llama_cpp__weight_int8pack_mm_default_34 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_34]\n",
      "      %aten_add_tensor_34 : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_add_tensor_32, %llama_cpp__weight_int8pack_mm_default_34), kwargs = {})\n",
      "      %aten_mul_tensor_85 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_34, %aten_add_tensor_34), kwargs = {})\n",
      "      %aten_mean_dim_10 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mean.dim](args = (%aten_mul_tensor_85, [-1], True), kwargs = {})\n",
      "      %aten_add_tensor_35 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mean_dim_10, %_lifted_tensor_constant121), kwargs = {})\n",
      "      %aten_rsqrt_default_10 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.rsqrt.default](args = (%aten_add_tensor_35,), kwargs = {})\n",
      "      %aten_mul_tensor_86 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_34, %aten_rsqrt_default_10), kwargs = {})\n",
      "      %aten_mul_tensor_87 : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_86, %p_layers_5_attention_norm_weight), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_50 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_87, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_51 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_87, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_52 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_87, [0]), kwargs = {})\n",
      "      return (aten_add_tensor_34, aten_squeeze_copy_dims_50, aten_squeeze_copy_dims_51, aten_squeeze_copy_dims_52)\n",
      "  %executorch_call_delegate_20 : [num_users=4] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_20, %getitem_53, %llama_cpp__weight_int8pack_mm_default_34), kwargs = {})\n",
      "  %getitem_57 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_20, 0), kwargs = {})\n",
      "  %getitem_58 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_20, 1), kwargs = {})\n",
      "  %getitem_59 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_20, 2), kwargs = {})\n",
      "  %getitem_60 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_20, 3), kwargs = {})\n",
      "  %alloc_35 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 288), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_35 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_58, %b_layers_5_attention_wq_weight, %b_layers_5_attention_wq_scales), kwargs = {out: %alloc_35})\n",
      "  %alloc_36 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 288), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_36 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_59, %b_layers_5_attention_wk_weight, %b_layers_5_attention_wk_scales), kwargs = {out: %alloc_36})\n",
      "  %alloc_37 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 288), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_37 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_60, %b_layers_5_attention_wv_weight, %b_layers_5_attention_wv_scales), kwargs = {out: %alloc_37})\n",
      "  %lowered_module_21 : [num_users=1] = get_attr[target=lowered_module_21]\n",
      "    backend_id: MPSBackend\n",
      "    lowered graph():\n",
      "      %b_layers_5_attention_mask : [num_users=1] = placeholder[target=b_layers_5_attention_mask]\n",
      "      %_lifted_tensor_constant122 : [num_users=1] = placeholder[target=_lifted_tensor_constant122]\n",
      "      %b_layers_5_attention_sdpa_kv_cache_v_cache : [num_users=3] = placeholder[target=b_layers_5_attention_sdpa_kv_cache_v_cache]\n",
      "      %b_layers_5_attention_sdpa_kv_cache_k_cache : [num_users=3] = placeholder[target=b_layers_5_attention_sdpa_kv_cache_k_cache]\n",
      "      %aten_index_tensor : [num_users=1] = placeholder[target=aten_index_tensor]\n",
      "      %aten_index_tensor_1 : [num_users=1] = placeholder[target=aten_index_tensor_1]\n",
      "      %llama_cpp__weight_int8pack_mm_default_35 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_35]\n",
      "      %llama_cpp__weight_int8pack_mm_default_36 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_36]\n",
      "      %llama_cpp__weight_int8pack_mm_default_37 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_37]\n",
      "      %input_pos : [num_users=3] = placeholder[target=input_pos]\n",
      "      %aten_view_copy_default_105 : [num_users=4] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_index_tensor, [1, 1, 1, 24]), kwargs = {})\n",
      "      %aten_view_copy_default_106 : [num_users=4] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_index_tensor_1, [1, 1, 1, 24]), kwargs = {})\n",
      "      %aten_view_copy_default_100 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%llama_cpp__weight_int8pack_mm_default_35, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_101 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%llama_cpp__weight_int8pack_mm_default_36, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_102 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%llama_cpp__weight_int8pack_mm_default_37, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_44 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%b_layers_5_attention_mask, 0), kwargs = {})\n",
      "      %aten_view_copy_default_103 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_view_copy_default_100, [1, 1, 6, -1, 2]), kwargs = {})\n",
      "      %aten_view_copy_default_104 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_view_copy_default_101, [1, 1, 6, -1, 2]), kwargs = {})\n",
      "      %aten_permute_copy_default_27 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten_view_copy_default_102, [0, 2, 1, 3]), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_45 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_unsqueeze_copy_default_44, 1), kwargs = {})\n",
      "      %aten_slice_copy_tensor_20 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_103, 4, 0, 1), kwargs = {})\n",
      "      %aten_slice_copy_tensor_21 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_103, 4, 1, 2), kwargs = {})\n",
      "      %aten_slice_copy_tensor_22 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_104, 4, 0, 1), kwargs = {})\n",
      "      %aten_slice_copy_tensor_23 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_104, 4, 1, 2), kwargs = {})\n",
      "      %aten_view_copy_default_110 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_permute_copy_default_27, [6, 1, 48]), kwargs = {})\n",
      "      %aten_index_tensor_7 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.index.Tensor](args = (%aten_unsqueeze_copy_default_45, [None, None, %input_pos]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_53 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_20, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_54 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_21, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_55 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_22, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_56 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_23, [4]), kwargs = {})\n",
      "      %aten_index_put_default_11 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.index_put.default](args = (%b_layers_5_attention_sdpa_kv_cache_v_cache, [None, None, %input_pos], %aten_view_copy_default_110), kwargs = {})\n",
      "      %aten__to_copy_default_5 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%aten_index_tensor_7,), kwargs = {dtype: torch.float32})\n",
      "      %aten_mul_tensor_88 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_53, %aten_view_copy_default_105), kwargs = {})\n",
      "      %aten_mul_tensor_90 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_53, %aten_view_copy_default_106), kwargs = {})\n",
      "      %aten_mul_tensor_89 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_54, %aten_view_copy_default_106), kwargs = {})\n",
      "      %aten_mul_tensor_91 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_54, %aten_view_copy_default_105), kwargs = {})\n",
      "      %aten_mul_tensor_92 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_55, %aten_view_copy_default_105), kwargs = {})\n",
      "      %aten_mul_tensor_94 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_55, %aten_view_copy_default_106), kwargs = {})\n",
      "      %aten_mul_tensor_93 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_56, %aten_view_copy_default_106), kwargs = {})\n",
      "      %aten_mul_tensor_95 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_56, %aten_view_copy_default_105), kwargs = {})\n",
      "      %aten_slice_scatter_default_22 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_scatter.default](args = (%b_layers_5_attention_sdpa_kv_cache_v_cache, %aten_index_put_default_11, 1, 0, 9223372036854775807), kwargs = {})\n",
      "      %aten_sub_tensor_10 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sub.Tensor](args = (%aten_mul_tensor_88, %aten_mul_tensor_89), kwargs = {})\n",
      "      %aten_add_tensor_36 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mul_tensor_90, %aten_mul_tensor_91), kwargs = {})\n",
      "      %aten_sub_tensor_11 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sub.Tensor](args = (%aten_mul_tensor_92, %aten_mul_tensor_93), kwargs = {})\n",
      "      %aten_add_tensor_37 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mul_tensor_94, %aten_mul_tensor_95), kwargs = {})\n",
      "      %aten_slice_scatter_default_23 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_scatter.default](args = (%b_layers_5_attention_sdpa_kv_cache_v_cache, %aten_slice_scatter_default_22, 0, 0, 9223372036854775807), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_40 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_sub_tensor_10, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_41 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_add_tensor_36, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_42 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_sub_tensor_11, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_43 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_add_tensor_37, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_47 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_slice_scatter_default_23, 2), kwargs = {})\n",
      "      %aten_cat_default_10 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.cat.default](args = ([%aten_unsqueeze_copy_default_40, %aten_unsqueeze_copy_default_41], -1), kwargs = {})\n",
      "      %aten_cat_default_11 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.cat.default](args = ([%aten_unsqueeze_copy_default_42, %aten_unsqueeze_copy_default_43], -1), kwargs = {})\n",
      "      %aten_expand_copy_default_31 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten_unsqueeze_copy_default_47, [1, 6, 1, 128, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_107 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_cat_default_10, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_108 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_cat_default_11, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_clone_default_11 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.clone.default](args = (%aten_expand_copy_default_31,), kwargs = {memory_format: torch.contiguous_format})\n",
      "      %aten_permute_copy_default_25 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten_view_copy_default_107, [0, 2, 1, 3]), kwargs = {})\n",
      "      %aten_permute_copy_default_26 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten_view_copy_default_108, [0, 2, 1, 3]), kwargs = {})\n",
      "      %aten_view_copy_default_112 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_clone_default_11, [1, 6, 128, 48]), kwargs = {})\n",
      "      %aten_expand_copy_default_32 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten_permute_copy_default_25, [1, 6, 1, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_109 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_permute_copy_default_26, [6, 1, 48]), kwargs = {})\n",
      "      %aten_expand_copy_default_35 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten_view_copy_default_112, [1, 6, 128, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_113 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_expand_copy_default_32, [6, 1, 48]), kwargs = {})\n",
      "      %aten_index_put_default_10 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.index_put.default](args = (%b_layers_5_attention_sdpa_kv_cache_k_cache, [None, None, %input_pos], %aten_view_copy_default_109), kwargs = {})\n",
      "      %aten_view_copy_default_117 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_expand_copy_default_35, [6, 128, 48]), kwargs = {})\n",
      "      %aten_slice_scatter_default_20 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_scatter.default](args = (%b_layers_5_attention_sdpa_kv_cache_k_cache, %aten_index_put_default_10, 1, 0, 9223372036854775807), kwargs = {})\n",
      "      %aten_slice_scatter_default_21 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_scatter.default](args = (%b_layers_5_attention_sdpa_kv_cache_k_cache, %aten_slice_scatter_default_20, 0, 0, 9223372036854775807), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_46 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_slice_scatter_default_21, 2), kwargs = {})\n",
      "      %aten_expand_copy_default_30 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten_unsqueeze_copy_default_46, [1, 6, 1, 128, 48]), kwargs = {})\n",
      "      %aten_clone_default_10 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.clone.default](args = (%aten_expand_copy_default_30,), kwargs = {memory_format: torch.contiguous_format})\n",
      "      %aten_view_copy_default_111 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_clone_default_10, [1, 6, 128, 48]), kwargs = {})\n",
      "      %aten_permute_copy_default_28 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten_view_copy_default_111, [0, 1, 3, 2]), kwargs = {})\n",
      "      %aten_expand_copy_default_33 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten_permute_copy_default_28, [1, 6, 48, 128]), kwargs = {})\n",
      "      %aten_view_copy_default_114 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_expand_copy_default_33, [6, 48, 128]), kwargs = {})\n",
      "      %aten_bmm_default_10 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.bmm.default](args = (%aten_view_copy_default_113, %aten_view_copy_default_114), kwargs = {})\n",
      "      %aten_view_copy_default_115 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_bmm_default_10, [1, 6, 1, 128]), kwargs = {})\n",
      "      %aten_mul_tensor_96 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_view_copy_default_115, %_lifted_tensor_constant122), kwargs = {})\n",
      "      %aten_add_tensor_38 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mul_tensor_96, %aten__to_copy_default_5), kwargs = {})\n",
      "      %aten__softmax_default_5 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._softmax.default](args = (%aten_add_tensor_38, -1, False), kwargs = {})\n",
      "      %aten_expand_copy_default_34 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.expand_copy.default](args = (%aten__softmax_default_5, [1, 6, 1, 128]), kwargs = {})\n",
      "      %aten_view_copy_default_116 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_expand_copy_default_34, [6, 1, 128]), kwargs = {})\n",
      "      %aten_bmm_default_11 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.bmm.default](args = (%aten_view_copy_default_116, %aten_view_copy_default_117), kwargs = {})\n",
      "      %aten_view_copy_default_118 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_bmm_default_11, [1, 6, 1, 48]), kwargs = {})\n",
      "      %aten_permute_copy_default_29 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten_view_copy_default_118, [0, 2, 1, 3]), kwargs = {})\n",
      "      %aten_view_copy_default_119 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_permute_copy_default_29, [1, 1, 288]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_57 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_view_copy_default_119, [0]), kwargs = {})\n",
      "      return (aten_slice_scatter_default_23, aten_slice_scatter_default_21, aten_squeeze_copy_dims_57)\n",
      "  %executorch_call_delegate_21 : [num_users=3] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_21, %b_layers_5_attention_sdpa_kv_cache_v_cache, %b_layers_5_attention_sdpa_kv_cache_k_cache, %getitem_4, %getitem_5, %llama_cpp__weight_int8pack_mm_default_35, %llama_cpp__weight_int8pack_mm_default_36, %llama_cpp__weight_int8pack_mm_default_37, %input_pos), kwargs = {})\n",
      "  %getitem_61 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_21, 0), kwargs = {})\n",
      "  %getitem_62 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_21, 1), kwargs = {})\n",
      "  %getitem_63 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_21, 2), kwargs = {})\n",
      "  %alloc_38 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 288), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_38 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_63, %b_layers_5_attention_wo_weight, %b_layers_5_attention_wo_scales), kwargs = {out: %alloc_38})\n",
      "  %lowered_module_22 : [num_users=1] = get_attr[target=lowered_module_22]\n",
      "    backend_id: MPSBackend\n",
      "    lowered graph():\n",
      "      %p_layers_5_ffn_norm_weight : [num_users=1] = placeholder[target=p_layers_5_ffn_norm_weight]\n",
      "      %_lifted_tensor_constant123 : [num_users=1] = placeholder[target=_lifted_tensor_constant123]\n",
      "      %aten_add_tensor_34 : [num_users=1] = placeholder[target=aten_add_tensor_34]\n",
      "      %llama_cpp__weight_int8pack_mm_default_38 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_38]\n",
      "      %aten_add_tensor_39 : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_add_tensor_34, %llama_cpp__weight_int8pack_mm_default_38), kwargs = {})\n",
      "      %aten_mul_tensor_97 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_39, %aten_add_tensor_39), kwargs = {})\n",
      "      %aten_mean_dim_11 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mean.dim](args = (%aten_mul_tensor_97, [-1], True), kwargs = {})\n",
      "      %aten_add_tensor_40 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mean_dim_11, %_lifted_tensor_constant123), kwargs = {})\n",
      "      %aten_rsqrt_default_11 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.rsqrt.default](args = (%aten_add_tensor_40,), kwargs = {})\n",
      "      %aten_mul_tensor_98 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_39, %aten_rsqrt_default_11), kwargs = {})\n",
      "      %aten_mul_tensor_99 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_98, %p_layers_5_ffn_norm_weight), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_58 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_99, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_59 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_99, [0]), kwargs = {})\n",
      "      return (aten_add_tensor_39, aten_squeeze_copy_dims_58, aten_squeeze_copy_dims_59)\n",
      "  %executorch_call_delegate_22 : [num_users=3] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_22, %getitem_57, %llama_cpp__weight_int8pack_mm_default_38), kwargs = {})\n",
      "  %getitem_64 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_22, 0), kwargs = {})\n",
      "  %getitem_65 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_22, 1), kwargs = {})\n",
      "  %getitem_66 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_22, 2), kwargs = {})\n",
      "  %alloc_39 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_39 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_65, %b_layers_5_feed_forward_w1_weight, %b_layers_5_feed_forward_w1_scales), kwargs = {out: %alloc_39})\n",
      "  %alloc_40 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 768), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_40 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_66, %b_layers_5_feed_forward_w3_weight, %b_layers_5_feed_forward_w3_scales), kwargs = {out: %alloc_40})\n",
      "  %lowered_module_23 : [num_users=1] = get_attr[target=lowered_module_23]\n",
      "    backend_id: MPSBackend\n",
      "    lowered graph():\n",
      "      %llama_cpp__weight_int8pack_mm_default_39 : [num_users=2] = placeholder[target=llama_cpp__weight_int8pack_mm_default_39]\n",
      "      %llama_cpp__weight_int8pack_mm_default_40 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_40]\n",
      "      %aten_sigmoid_default_5 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sigmoid.default](args = (%llama_cpp__weight_int8pack_mm_default_39,), kwargs = {})\n",
      "      %aten_mul_tensor_100 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%llama_cpp__weight_int8pack_mm_default_39, %aten_sigmoid_default_5), kwargs = {})\n",
      "      %aten_mul_tensor_101 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_100, %llama_cpp__weight_int8pack_mm_default_40), kwargs = {})\n",
      "      return (aten_mul_tensor_101,)\n",
      "  %executorch_call_delegate_23 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_23, %llama_cpp__weight_int8pack_mm_default_39, %llama_cpp__weight_int8pack_mm_default_40), kwargs = {})\n",
      "  %getitem_67 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_23, 0), kwargs = {})\n",
      "  %alloc_41 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 288), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_41 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_67, %b_layers_5_feed_forward_w2_weight, %b_layers_5_feed_forward_w2_scales), kwargs = {out: %alloc_41})\n",
      "  %lowered_module_24 : [num_users=1] = get_attr[target=lowered_module_24]\n",
      "    backend_id: MPSBackend\n",
      "    lowered graph():\n",
      "      %p_norm_weight : [num_users=1] = placeholder[target=p_norm_weight]\n",
      "      %_lifted_tensor_constant124 : [num_users=1] = placeholder[target=_lifted_tensor_constant124]\n",
      "      %aten_add_tensor_39 : [num_users=1] = placeholder[target=aten_add_tensor_39]\n",
      "      %llama_cpp__weight_int8pack_mm_default_41 : [num_users=1] = placeholder[target=llama_cpp__weight_int8pack_mm_default_41]\n",
      "      %aten_add_tensor_41 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_add_tensor_39, %llama_cpp__weight_int8pack_mm_default_41), kwargs = {})\n",
      "      %aten_mul_tensor_102 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_41, %aten_add_tensor_41), kwargs = {})\n",
      "      %aten_mean_dim_12 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mean.dim](args = (%aten_mul_tensor_102, [-1], True), kwargs = {})\n",
      "      %aten_add_tensor_42 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mean_dim_12, %_lifted_tensor_constant124), kwargs = {})\n",
      "      %aten_rsqrt_default_12 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.rsqrt.default](args = (%aten_add_tensor_42,), kwargs = {})\n",
      "      %aten_mul_tensor_103 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_41, %aten_rsqrt_default_12), kwargs = {})\n",
      "      %aten_mul_tensor_104 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_103, %p_norm_weight), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_60 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_104, [0]), kwargs = {})\n",
      "      return (aten_squeeze_copy_dims_60,)\n",
      "  %executorch_call_delegate_24 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_24, %getitem_64, %llama_cpp__weight_int8pack_mm_default_41), kwargs = {})\n",
      "  %getitem_68 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_24, 0), kwargs = {})\n",
      "  %alloc_42 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 32000), torch.float32),), kwargs = {})\n",
      "  %llama_cpp__weight_int8pack_mm_default_42 : [num_users=1] = call_function[target=torch.ops.llama_cpp._weight_int8pack_mm.out](args = (%getitem_68, %b_output_weight, %b_output_scales), kwargs = {out: %alloc_42})\n",
      "  %copy_ : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_0_attention_sdpa_kv_cache_k_cache, %getitem_7), kwargs = {})\n",
      "  %copy__1 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_0_attention_sdpa_kv_cache_v_cache, %getitem_6), kwargs = {})\n",
      "  %copy__2 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_1_attention_sdpa_kv_cache_k_cache, %getitem_18), kwargs = {})\n",
      "  %copy__3 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_1_attention_sdpa_kv_cache_v_cache, %getitem_17), kwargs = {})\n",
      "  %copy__4 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_2_attention_sdpa_kv_cache_k_cache, %getitem_29), kwargs = {})\n",
      "  %copy__5 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_2_attention_sdpa_kv_cache_v_cache, %getitem_28), kwargs = {})\n",
      "  %copy__6 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_3_attention_sdpa_kv_cache_k_cache, %getitem_40), kwargs = {})\n",
      "  %copy__7 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_3_attention_sdpa_kv_cache_v_cache, %getitem_39), kwargs = {})\n",
      "  %copy__8 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_4_attention_sdpa_kv_cache_k_cache, %getitem_51), kwargs = {})\n",
      "  %copy__9 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_4_attention_sdpa_kv_cache_v_cache, %getitem_50), kwargs = {})\n",
      "  %copy__10 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_5_attention_sdpa_kv_cache_k_cache, %getitem_62), kwargs = {})\n",
      "  %copy__11 : [num_users=0] = call_function[target=torch.ops.aten.copy_.default](args = (%b_layers_5_attention_sdpa_kv_cache_v_cache, %getitem_61), kwargs = {})\n",
      "  return (llama_cpp__weight_int8pack_mm_default_42,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from executorch.exir.backend.utils import print_delegated_graph\n",
    "\n",
    "print_delegated_graph(builder.export_program.exported_program(\"forward\").graph_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2024-05-29 09:33:29,995 utils.py:114] Saved exported program to stories15M_int8_mps_llama_cpp.pte\n"
     ]
    }
   ],
   "source": [
    "builder.save_to_pte(\"stories15M_int8_mps_llama_cpp.pte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2024-05-29 09:31:26,949 utils.py:114] Saved exported program to stories15M_int8_llama_cpp.pte\n"
     ]
    }
   ],
   "source": [
    "builder.save_to_pte(\"stories15M_int8_llama_cpp.pte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from executorch.extension.pybindings.portable_lib import _get_operator_names\n",
    "\n",
    "names = _get_operator_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aten::sym_size.int\n",
      "aten::_local_scalar_dense\n",
      "aten::sym_numel\n",
      "executorch_prim::add.Scalar\n",
      "executorch_prim::sub.Scalar\n",
      "executorch_prim::mul.Scalar\n",
      "executorch_prim::floordiv.Scalar\n",
      "executorch_prim::truediv.Scalar\n",
      "executorch_prim::eq.Scalar\n",
      "executorch_prim::gt.Scalar\n",
      "executorch_prim::lt.Scalar\n",
      "executorch_prim::ge.Scalar\n",
      "executorch_prim::le.Scalar\n",
      "executorch_prim::floordiv.int\n",
      "executorch_prim::et_copy_index.tensor\n",
      "executorch_prim::et_view.default\n",
      "aten::_cdist_forward.out\n",
      "aten::_log_softmax.out\n",
      "aten::_native_batch_norm_legit.out\n",
      "aten::_native_batch_norm_legit.no_stats_out\n",
      "aten::_native_batch_norm_legit_no_training.out\n",
      "aten::_pdist_forward.out\n",
      "aten::_softmax.out\n",
      "aten::_to_copy.out\n",
      "aten::abs.out\n",
      "aten::acos.out\n",
      "aten::acosh.out\n",
      "aten::add.out\n",
      "aten::add.Scalar_out\n",
      "aten::addmm.out\n",
      "aten::alias_copy.out\n",
      "aten::amax.out\n",
      "aten::amin.out\n",
      "aten::any.all_out\n",
      "aten::any.dims_out\n",
      "aten::any.out\n",
      "aten::arange.out\n",
      "aten::arange.start_out\n",
      "aten::argmax.out\n",
      "aten::argmin.out\n",
      "aten::as_strided_copy.out\n",
      "aten::asin.out\n",
      "aten::asinh.out\n",
      "aten::atan.out\n",
      "aten::atan2.out\n",
      "aten::atanh.out\n",
      "aten::avg_pool2d.out\n",
      "aten::bitwise_and.Scalar_out\n",
      "aten::bitwise_and.Tensor_out\n",
      "aten::bitwise_not.out\n",
      "aten::bitwise_or.Scalar_out\n",
      "aten::bitwise_or.Tensor_out\n",
      "aten::bitwise_xor.Scalar_out\n",
      "aten::bitwise_xor.Tensor_out\n",
      "aten::bmm.out\n",
      "aten::cat.out\n",
      "aten::ceil.out\n",
      "aten::clamp.out\n",
      "aten::clamp.Tensor_out\n",
      "aten::clone.out\n",
      "aten::constant_pad_nd.out\n",
      "aten::convolution.out\n",
      "aten::copy.out\n",
      "aten::copy_\n",
      "aten::cos.out\n",
      "aten::cosh.out\n",
      "aten::cumsum.out\n",
      "aten::detach_copy.out\n",
      "aten::diagonal_copy.out\n",
      "aten::div.out\n",
      "aten::div.Scalar_mode_out\n",
      "aten::div.Scalar_out\n",
      "aten::div.out_mode\n",
      "aten::embedding.out\n",
      "aten::empty.out\n",
      "aten::eq.Scalar_out\n",
      "aten::eq.Tensor_out\n",
      "aten::erf.out\n",
      "aten::exp.out\n",
      "aten::expand_copy.out\n",
      "aten::expm1.out\n",
      "aten::fill.Scalar_out\n",
      "aten::fill.Tensor_out\n",
      "aten::flip.out\n",
      "aten::floor.out\n",
      "aten::floor_divide.out\n",
      "aten::fmod.Tensor_out\n",
      "aten::fmod.Scalar_out\n",
      "aten::full.out\n",
      "aten::full_like.out\n",
      "aten::ge.Scalar_out\n",
      "aten::ge.Tensor_out\n",
      "aten::gelu.out\n",
      "aten::glu.out\n",
      "aten::gt.Scalar_out\n",
      "aten::gt.Tensor_out\n",
      "aten::hardtanh.out\n",
      "aten::index.Tensor_out\n",
      "aten::index_put.out\n",
      "aten::index_select.out\n",
      "aten::isinf.out\n",
      "aten::isnan.out\n",
      "aten::le.Scalar_out\n",
      "aten::le.Tensor_out\n",
      "aten::leaky_relu.out\n",
      "aten::lift_fresh_copy.out\n",
      "aten::log.out\n",
      "aten::log10.out\n",
      "aten::log1p.out\n",
      "aten::log2.out\n",
      "aten::logical_and.out\n",
      "aten::logical_not.out\n",
      "aten::logical_or.out\n",
      "aten::logical_xor.out\n",
      "aten::logit.out\n",
      "aten::lt.Scalar_out\n",
      "aten::lt.Tensor_out\n",
      "aten::masked_fill.Scalar_out\n",
      "aten::max.dim_max\n",
      "aten::maximum.out\n",
      "aten::max_pool2d_with_indices.out\n",
      "aten::mean.out\n",
      "aten::min.dim_min\n",
      "aten::minimum.out\n",
      "aten::mm.out\n",
      "aten::mul.out\n",
      "aten::mul.Scalar_out\n",
      "aten::native_group_norm.out\n",
      "aten::native_layer_norm.out\n",
      "aten::ne.Scalar_out\n",
      "aten::ne.Tensor_out\n",
      "aten::neg.out\n",
      "aten::nonzero.out\n",
      "aten::ones.out\n",
      "aten::permute_copy.out\n",
      "aten::pixel_shuffle.out\n",
      "aten::pow.Scalar_out\n",
      "aten::pow.Tensor_Scalar_out\n",
      "aten::pow.Tensor_Tensor_out\n",
      "aten::prod.int_out\n",
      "aten::prod.out\n",
      "aten::reciprocal.out\n",
      "aten::relu.out\n",
      "aten::remainder.Tensor_out\n",
      "aten::remainder.Scalar_out\n",
      "aten::repeat.out\n",
      "aten::reflection_pad1d.out\n",
      "aten::reflection_pad2d.out\n",
      "aten::reflection_pad3d.out\n",
      "aten::replication_pad1d.out\n",
      "aten::replication_pad2d.out\n",
      "aten::replication_pad3d.out\n",
      "aten::roll.out\n",
      "aten::round.out\n",
      "aten::rsqrt.out\n",
      "aten::rsub.Scalar_out\n",
      "aten::scalar_tensor.out\n",
      "aten::scatter_add.out\n",
      "aten::select_copy.int_out\n",
      "aten::select_scatter.out\n",
      "aten::sigmoid.out\n",
      "aten::sign.out\n",
      "aten::sin.out\n",
      "aten::sinh.out\n",
      "aten::slice_copy.Tensor_out\n",
      "aten::slice_scatter.out\n",
      "aten::split_copy.Tensor_out\n",
      "aten::split_with_sizes_copy.out\n",
      "aten::sqrt.out\n",
      "aten::squeeze_copy.dim_out\n",
      "aten::squeeze_copy.dims_out\n",
      "aten::stack.out\n",
      "aten::sub.out\n",
      "aten::sub.Scalar_out\n",
      "aten::sum.IntList_out\n",
      "aten::t_copy.out\n",
      "aten::tan.out\n",
      "aten::tanh.out\n",
      "aten::transpose_copy.int_out\n",
      "aten::tril.out\n",
      "aten::trunc.out\n",
      "aten::unbind_copy.int_out\n",
      "aten::unsqueeze_copy.out\n",
      "aten::var.correction_out\n",
      "aten::var.out\n",
      "aten::view_copy.out\n",
      "aten::where.self_out\n",
      "aten::zeros.out\n",
      "dim_order_ops::_to_dim_order_copy.out\n",
      "quantized_decomposed::add.out\n",
      "quantized_decomposed::choose_qparams.Tensor_out\n",
      "quantized_decomposed::dequantize_per_tensor.out\n",
      "quantized_decomposed::dequantize_per_tensor.Tensor_out\n",
      "quantized_decomposed::quantize_per_channel.out\n",
      "quantized_decomposed::dequantize_per_channel.out\n",
      "quantized_decomposed::embedding_byte.out\n",
      "quantized_decomposed::embedding_byte.dtype_out\n",
      "quantized_decomposed::embedding_4bit.out\n",
      "quantized_decomposed::embedding_4bit.dtype_out\n",
      "quantized_decomposed::mixed_mm.out\n",
      "quantized_decomposed::mixed_linear.out\n",
      "quantized_decomposed::quantize_per_tensor.out\n",
      "quantized_decomposed::quantize_per_tensor.Tensor_out\n",
      "llama::sdpa_with_kv_cache.out\n",
      "llama_cpp::_weight_int8pack_mm.out\n",
      "205\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(names))\n",
    "print(len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2024-05-08 14:39:40,251 utils.py:112] Saved exported program to stories110M_int8.pte\n"
     ]
    }
   ],
   "source": [
    "builder.save_to_pte(\"stories110M_int8.pte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2024-05-08 17:58:18,813 utils.py:113] Saved exported program to stories110M_int8_mps.pte\n"
     ]
    }
   ],
   "source": [
    "builder.save_to_pte(\"stories110M_int8_mps.pte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from executorch.extension.pybindings.portable_lib import _load_for_executorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "op1 = torch.ops.aten._weight_int8pack_mm.default\n",
    "op2 = torch.ops.llama_cpp._weight_int8pack_mm.default\n",
    "mps_device = torch.device(\"mps\")  # Device object representing GPU.\n",
    "\n",
    "A = torch.randn(4, 8, dtype=torch.float, device=mps_device)\n",
    "B = torch.ones(8, 8, dtype=torch.int8, device=mps_device)\n",
    "scales = torch.randn(8, dtype=torch.float, device=mps_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2965,  0.1211, -0.0488, -0.0738, -0.5035,  0.7861,  0.9290,  0.6385],\n",
      "        [ 2.7348,  1.1167, -0.4501, -0.6806, -4.6442,  7.2503,  8.5685,  5.8890],\n",
      "        [-0.0908, -0.0371,  0.0149,  0.0226,  0.1542, -0.2408, -0.2845, -0.1956],\n",
      "        [-2.2508, -0.9191,  0.3705,  0.5601,  3.8223, -5.9672, -7.0521, -4.8468]],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "C1 = op1(A, B, scales)\n",
    "print(C1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2965,  0.1211, -0.0488, -0.0738, -0.5035,  0.7861,  0.9290,  0.6385],\n",
      "        [ 2.7348,  1.1167, -0.4501, -0.6806, -4.6442,  7.2503,  8.5685,  5.8890],\n",
      "        [-0.0908, -0.0371,  0.0149,  0.0226,  0.1542, -0.2408, -0.2845, -0.1956],\n",
      "        [-2.2508, -0.9191,  0.3705,  0.5601,  3.8223, -5.9672, -7.0521, -4.8468]],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "C2 = op2(A, B, scales)\n",
    "print(C2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(C1, C2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentencepiece import SentencePieceProcessor as SPP\n",
    "\n",
    "sp_model = SPP(model_file=\"tokenizer.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 9038, 2501, 263, 931]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Once upon a time\"\n",
    "\n",
    "prompt_tokens = sp_model.encode(prompt)\n",
    "prompt_tokens = [sp_model.bos_id()] + prompt_tokens\n",
    "print(prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.8514,  1.4318, -5.8515,  ..., -5.8277, -5.8277, -5.8515]],\n",
       "       grad_fn=<NotImplemented>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[prompt_tokens[0]]], dtype=torch.int64)\n",
    "pos = torch.tensor([0], dtype=torch.int64)\n",
    "\n",
    "model.model(t, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge module\n",
    "builder = model.export_to_edge()\n",
    "edge_module = builder.edge_manager._edge_programs[\"forward\"].module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom op module\n",
    "builder.edge_manager = builder.edge_manager.transform([ReplaceMMPass()])\n",
    "custom_op_module = builder.edge_manager._edge_programs[\"forward\"].module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2024-05-29 14:04:22,525 mps_partitioner.py:121] Found 25 subgraphs to be partitioned.\n",
      "[INFO 2024-05-29 14:04:22,526 utils.py:527] The buffer node is a mutated buffer node, which is not constant.\n",
      "[INFO 2024-05-29 14:04:22,526 utils.py:527] The buffer node is a mutated buffer node, which is not constant.\n",
      "[INFO 2024-05-29 14:04:22,527 utils.py:527] The buffer node is a mutated buffer node, which is not constant.\n",
      "[INFO 2024-05-29 14:04:22,528 utils.py:527] The buffer node is a mutated buffer node, which is not constant.\n",
      "[INFO 2024-05-29 14:04:22,528 utils.py:527] The buffer node is a mutated buffer node, which is not constant.\n",
      "[INFO 2024-05-29 14:04:22,529 utils.py:527] The buffer node is a mutated buffer node, which is not constant.\n",
      "[INFO 2024-05-29 14:04:22,530 utils.py:527] The buffer node is a mutated buffer node, which is not constant.\n",
      "[INFO 2024-05-29 14:04:22,530 utils.py:527] The buffer node is a mutated buffer node, which is not constant.\n",
      "[INFO 2024-05-29 14:04:22,531 utils.py:527] The buffer node is a mutated buffer node, which is not constant.\n",
      "[INFO 2024-05-29 14:04:22,531 utils.py:527] The buffer node is a mutated buffer node, which is not constant.\n",
      "[INFO 2024-05-29 14:04:22,532 utils.py:527] The buffer node is a mutated buffer node, which is not constant.\n",
      "[INFO 2024-05-29 14:04:22,532 utils.py:527] The buffer node is a mutated buffer node, which is not constant.\n",
      "[INFO 2024-05-29 14:04:22,572 mps_preprocess.py:115] Visiting: aten_add_tensor_41, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:22,573 mps_preprocess.py:115] Visiting: aten_mul_tensor_102, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:22,573 mps_preprocess.py:115] Visiting: aten_mean_dim_12, aten.mean.dim\n",
      "[INFO 2024-05-29 14:04:22,574 mps_preprocess.py:115] Visiting: aten_add_tensor_42, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:22,574 mps_preprocess.py:115] Visiting: aten_rsqrt_default_12, aten.rsqrt.default\n",
      "[INFO 2024-05-29 14:04:22,574 mps_preprocess.py:115] Visiting: aten_mul_tensor_103, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:22,574 mps_preprocess.py:115] Visiting: aten_mul_tensor_104, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:22,575 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_60, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:22,697 mps_preprocess.py:115] Visiting: aten_sigmoid_default_5, aten.sigmoid.default\n",
      "[INFO 2024-05-29 14:04:22,698 mps_preprocess.py:115] Visiting: aten_mul_tensor_100, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:22,699 mps_preprocess.py:115] Visiting: aten_mul_tensor_101, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:22,803 mps_preprocess.py:115] Visiting: aten_add_tensor_39, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:22,804 mps_preprocess.py:115] Visiting: aten_mul_tensor_97, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:22,804 mps_preprocess.py:115] Visiting: aten_mean_dim_11, aten.mean.dim\n",
      "[INFO 2024-05-29 14:04:22,805 mps_preprocess.py:115] Visiting: aten_add_tensor_40, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:22,805 mps_preprocess.py:115] Visiting: aten_rsqrt_default_11, aten.rsqrt.default\n",
      "[INFO 2024-05-29 14:04:22,805 mps_preprocess.py:115] Visiting: aten_mul_tensor_98, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:22,805 mps_preprocess.py:115] Visiting: aten_mul_tensor_99, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:22,805 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_58, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:22,806 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_59, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:22,956 mps_preprocess.py:115] Visiting: aten_view_copy_default_105, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:22,956 mps_preprocess.py:115] Visiting: aten_view_copy_default_106, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:22,957 mps_preprocess.py:115] Visiting: aten_view_copy_default_100, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:22,957 mps_preprocess.py:115] Visiting: aten_view_copy_default_101, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:22,957 mps_preprocess.py:115] Visiting: aten_view_copy_default_102, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:22,957 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_44, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:22,957 mps_preprocess.py:115] Visiting: aten_view_copy_default_103, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:22,958 mps_preprocess.py:115] Visiting: aten_view_copy_default_104, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:22,958 mps_preprocess.py:115] Visiting: aten_permute_copy_default_27, aten.permute_copy.default\n",
      "[INFO 2024-05-29 14:04:22,958 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_45, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:22,958 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_20, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 14:04:22,958 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_21, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 14:04:22,958 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_22, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 14:04:22,959 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_23, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 14:04:22,959 mps_preprocess.py:115] Visiting: aten_view_copy_default_110, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:22,959 mps_preprocess.py:115] Visiting: aten_index_tensor_7, aten.index.Tensor\n",
      "[INFO 2024-05-29 14:04:22,959 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_53, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:22,960 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_54, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:22,960 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_55, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:22,960 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_56, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:22,960 mps_preprocess.py:115] Visiting: aten_index_put_default_11, aten.index_put.default\n",
      "[INFO 2024-05-29 14:04:22,960 mps_preprocess.py:115] Visiting: aten__to_copy_default_5, aten._to_copy.default\n",
      "[INFO 2024-05-29 14:04:22,960 mps_preprocess.py:115] Visiting: aten_mul_tensor_88, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:22,961 mps_preprocess.py:115] Visiting: aten_mul_tensor_90, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:22,961 mps_preprocess.py:115] Visiting: aten_mul_tensor_89, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:22,961 mps_preprocess.py:115] Visiting: aten_mul_tensor_91, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:22,961 mps_preprocess.py:115] Visiting: aten_mul_tensor_92, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:22,961 mps_preprocess.py:115] Visiting: aten_mul_tensor_94, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:22,962 mps_preprocess.py:115] Visiting: aten_mul_tensor_93, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:22,962 mps_preprocess.py:115] Visiting: aten_mul_tensor_95, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:22,962 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_22, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 14:04:22,963 mps_preprocess.py:115] Visiting: aten_sub_tensor_10, aten.sub.Tensor\n",
      "[INFO 2024-05-29 14:04:22,963 mps_preprocess.py:115] Visiting: aten_add_tensor_36, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:22,963 mps_preprocess.py:115] Visiting: aten_sub_tensor_11, aten.sub.Tensor\n",
      "[INFO 2024-05-29 14:04:22,963 mps_preprocess.py:115] Visiting: aten_add_tensor_37, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:22,964 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_23, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 14:04:22,965 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_40, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:22,965 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_41, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:22,965 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_42, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:22,966 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_43, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:22,966 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_47, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:22,966 mps_preprocess.py:115] Visiting: aten_cat_default_10, aten.cat.default\n",
      "[INFO 2024-05-29 14:04:22,967 mps_preprocess.py:115] Visiting: aten_cat_default_11, aten.cat.default\n",
      "[INFO 2024-05-29 14:04:22,967 mps_preprocess.py:115] Visiting: aten_expand_copy_default_31, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:22,967 mps_preprocess.py:115] Visiting: aten_view_copy_default_107, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:22,967 mps_preprocess.py:115] Visiting: aten_view_copy_default_108, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:22,967 mps_preprocess.py:115] Visiting: aten_clone_default_11, aten.clone.default\n",
      "[INFO 2024-05-29 14:04:22,968 mps_preprocess.py:115] Visiting: aten_permute_copy_default_25, aten.permute_copy.default\n",
      "[INFO 2024-05-29 14:04:22,968 mps_preprocess.py:115] Visiting: aten_permute_copy_default_26, aten.permute_copy.default\n",
      "[INFO 2024-05-29 14:04:22,968 mps_preprocess.py:115] Visiting: aten_view_copy_default_112, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:22,968 mps_preprocess.py:115] Visiting: aten_expand_copy_default_32, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:22,968 mps_preprocess.py:115] Visiting: aten_view_copy_default_109, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:22,968 mps_preprocess.py:115] Visiting: aten_expand_copy_default_35, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:22,969 mps_preprocess.py:115] Visiting: aten_view_copy_default_113, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:22,969 mps_preprocess.py:115] Visiting: aten_index_put_default_10, aten.index_put.default\n",
      "[INFO 2024-05-29 14:04:22,969 mps_preprocess.py:115] Visiting: aten_view_copy_default_117, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:22,969 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_20, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 14:04:22,970 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_21, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 14:04:22,971 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_46, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:22,971 mps_preprocess.py:115] Visiting: aten_expand_copy_default_30, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:22,971 mps_preprocess.py:115] Visiting: aten_clone_default_10, aten.clone.default\n",
      "[INFO 2024-05-29 14:04:22,971 mps_preprocess.py:115] Visiting: aten_view_copy_default_111, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:22,971 mps_preprocess.py:115] Visiting: aten_permute_copy_default_28, aten.permute_copy.default\n",
      "[INFO 2024-05-29 14:04:22,972 mps_preprocess.py:115] Visiting: aten_expand_copy_default_33, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:22,972 mps_preprocess.py:115] Visiting: aten_view_copy_default_114, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:22,972 mps_preprocess.py:115] Visiting: aten_bmm_default_10, aten.bmm.default\n",
      "[INFO 2024-05-29 14:04:22,972 mps_preprocess.py:115] Visiting: aten_view_copy_default_115, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:22,972 mps_preprocess.py:115] Visiting: aten_mul_tensor_96, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:22,973 mps_preprocess.py:115] Visiting: aten_add_tensor_38, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:22,973 mps_preprocess.py:115] Visiting: aten__softmax_default_5, aten._softmax.default\n",
      "[INFO 2024-05-29 14:04:22,973 mps_preprocess.py:115] Visiting: aten_expand_copy_default_34, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:22,973 mps_preprocess.py:115] Visiting: aten_view_copy_default_116, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:22,973 mps_preprocess.py:115] Visiting: aten_bmm_default_11, aten.bmm.default\n",
      "[INFO 2024-05-29 14:04:22,973 mps_preprocess.py:115] Visiting: aten_view_copy_default_118, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:22,974 mps_preprocess.py:115] Visiting: aten_permute_copy_default_29, aten.permute_copy.default\n",
      "[INFO 2024-05-29 14:04:22,974 mps_preprocess.py:115] Visiting: aten_view_copy_default_119, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:22,974 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_57, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:23,221 mps_preprocess.py:115] Visiting: aten_add_tensor_34, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:23,222 mps_preprocess.py:115] Visiting: aten_mul_tensor_85, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:23,222 mps_preprocess.py:115] Visiting: aten_mean_dim_10, aten.mean.dim\n",
      "[INFO 2024-05-29 14:04:23,222 mps_preprocess.py:115] Visiting: aten_add_tensor_35, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:23,222 mps_preprocess.py:115] Visiting: aten_rsqrt_default_10, aten.rsqrt.default\n",
      "[INFO 2024-05-29 14:04:23,222 mps_preprocess.py:115] Visiting: aten_mul_tensor_86, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:23,222 mps_preprocess.py:115] Visiting: aten_mul_tensor_87, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:23,223 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_50, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:23,223 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_51, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:23,223 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_52, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:23,320 mps_preprocess.py:115] Visiting: aten_sigmoid_default_4, aten.sigmoid.default\n",
      "[INFO 2024-05-29 14:04:23,321 mps_preprocess.py:115] Visiting: aten_mul_tensor_83, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:23,321 mps_preprocess.py:115] Visiting: aten_mul_tensor_84, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:23,424 mps_preprocess.py:115] Visiting: aten_add_tensor_32, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:23,425 mps_preprocess.py:115] Visiting: aten_mul_tensor_80, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:23,425 mps_preprocess.py:115] Visiting: aten_mean_dim_9, aten.mean.dim\n",
      "[INFO 2024-05-29 14:04:23,425 mps_preprocess.py:115] Visiting: aten_add_tensor_33, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:23,425 mps_preprocess.py:115] Visiting: aten_rsqrt_default_9, aten.rsqrt.default\n",
      "[INFO 2024-05-29 14:04:23,425 mps_preprocess.py:115] Visiting: aten_mul_tensor_81, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:23,426 mps_preprocess.py:115] Visiting: aten_mul_tensor_82, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:23,426 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_48, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:23,426 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_49, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:23,537 mps_preprocess.py:115] Visiting: aten_view_copy_default_85, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:23,538 mps_preprocess.py:115] Visiting: aten_view_copy_default_86, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:23,538 mps_preprocess.py:115] Visiting: aten_view_copy_default_80, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:23,538 mps_preprocess.py:115] Visiting: aten_view_copy_default_81, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:23,538 mps_preprocess.py:115] Visiting: aten_view_copy_default_82, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:23,539 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_36, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:23,539 mps_preprocess.py:115] Visiting: aten_view_copy_default_83, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:23,539 mps_preprocess.py:115] Visiting: aten_view_copy_default_84, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:23,539 mps_preprocess.py:115] Visiting: aten_permute_copy_default_22, aten.permute_copy.default\n",
      "[INFO 2024-05-29 14:04:23,539 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_37, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:23,540 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_16, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 14:04:23,540 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_17, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 14:04:23,540 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_18, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 14:04:23,540 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_19, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 14:04:23,540 mps_preprocess.py:115] Visiting: aten_view_copy_default_90, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:23,541 mps_preprocess.py:115] Visiting: aten_index_tensor_6, aten.index.Tensor\n",
      "[INFO 2024-05-29 14:04:23,541 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_43, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:23,541 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_44, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:23,541 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_45, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:23,541 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_46, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:23,542 mps_preprocess.py:115] Visiting: aten_index_put_default_9, aten.index_put.default\n",
      "[INFO 2024-05-29 14:04:23,542 mps_preprocess.py:115] Visiting: aten__to_copy_default_4, aten._to_copy.default\n",
      "[INFO 2024-05-29 14:04:23,542 mps_preprocess.py:115] Visiting: aten_mul_tensor_71, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:23,542 mps_preprocess.py:115] Visiting: aten_mul_tensor_73, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:23,542 mps_preprocess.py:115] Visiting: aten_mul_tensor_72, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:23,543 mps_preprocess.py:115] Visiting: aten_mul_tensor_74, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:23,543 mps_preprocess.py:115] Visiting: aten_mul_tensor_75, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:23,543 mps_preprocess.py:115] Visiting: aten_mul_tensor_77, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:23,543 mps_preprocess.py:115] Visiting: aten_mul_tensor_76, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:23,543 mps_preprocess.py:115] Visiting: aten_mul_tensor_78, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:23,543 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_18, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 14:04:23,544 mps_preprocess.py:115] Visiting: aten_sub_tensor_8, aten.sub.Tensor\n",
      "[INFO 2024-05-29 14:04:23,544 mps_preprocess.py:115] Visiting: aten_add_tensor_29, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:23,544 mps_preprocess.py:115] Visiting: aten_sub_tensor_9, aten.sub.Tensor\n",
      "[INFO 2024-05-29 14:04:23,545 mps_preprocess.py:115] Visiting: aten_add_tensor_30, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:23,545 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_19, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 14:04:23,545 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_32, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:23,546 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_33, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:23,546 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_34, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:23,546 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_35, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:23,547 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_39, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:23,547 mps_preprocess.py:115] Visiting: aten_cat_default_8, aten.cat.default\n",
      "[INFO 2024-05-29 14:04:23,547 mps_preprocess.py:115] Visiting: aten_cat_default_9, aten.cat.default\n",
      "[INFO 2024-05-29 14:04:23,548 mps_preprocess.py:115] Visiting: aten_expand_copy_default_25, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:23,548 mps_preprocess.py:115] Visiting: aten_view_copy_default_87, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:23,548 mps_preprocess.py:115] Visiting: aten_view_copy_default_88, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:23,549 mps_preprocess.py:115] Visiting: aten_clone_default_9, aten.clone.default\n",
      "[INFO 2024-05-29 14:04:23,549 mps_preprocess.py:115] Visiting: aten_permute_copy_default_20, aten.permute_copy.default\n",
      "[INFO 2024-05-29 14:04:23,549 mps_preprocess.py:115] Visiting: aten_permute_copy_default_21, aten.permute_copy.default\n",
      "[INFO 2024-05-29 14:04:23,549 mps_preprocess.py:115] Visiting: aten_view_copy_default_92, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:23,550 mps_preprocess.py:115] Visiting: aten_expand_copy_default_26, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:23,550 mps_preprocess.py:115] Visiting: aten_view_copy_default_89, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:23,550 mps_preprocess.py:115] Visiting: aten_expand_copy_default_29, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:23,551 mps_preprocess.py:115] Visiting: aten_view_copy_default_93, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:23,551 mps_preprocess.py:115] Visiting: aten_index_put_default_8, aten.index_put.default\n",
      "[INFO 2024-05-29 14:04:23,551 mps_preprocess.py:115] Visiting: aten_view_copy_default_97, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:23,551 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_16, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 14:04:23,552 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_17, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 14:04:23,553 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_38, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:23,553 mps_preprocess.py:115] Visiting: aten_expand_copy_default_24, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:23,553 mps_preprocess.py:115] Visiting: aten_clone_default_8, aten.clone.default\n",
      "[INFO 2024-05-29 14:04:23,553 mps_preprocess.py:115] Visiting: aten_view_copy_default_91, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:23,554 mps_preprocess.py:115] Visiting: aten_permute_copy_default_23, aten.permute_copy.default\n",
      "[INFO 2024-05-29 14:04:23,554 mps_preprocess.py:115] Visiting: aten_expand_copy_default_27, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:23,554 mps_preprocess.py:115] Visiting: aten_view_copy_default_94, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:23,554 mps_preprocess.py:115] Visiting: aten_bmm_default_8, aten.bmm.default\n",
      "[INFO 2024-05-29 14:04:23,555 mps_preprocess.py:115] Visiting: aten_view_copy_default_95, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:23,555 mps_preprocess.py:115] Visiting: aten_mul_tensor_79, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:23,555 mps_preprocess.py:115] Visiting: aten_add_tensor_31, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:23,555 mps_preprocess.py:115] Visiting: aten__softmax_default_4, aten._softmax.default\n",
      "[INFO 2024-05-29 14:04:23,556 mps_preprocess.py:115] Visiting: aten_expand_copy_default_28, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:23,556 mps_preprocess.py:115] Visiting: aten_view_copy_default_96, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:23,556 mps_preprocess.py:115] Visiting: aten_bmm_default_9, aten.bmm.default\n",
      "[INFO 2024-05-29 14:04:23,556 mps_preprocess.py:115] Visiting: aten_view_copy_default_98, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:23,557 mps_preprocess.py:115] Visiting: aten_permute_copy_default_24, aten.permute_copy.default\n",
      "[INFO 2024-05-29 14:04:23,557 mps_preprocess.py:115] Visiting: aten_view_copy_default_99, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:23,557 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_47, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:23,785 mps_preprocess.py:115] Visiting: aten_add_tensor_27, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:23,786 mps_preprocess.py:115] Visiting: aten_mul_tensor_68, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:23,786 mps_preprocess.py:115] Visiting: aten_mean_dim_8, aten.mean.dim\n",
      "[INFO 2024-05-29 14:04:23,786 mps_preprocess.py:115] Visiting: aten_add_tensor_28, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:23,787 mps_preprocess.py:115] Visiting: aten_rsqrt_default_8, aten.rsqrt.default\n",
      "[INFO 2024-05-29 14:04:23,787 mps_preprocess.py:115] Visiting: aten_mul_tensor_69, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:23,787 mps_preprocess.py:115] Visiting: aten_mul_tensor_70, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:23,787 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_40, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:23,787 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_41, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:23,788 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_42, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:23,897 mps_preprocess.py:115] Visiting: aten_sigmoid_default_3, aten.sigmoid.default\n",
      "[INFO 2024-05-29 14:04:23,897 mps_preprocess.py:115] Visiting: aten_mul_tensor_66, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:23,898 mps_preprocess.py:115] Visiting: aten_mul_tensor_67, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:23,996 mps_preprocess.py:115] Visiting: aten_add_tensor_25, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:23,997 mps_preprocess.py:115] Visiting: aten_mul_tensor_63, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:23,997 mps_preprocess.py:115] Visiting: aten_mean_dim_7, aten.mean.dim\n",
      "[INFO 2024-05-29 14:04:23,997 mps_preprocess.py:115] Visiting: aten_add_tensor_26, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:23,998 mps_preprocess.py:115] Visiting: aten_rsqrt_default_7, aten.rsqrt.default\n",
      "[INFO 2024-05-29 14:04:23,998 mps_preprocess.py:115] Visiting: aten_mul_tensor_64, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:23,998 mps_preprocess.py:115] Visiting: aten_mul_tensor_65, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:23,998 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_38, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:23,999 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_39, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:24,104 mps_preprocess.py:115] Visiting: aten_view_copy_default_65, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,105 mps_preprocess.py:115] Visiting: aten_view_copy_default_66, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,105 mps_preprocess.py:115] Visiting: aten_view_copy_default_60, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,105 mps_preprocess.py:115] Visiting: aten_view_copy_default_61, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,105 mps_preprocess.py:115] Visiting: aten_view_copy_default_62, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,105 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_28, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:24,106 mps_preprocess.py:115] Visiting: aten_view_copy_default_63, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,106 mps_preprocess.py:115] Visiting: aten_view_copy_default_64, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,106 mps_preprocess.py:115] Visiting: aten_permute_copy_default_17, aten.permute_copy.default\n",
      "[INFO 2024-05-29 14:04:24,106 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_29, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:24,106 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_12, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 14:04:24,107 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_13, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 14:04:24,107 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_14, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 14:04:24,107 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_15, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 14:04:24,107 mps_preprocess.py:115] Visiting: aten_view_copy_default_70, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,107 mps_preprocess.py:115] Visiting: aten_index_tensor_5, aten.index.Tensor\n",
      "[INFO 2024-05-29 14:04:24,108 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_33, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:24,108 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_34, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:24,108 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_35, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:24,108 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_36, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:24,108 mps_preprocess.py:115] Visiting: aten_index_put_default_7, aten.index_put.default\n",
      "[INFO 2024-05-29 14:04:24,108 mps_preprocess.py:115] Visiting: aten__to_copy_default_3, aten._to_copy.default\n",
      "[INFO 2024-05-29 14:04:24,109 mps_preprocess.py:115] Visiting: aten_mul_tensor_54, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:24,109 mps_preprocess.py:115] Visiting: aten_mul_tensor_56, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:24,109 mps_preprocess.py:115] Visiting: aten_mul_tensor_55, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:24,109 mps_preprocess.py:115] Visiting: aten_mul_tensor_57, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:24,109 mps_preprocess.py:115] Visiting: aten_mul_tensor_58, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:24,109 mps_preprocess.py:115] Visiting: aten_mul_tensor_60, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:24,109 mps_preprocess.py:115] Visiting: aten_mul_tensor_59, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:24,110 mps_preprocess.py:115] Visiting: aten_mul_tensor_61, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:24,110 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_14, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 14:04:24,110 mps_preprocess.py:115] Visiting: aten_sub_tensor_6, aten.sub.Tensor\n",
      "[INFO 2024-05-29 14:04:24,111 mps_preprocess.py:115] Visiting: aten_add_tensor_22, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:24,111 mps_preprocess.py:115] Visiting: aten_sub_tensor_7, aten.sub.Tensor\n",
      "[INFO 2024-05-29 14:04:24,111 mps_preprocess.py:115] Visiting: aten_add_tensor_23, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:24,111 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_15, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 14:04:24,112 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_24, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:24,112 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_25, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:24,112 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_26, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:24,113 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_27, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:24,113 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_31, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:24,113 mps_preprocess.py:115] Visiting: aten_cat_default_6, aten.cat.default\n",
      "[INFO 2024-05-29 14:04:24,114 mps_preprocess.py:115] Visiting: aten_cat_default_7, aten.cat.default\n",
      "[INFO 2024-05-29 14:04:24,114 mps_preprocess.py:115] Visiting: aten_expand_copy_default_19, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:24,114 mps_preprocess.py:115] Visiting: aten_view_copy_default_67, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,114 mps_preprocess.py:115] Visiting: aten_view_copy_default_68, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,115 mps_preprocess.py:115] Visiting: aten_clone_default_7, aten.clone.default\n",
      "[INFO 2024-05-29 14:04:24,115 mps_preprocess.py:115] Visiting: aten_permute_copy_default_15, aten.permute_copy.default\n",
      "[INFO 2024-05-29 14:04:24,115 mps_preprocess.py:115] Visiting: aten_permute_copy_default_16, aten.permute_copy.default\n",
      "[INFO 2024-05-29 14:04:24,115 mps_preprocess.py:115] Visiting: aten_view_copy_default_72, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,115 mps_preprocess.py:115] Visiting: aten_expand_copy_default_20, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:24,116 mps_preprocess.py:115] Visiting: aten_view_copy_default_69, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,116 mps_preprocess.py:115] Visiting: aten_expand_copy_default_23, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:24,116 mps_preprocess.py:115] Visiting: aten_view_copy_default_73, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,116 mps_preprocess.py:115] Visiting: aten_index_put_default_6, aten.index_put.default\n",
      "[INFO 2024-05-29 14:04:24,117 mps_preprocess.py:115] Visiting: aten_view_copy_default_77, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,117 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_12, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 14:04:24,118 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_13, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 14:04:24,119 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_30, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:24,119 mps_preprocess.py:115] Visiting: aten_expand_copy_default_18, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:24,119 mps_preprocess.py:115] Visiting: aten_clone_default_6, aten.clone.default\n",
      "[INFO 2024-05-29 14:04:24,119 mps_preprocess.py:115] Visiting: aten_view_copy_default_71, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,120 mps_preprocess.py:115] Visiting: aten_permute_copy_default_18, aten.permute_copy.default\n",
      "[INFO 2024-05-29 14:04:24,120 mps_preprocess.py:115] Visiting: aten_expand_copy_default_21, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:24,120 mps_preprocess.py:115] Visiting: aten_view_copy_default_74, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,121 mps_preprocess.py:115] Visiting: aten_bmm_default_6, aten.bmm.default\n",
      "[INFO 2024-05-29 14:04:24,121 mps_preprocess.py:115] Visiting: aten_view_copy_default_75, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,121 mps_preprocess.py:115] Visiting: aten_mul_tensor_62, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:24,121 mps_preprocess.py:115] Visiting: aten_add_tensor_24, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:24,122 mps_preprocess.py:115] Visiting: aten__softmax_default_3, aten._softmax.default\n",
      "[INFO 2024-05-29 14:04:24,122 mps_preprocess.py:115] Visiting: aten_expand_copy_default_22, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:24,122 mps_preprocess.py:115] Visiting: aten_view_copy_default_76, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,122 mps_preprocess.py:115] Visiting: aten_bmm_default_7, aten.bmm.default\n",
      "[INFO 2024-05-29 14:04:24,122 mps_preprocess.py:115] Visiting: aten_view_copy_default_78, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,123 mps_preprocess.py:115] Visiting: aten_permute_copy_default_19, aten.permute_copy.default\n",
      "[INFO 2024-05-29 14:04:24,123 mps_preprocess.py:115] Visiting: aten_view_copy_default_79, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,123 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_37, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:24,351 mps_preprocess.py:115] Visiting: aten_add_tensor_20, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:24,351 mps_preprocess.py:115] Visiting: aten_mul_tensor_51, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:24,352 mps_preprocess.py:115] Visiting: aten_mean_dim_6, aten.mean.dim\n",
      "[INFO 2024-05-29 14:04:24,352 mps_preprocess.py:115] Visiting: aten_add_tensor_21, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:24,352 mps_preprocess.py:115] Visiting: aten_rsqrt_default_6, aten.rsqrt.default\n",
      "[INFO 2024-05-29 14:04:24,352 mps_preprocess.py:115] Visiting: aten_mul_tensor_52, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:24,352 mps_preprocess.py:115] Visiting: aten_mul_tensor_53, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:24,353 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_30, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:24,353 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_31, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:24,353 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_32, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:24,442 mps_preprocess.py:115] Visiting: aten_sigmoid_default_2, aten.sigmoid.default\n",
      "[INFO 2024-05-29 14:04:24,442 mps_preprocess.py:115] Visiting: aten_mul_tensor_49, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:24,443 mps_preprocess.py:115] Visiting: aten_mul_tensor_50, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:24,528 mps_preprocess.py:115] Visiting: aten_add_tensor_18, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:24,529 mps_preprocess.py:115] Visiting: aten_mul_tensor_46, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:24,529 mps_preprocess.py:115] Visiting: aten_mean_dim_5, aten.mean.dim\n",
      "[INFO 2024-05-29 14:04:24,529 mps_preprocess.py:115] Visiting: aten_add_tensor_19, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:24,530 mps_preprocess.py:115] Visiting: aten_rsqrt_default_5, aten.rsqrt.default\n",
      "[INFO 2024-05-29 14:04:24,530 mps_preprocess.py:115] Visiting: aten_mul_tensor_47, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:24,530 mps_preprocess.py:115] Visiting: aten_mul_tensor_48, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:24,530 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_28, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:24,531 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_29, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:24,631 mps_preprocess.py:115] Visiting: aten_view_copy_default_45, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,631 mps_preprocess.py:115] Visiting: aten_view_copy_default_46, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,632 mps_preprocess.py:115] Visiting: aten_view_copy_default_40, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,632 mps_preprocess.py:115] Visiting: aten_view_copy_default_41, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,632 mps_preprocess.py:115] Visiting: aten_view_copy_default_42, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,632 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_20, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:24,632 mps_preprocess.py:115] Visiting: aten_view_copy_default_43, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,633 mps_preprocess.py:115] Visiting: aten_view_copy_default_44, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,633 mps_preprocess.py:115] Visiting: aten_permute_copy_default_12, aten.permute_copy.default\n",
      "[INFO 2024-05-29 14:04:24,633 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_21, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:24,633 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_8, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 14:04:24,633 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_9, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 14:04:24,633 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_10, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 14:04:24,634 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_11, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 14:04:24,634 mps_preprocess.py:115] Visiting: aten_view_copy_default_50, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,634 mps_preprocess.py:115] Visiting: aten_index_tensor_4, aten.index.Tensor\n",
      "[INFO 2024-05-29 14:04:24,634 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_23, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:24,634 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_24, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:24,635 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_25, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:24,635 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_26, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:24,635 mps_preprocess.py:115] Visiting: aten_index_put_default_5, aten.index_put.default\n",
      "[INFO 2024-05-29 14:04:24,635 mps_preprocess.py:115] Visiting: aten__to_copy_default_2, aten._to_copy.default\n",
      "[INFO 2024-05-29 14:04:24,635 mps_preprocess.py:115] Visiting: aten_mul_tensor_37, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:24,635 mps_preprocess.py:115] Visiting: aten_mul_tensor_39, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:24,636 mps_preprocess.py:115] Visiting: aten_mul_tensor_38, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:24,636 mps_preprocess.py:115] Visiting: aten_mul_tensor_40, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:24,636 mps_preprocess.py:115] Visiting: aten_mul_tensor_41, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:24,636 mps_preprocess.py:115] Visiting: aten_mul_tensor_43, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:24,636 mps_preprocess.py:115] Visiting: aten_mul_tensor_42, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:24,636 mps_preprocess.py:115] Visiting: aten_mul_tensor_44, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:24,636 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_10, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 14:04:24,637 mps_preprocess.py:115] Visiting: aten_sub_tensor_4, aten.sub.Tensor\n",
      "[INFO 2024-05-29 14:04:24,637 mps_preprocess.py:115] Visiting: aten_add_tensor_15, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:24,638 mps_preprocess.py:115] Visiting: aten_sub_tensor_5, aten.sub.Tensor\n",
      "[INFO 2024-05-29 14:04:24,638 mps_preprocess.py:115] Visiting: aten_add_tensor_16, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:24,638 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_11, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 14:04:24,638 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_16, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:24,639 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_17, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:24,639 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_18, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:24,639 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_19, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:24,639 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_23, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:24,640 mps_preprocess.py:115] Visiting: aten_cat_default_4, aten.cat.default\n",
      "[INFO 2024-05-29 14:04:24,640 mps_preprocess.py:115] Visiting: aten_cat_default_5, aten.cat.default\n",
      "[INFO 2024-05-29 14:04:24,640 mps_preprocess.py:115] Visiting: aten_expand_copy_default_13, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:24,640 mps_preprocess.py:115] Visiting: aten_view_copy_default_47, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,640 mps_preprocess.py:115] Visiting: aten_view_copy_default_48, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,641 mps_preprocess.py:115] Visiting: aten_clone_default_5, aten.clone.default\n",
      "[INFO 2024-05-29 14:04:24,641 mps_preprocess.py:115] Visiting: aten_permute_copy_default_10, aten.permute_copy.default\n",
      "[INFO 2024-05-29 14:04:24,641 mps_preprocess.py:115] Visiting: aten_permute_copy_default_11, aten.permute_copy.default\n",
      "[INFO 2024-05-29 14:04:24,641 mps_preprocess.py:115] Visiting: aten_view_copy_default_52, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,641 mps_preprocess.py:115] Visiting: aten_expand_copy_default_14, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:24,641 mps_preprocess.py:115] Visiting: aten_view_copy_default_49, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,642 mps_preprocess.py:115] Visiting: aten_expand_copy_default_17, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:24,642 mps_preprocess.py:115] Visiting: aten_view_copy_default_53, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,642 mps_preprocess.py:115] Visiting: aten_index_put_default_4, aten.index_put.default\n",
      "[INFO 2024-05-29 14:04:24,642 mps_preprocess.py:115] Visiting: aten_view_copy_default_57, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,643 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_8, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 14:04:24,643 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_9, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 14:04:24,644 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_22, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:24,644 mps_preprocess.py:115] Visiting: aten_expand_copy_default_12, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:24,645 mps_preprocess.py:115] Visiting: aten_clone_default_4, aten.clone.default\n",
      "[INFO 2024-05-29 14:04:24,645 mps_preprocess.py:115] Visiting: aten_view_copy_default_51, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,645 mps_preprocess.py:115] Visiting: aten_permute_copy_default_13, aten.permute_copy.default\n",
      "[INFO 2024-05-29 14:04:24,645 mps_preprocess.py:115] Visiting: aten_expand_copy_default_15, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:24,646 mps_preprocess.py:115] Visiting: aten_view_copy_default_54, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,648 mps_preprocess.py:115] Visiting: aten_bmm_default_4, aten.bmm.default\n",
      "[INFO 2024-05-29 14:04:24,652 mps_preprocess.py:115] Visiting: aten_view_copy_default_55, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,656 mps_preprocess.py:115] Visiting: aten_mul_tensor_45, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:24,657 mps_preprocess.py:115] Visiting: aten_add_tensor_17, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:24,657 mps_preprocess.py:115] Visiting: aten__softmax_default_2, aten._softmax.default\n",
      "[INFO 2024-05-29 14:04:24,657 mps_preprocess.py:115] Visiting: aten_expand_copy_default_16, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:24,658 mps_preprocess.py:115] Visiting: aten_view_copy_default_56, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,658 mps_preprocess.py:115] Visiting: aten_bmm_default_5, aten.bmm.default\n",
      "[INFO 2024-05-29 14:04:24,666 mps_preprocess.py:115] Visiting: aten_view_copy_default_58, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,675 mps_preprocess.py:115] Visiting: aten_permute_copy_default_14, aten.permute_copy.default\n",
      "[INFO 2024-05-29 14:04:24,680 mps_preprocess.py:115] Visiting: aten_view_copy_default_59, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:24,685 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_27, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:24,919 mps_preprocess.py:115] Visiting: aten_add_tensor_13, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:24,920 mps_preprocess.py:115] Visiting: aten_mul_tensor_34, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:24,920 mps_preprocess.py:115] Visiting: aten_mean_dim_4, aten.mean.dim\n",
      "[INFO 2024-05-29 14:04:24,921 mps_preprocess.py:115] Visiting: aten_add_tensor_14, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:24,921 mps_preprocess.py:115] Visiting: aten_rsqrt_default_4, aten.rsqrt.default\n",
      "[INFO 2024-05-29 14:04:24,921 mps_preprocess.py:115] Visiting: aten_mul_tensor_35, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:24,921 mps_preprocess.py:115] Visiting: aten_mul_tensor_36, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:24,921 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_20, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:24,922 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_21, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:24,922 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_22, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:25,008 mps_preprocess.py:115] Visiting: aten_sigmoid_default_1, aten.sigmoid.default\n",
      "[INFO 2024-05-29 14:04:25,009 mps_preprocess.py:115] Visiting: aten_mul_tensor_32, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:25,009 mps_preprocess.py:115] Visiting: aten_mul_tensor_33, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:25,090 mps_preprocess.py:115] Visiting: aten_add_tensor_11, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:25,091 mps_preprocess.py:115] Visiting: aten_mul_tensor_29, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:25,091 mps_preprocess.py:115] Visiting: aten_mean_dim_3, aten.mean.dim\n",
      "[INFO 2024-05-29 14:04:25,092 mps_preprocess.py:115] Visiting: aten_add_tensor_12, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:25,092 mps_preprocess.py:115] Visiting: aten_rsqrt_default_3, aten.rsqrt.default\n",
      "[INFO 2024-05-29 14:04:25,092 mps_preprocess.py:115] Visiting: aten_mul_tensor_30, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:25,092 mps_preprocess.py:115] Visiting: aten_mul_tensor_31, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:25,092 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_18, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:25,093 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_19, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:25,187 mps_preprocess.py:115] Visiting: aten_view_copy_default_25, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,187 mps_preprocess.py:115] Visiting: aten_view_copy_default_26, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,187 mps_preprocess.py:115] Visiting: aten_view_copy_default_20, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,188 mps_preprocess.py:115] Visiting: aten_view_copy_default_21, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,188 mps_preprocess.py:115] Visiting: aten_view_copy_default_22, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,188 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_12, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:25,188 mps_preprocess.py:115] Visiting: aten_view_copy_default_23, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,188 mps_preprocess.py:115] Visiting: aten_view_copy_default_24, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,189 mps_preprocess.py:115] Visiting: aten_permute_copy_default_7, aten.permute_copy.default\n",
      "[INFO 2024-05-29 14:04:25,189 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_13, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:25,189 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_4, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 14:04:25,189 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_5, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 14:04:25,189 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_6, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 14:04:25,189 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_7, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 14:04:25,190 mps_preprocess.py:115] Visiting: aten_view_copy_default_30, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,190 mps_preprocess.py:115] Visiting: aten_index_tensor_3, aten.index.Tensor\n",
      "[INFO 2024-05-29 14:04:25,190 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_13, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:25,190 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_14, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:25,190 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_15, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:25,191 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_16, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:25,191 mps_preprocess.py:115] Visiting: aten_index_put_default_3, aten.index_put.default\n",
      "[INFO 2024-05-29 14:04:25,191 mps_preprocess.py:115] Visiting: aten__to_copy_default_1, aten._to_copy.default\n",
      "[INFO 2024-05-29 14:04:25,191 mps_preprocess.py:115] Visiting: aten_mul_tensor_20, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:25,191 mps_preprocess.py:115] Visiting: aten_mul_tensor_22, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:25,191 mps_preprocess.py:115] Visiting: aten_mul_tensor_21, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:25,191 mps_preprocess.py:115] Visiting: aten_mul_tensor_23, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:25,192 mps_preprocess.py:115] Visiting: aten_mul_tensor_24, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:25,192 mps_preprocess.py:115] Visiting: aten_mul_tensor_26, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:25,192 mps_preprocess.py:115] Visiting: aten_mul_tensor_25, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:25,192 mps_preprocess.py:115] Visiting: aten_mul_tensor_27, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:25,192 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_6, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 14:04:25,193 mps_preprocess.py:115] Visiting: aten_sub_tensor_2, aten.sub.Tensor\n",
      "[INFO 2024-05-29 14:04:25,193 mps_preprocess.py:115] Visiting: aten_add_tensor_8, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:25,193 mps_preprocess.py:115] Visiting: aten_sub_tensor_3, aten.sub.Tensor\n",
      "[INFO 2024-05-29 14:04:25,194 mps_preprocess.py:115] Visiting: aten_add_tensor_9, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:25,194 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_7, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 14:04:25,194 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_8, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:25,194 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_9, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:25,195 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_10, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:25,195 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_11, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:25,195 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_15, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:25,195 mps_preprocess.py:115] Visiting: aten_cat_default_2, aten.cat.default\n",
      "[INFO 2024-05-29 14:04:25,195 mps_preprocess.py:115] Visiting: aten_cat_default_3, aten.cat.default\n",
      "[INFO 2024-05-29 14:04:25,196 mps_preprocess.py:115] Visiting: aten_expand_copy_default_7, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:25,196 mps_preprocess.py:115] Visiting: aten_view_copy_default_27, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,196 mps_preprocess.py:115] Visiting: aten_view_copy_default_28, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,197 mps_preprocess.py:115] Visiting: aten_clone_default_3, aten.clone.default\n",
      "[INFO 2024-05-29 14:04:25,197 mps_preprocess.py:115] Visiting: aten_permute_copy_default_5, aten.permute_copy.default\n",
      "[INFO 2024-05-29 14:04:25,197 mps_preprocess.py:115] Visiting: aten_permute_copy_default_6, aten.permute_copy.default\n",
      "[INFO 2024-05-29 14:04:25,197 mps_preprocess.py:115] Visiting: aten_view_copy_default_32, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,198 mps_preprocess.py:115] Visiting: aten_expand_copy_default_8, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:25,198 mps_preprocess.py:115] Visiting: aten_view_copy_default_29, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,198 mps_preprocess.py:115] Visiting: aten_expand_copy_default_11, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:25,198 mps_preprocess.py:115] Visiting: aten_view_copy_default_33, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,199 mps_preprocess.py:115] Visiting: aten_index_put_default_2, aten.index_put.default\n",
      "[INFO 2024-05-29 14:04:25,199 mps_preprocess.py:115] Visiting: aten_view_copy_default_37, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,199 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_4, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 14:04:25,200 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_5, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 14:04:25,200 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_14, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:25,200 mps_preprocess.py:115] Visiting: aten_expand_copy_default_6, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:25,201 mps_preprocess.py:115] Visiting: aten_clone_default_2, aten.clone.default\n",
      "[INFO 2024-05-29 14:04:25,201 mps_preprocess.py:115] Visiting: aten_view_copy_default_31, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,201 mps_preprocess.py:115] Visiting: aten_permute_copy_default_8, aten.permute_copy.default\n",
      "[INFO 2024-05-29 14:04:25,201 mps_preprocess.py:115] Visiting: aten_expand_copy_default_9, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:25,202 mps_preprocess.py:115] Visiting: aten_view_copy_default_34, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,202 mps_preprocess.py:115] Visiting: aten_bmm_default_2, aten.bmm.default\n",
      "[INFO 2024-05-29 14:04:25,202 mps_preprocess.py:115] Visiting: aten_view_copy_default_35, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,202 mps_preprocess.py:115] Visiting: aten_mul_tensor_28, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:25,203 mps_preprocess.py:115] Visiting: aten_add_tensor_10, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:25,203 mps_preprocess.py:115] Visiting: aten__softmax_default_1, aten._softmax.default\n",
      "[INFO 2024-05-29 14:04:25,203 mps_preprocess.py:115] Visiting: aten_expand_copy_default_10, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:25,204 mps_preprocess.py:115] Visiting: aten_view_copy_default_36, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,204 mps_preprocess.py:115] Visiting: aten_bmm_default_3, aten.bmm.default\n",
      "[INFO 2024-05-29 14:04:25,204 mps_preprocess.py:115] Visiting: aten_view_copy_default_38, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,204 mps_preprocess.py:115] Visiting: aten_permute_copy_default_9, aten.permute_copy.default\n",
      "[INFO 2024-05-29 14:04:25,205 mps_preprocess.py:115] Visiting: aten_view_copy_default_39, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,205 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_17, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:25,419 mps_preprocess.py:115] Visiting: aten_add_tensor_6, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:25,420 mps_preprocess.py:115] Visiting: aten_mul_tensor_17, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:25,420 mps_preprocess.py:115] Visiting: aten_mean_dim_2, aten.mean.dim\n",
      "[INFO 2024-05-29 14:04:25,420 mps_preprocess.py:115] Visiting: aten_add_tensor_7, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:25,420 mps_preprocess.py:115] Visiting: aten_rsqrt_default_2, aten.rsqrt.default\n",
      "[INFO 2024-05-29 14:04:25,421 mps_preprocess.py:115] Visiting: aten_mul_tensor_18, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:25,421 mps_preprocess.py:115] Visiting: aten_mul_tensor_19, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:25,421 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_10, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:25,421 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_11, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:25,421 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_12, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:25,503 mps_preprocess.py:115] Visiting: aten_sigmoid_default, aten.sigmoid.default\n",
      "[INFO 2024-05-29 14:04:25,504 mps_preprocess.py:115] Visiting: aten_mul_tensor_15, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:25,504 mps_preprocess.py:115] Visiting: aten_mul_tensor_16, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:25,582 mps_preprocess.py:115] Visiting: aten_add_tensor_4, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:25,583 mps_preprocess.py:115] Visiting: aten_mul_tensor_12, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:25,583 mps_preprocess.py:115] Visiting: aten_mean_dim_1, aten.mean.dim\n",
      "[INFO 2024-05-29 14:04:25,583 mps_preprocess.py:115] Visiting: aten_add_tensor_5, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:25,583 mps_preprocess.py:115] Visiting: aten_rsqrt_default_1, aten.rsqrt.default\n",
      "[INFO 2024-05-29 14:04:25,583 mps_preprocess.py:115] Visiting: aten_mul_tensor_13, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:25,584 mps_preprocess.py:115] Visiting: aten_mul_tensor_14, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:25,584 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_8, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:25,584 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_9, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:25,674 mps_preprocess.py:115] Visiting: aten_view_copy_default, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,675 mps_preprocess.py:115] Visiting: aten_view_copy_default_1, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,675 mps_preprocess.py:115] Visiting: aten_view_copy_default_2, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,675 mps_preprocess.py:115] Visiting: aten_index_tensor, aten.index.Tensor\n",
      "[INFO 2024-05-29 14:04:25,676 mps_preprocess.py:115] Visiting: aten_index_tensor_1, aten.index.Tensor\n",
      "[INFO 2024-05-29 14:04:25,676 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_4, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:25,676 mps_preprocess.py:115] Visiting: aten_view_copy_default_3, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,676 mps_preprocess.py:115] Visiting: aten_view_copy_default_4, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,677 mps_preprocess.py:115] Visiting: aten_permute_copy_default_2, aten.permute_copy.default\n",
      "[INFO 2024-05-29 14:04:25,677 mps_preprocess.py:115] Visiting: aten_view_copy_default_5, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,677 mps_preprocess.py:115] Visiting: aten_view_copy_default_6, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,677 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_5, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:25,677 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 14:04:25,677 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_1, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 14:04:25,678 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_2, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 14:04:25,678 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_3, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-29 14:04:25,678 mps_preprocess.py:115] Visiting: aten_view_copy_default_10, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,678 mps_preprocess.py:115] Visiting: aten_index_tensor_2, aten.index.Tensor\n",
      "[INFO 2024-05-29 14:04:25,679 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_3, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:25,679 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_4, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:25,679 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_5, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:25,679 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_6, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:25,679 mps_preprocess.py:115] Visiting: aten_index_put_default_1, aten.index_put.default\n",
      "[INFO 2024-05-29 14:04:25,679 mps_preprocess.py:115] Visiting: aten__to_copy_default, aten._to_copy.default\n",
      "[INFO 2024-05-29 14:04:25,680 mps_preprocess.py:115] Visiting: aten_mul_tensor_3, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:25,680 mps_preprocess.py:115] Visiting: aten_mul_tensor_5, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:25,680 mps_preprocess.py:115] Visiting: aten_mul_tensor_4, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:25,680 mps_preprocess.py:115] Visiting: aten_mul_tensor_6, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:25,681 mps_preprocess.py:115] Visiting: aten_mul_tensor_7, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:25,681 mps_preprocess.py:115] Visiting: aten_mul_tensor_9, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:25,681 mps_preprocess.py:115] Visiting: aten_mul_tensor_8, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:25,681 mps_preprocess.py:115] Visiting: aten_mul_tensor_10, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:25,682 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_2, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 14:04:25,682 mps_preprocess.py:115] Visiting: aten_sub_tensor, aten.sub.Tensor\n",
      "[INFO 2024-05-29 14:04:25,682 mps_preprocess.py:115] Visiting: aten_add_tensor_1, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:25,683 mps_preprocess.py:115] Visiting: aten_sub_tensor_1, aten.sub.Tensor\n",
      "[INFO 2024-05-29 14:04:25,683 mps_preprocess.py:115] Visiting: aten_add_tensor_2, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:25,683 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_3, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 14:04:25,683 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:25,684 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_1, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:25,684 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_2, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:25,684 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_3, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:25,684 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_7, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:25,684 mps_preprocess.py:115] Visiting: aten_cat_default, aten.cat.default\n",
      "[INFO 2024-05-29 14:04:25,685 mps_preprocess.py:115] Visiting: aten_cat_default_1, aten.cat.default\n",
      "[INFO 2024-05-29 14:04:25,685 mps_preprocess.py:115] Visiting: aten_expand_copy_default_1, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:25,685 mps_preprocess.py:115] Visiting: aten_view_copy_default_7, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,685 mps_preprocess.py:115] Visiting: aten_view_copy_default_8, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,686 mps_preprocess.py:115] Visiting: aten_clone_default_1, aten.clone.default\n",
      "[INFO 2024-05-29 14:04:25,686 mps_preprocess.py:115] Visiting: aten_permute_copy_default, aten.permute_copy.default\n",
      "[INFO 2024-05-29 14:04:25,686 mps_preprocess.py:115] Visiting: aten_permute_copy_default_1, aten.permute_copy.default\n",
      "[INFO 2024-05-29 14:04:25,686 mps_preprocess.py:115] Visiting: aten_view_copy_default_12, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,687 mps_preprocess.py:115] Visiting: aten_expand_copy_default_2, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:25,687 mps_preprocess.py:115] Visiting: aten_view_copy_default_9, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,687 mps_preprocess.py:115] Visiting: aten_expand_copy_default_5, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:25,687 mps_preprocess.py:115] Visiting: aten_view_copy_default_13, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,688 mps_preprocess.py:115] Visiting: aten_index_put_default, aten.index_put.default\n",
      "[INFO 2024-05-29 14:04:25,688 mps_preprocess.py:115] Visiting: aten_view_copy_default_17, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,688 mps_preprocess.py:115] Visiting: aten_slice_scatter_default, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 14:04:25,689 mps_preprocess.py:115] Visiting: aten_slice_scatter_default_1, aten.slice_scatter.default\n",
      "[INFO 2024-05-29 14:04:25,690 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_6, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-29 14:04:25,690 mps_preprocess.py:115] Visiting: aten_expand_copy_default, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:25,690 mps_preprocess.py:115] Visiting: aten_clone_default, aten.clone.default\n",
      "[INFO 2024-05-29 14:04:25,690 mps_preprocess.py:115] Visiting: aten_view_copy_default_11, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,691 mps_preprocess.py:115] Visiting: aten_permute_copy_default_3, aten.permute_copy.default\n",
      "[INFO 2024-05-29 14:04:25,691 mps_preprocess.py:115] Visiting: aten_expand_copy_default_3, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:25,691 mps_preprocess.py:115] Visiting: aten_view_copy_default_14, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,691 mps_preprocess.py:115] Visiting: aten_bmm_default, aten.bmm.default\n",
      "[INFO 2024-05-29 14:04:25,692 mps_preprocess.py:115] Visiting: aten_view_copy_default_15, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,692 mps_preprocess.py:115] Visiting: aten_mul_tensor_11, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:25,692 mps_preprocess.py:115] Visiting: aten_add_tensor_3, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:25,692 mps_preprocess.py:115] Visiting: aten__softmax_default, aten._softmax.default\n",
      "[INFO 2024-05-29 14:04:25,692 mps_preprocess.py:115] Visiting: aten_expand_copy_default_4, aten.expand_copy.default\n",
      "[INFO 2024-05-29 14:04:25,693 mps_preprocess.py:115] Visiting: aten_view_copy_default_16, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,693 mps_preprocess.py:115] Visiting: aten_bmm_default_1, aten.bmm.default\n",
      "[INFO 2024-05-29 14:04:25,693 mps_preprocess.py:115] Visiting: aten_view_copy_default_18, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,693 mps_preprocess.py:115] Visiting: aten_permute_copy_default_4, aten.permute_copy.default\n",
      "[INFO 2024-05-29 14:04:25,694 mps_preprocess.py:115] Visiting: aten_view_copy_default_19, aten.view_copy.default\n",
      "[INFO 2024-05-29 14:04:25,694 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_7, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:25,915 mps_preprocess.py:115] Visiting: aten_embedding_default, aten.embedding.default\n",
      "[INFO 2024-05-29 14:04:25,916 mps_preprocess.py:115] Visiting: aten_mul_tensor, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:25,917 mps_preprocess.py:115] Visiting: aten_mean_dim, aten.mean.dim\n",
      "[INFO 2024-05-29 14:04:25,917 mps_preprocess.py:115] Visiting: aten_add_tensor, aten.add.Tensor\n",
      "[INFO 2024-05-29 14:04:25,917 mps_preprocess.py:115] Visiting: aten_rsqrt_default, aten.rsqrt.default\n",
      "[INFO 2024-05-29 14:04:25,917 mps_preprocess.py:115] Visiting: aten_mul_tensor_1, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:25,917 mps_preprocess.py:115] Visiting: aten_mul_tensor_2, aten.mul.Tensor\n",
      "[INFO 2024-05-29 14:04:25,918 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:25,918 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_1, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-29 14:04:25,918 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_2, aten.squeeze_copy.dims\n"
     ]
    }
   ],
   "source": [
    "# lowered to mps module\n",
    "builder.to_backend(partitioners)\n",
    "mps_module = builder.edge_manager._edge_programs[\"forward\"].module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[program.cpp:130] InternalConsistency verification requested but not available\n",
      "loc(\"mps_broadcast_to\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/91a344b1-f985-11ee-b563-fe8bc7981bff/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":1392:0)): error: 'anec.broadcast' op failed: input cannot be broadcasted to the target shape\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loc(\"mps_broadcast_to\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/91a344b1-f985-11ee-b563-fe8bc7981bff/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":1392:0)): error: 'anec.broadcast' op failed: input cannot be broadcasted to the target shape\n"
     ]
    }
   ],
   "source": [
    "# to executorch\n",
    "builder.to_executorch()\n",
    "from executorch.extension.pybindings._portable_lib import _load_for_executorch_from_buffer\n",
    "\n",
    "ep = _load_for_executorch_from_buffer(builder.export_program.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.8514,  1.4318, -5.8515,  ..., -5.8277, -5.8277, -5.8515]],\n",
      "       grad_fn=<NotImplemented>)\n"
     ]
    }
   ],
   "source": [
    "# eager result\n",
    "print(model.model.forward(t, pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.8514,  1.4318, -5.8515,  ..., -5.8277, -5.8277, -5.8515]],\n",
      "       grad_fn=<NotImplemented>)\n"
     ]
    }
   ],
   "source": [
    "# edge result\n",
    "print(edge_module.forward(t, pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.8514,  1.4318, -5.8515,  ..., -5.8277, -5.8277, -5.8515]],\n",
      "       grad_fn=<NotImplemented>)\n"
     ]
    }
   ],
   "source": [
    "# edge with custom op\n",
    "print(custom_op_module.forward(t, pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.8514,  1.4318, -5.8515,  ..., -5.8277, -5.8277, -5.8515]],\n",
      "       grad_fn=<NotImplemented>)\n"
     ]
    }
   ],
   "source": [
    "# mps result\n",
    "print(mps_module.forward(t, pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-1.5128,  3.8322, -1.5128,  ..., -1.5174, -1.5174, -1.5128]])]\n"
     ]
    }
   ],
   "source": [
    "#ep result\n",
    "print(ep.forward((t, pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<executorch.exir.program._program.ExecutorchProgramManager object at 0x35aba4cd0>\n"
     ]
    }
   ],
   "source": [
    "print(builder.export_program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "executorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
