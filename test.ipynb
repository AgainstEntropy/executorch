{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0.dev20240507\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from executorch.examples.models.llama2.builder import load_llama_model, DType\n",
    "from executorch.examples.models.llama2.source_transformation.quantize import WeightOnlyInt8QuantHandler\n",
    "from executorch.examples.models.llama2.source_transformation.sdpa import replace_sdpa_with_simple_sdpa, replace_sdpa_with_custom_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2024-06-07 13:54:55,708 builder.py:84] Loading model with checkpoint=/Users/larryliu/Meta-Llama-3-8B/consolidated.00.pth, params=/Users/larryliu/Meta-Llama-3-8B/config.json, use_kv_cache=True, weight_type=WeightType.LLAMA\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "PytorchStreamReader failed reading zip archive: failed finding central directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m params \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/Users/larryliu/Meta-Llama-3-8B/config.json\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m transforms \u001b[39m=\u001b[39m [\n\u001b[1;32m      6\u001b[0m     \u001b[39mlambda\u001b[39;00m m: WeightOnlyInt8QuantHandler(m)\u001b[39m.\u001b[39mquantized_model(),\n\u001b[1;32m      7\u001b[0m     replace_sdpa_with_custom_op,\n\u001b[1;32m      8\u001b[0m ]\n\u001b[0;32m---> 10\u001b[0m model \u001b[39m=\u001b[39m load_llama_model(\n\u001b[1;32m     11\u001b[0m     checkpoint\u001b[39m=\u001b[39;49mcheckpoint,\n\u001b[1;32m     12\u001b[0m     params_path\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m     13\u001b[0m     use_kv_cache\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     14\u001b[0m     use_sdpa_with_kv_cache\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m )\u001b[39m.\u001b[39mto_dtype(DType\u001b[39m.\u001b[39mfp32)\u001b[39m.\u001b[39msource_transform(transforms)\n",
      "File \u001b[0;32m~/miniconda3/envs/executorch/lib/python3.11/site-packages/executorch/examples/models/llama2/builder.py:87\u001b[0m, in \u001b[0;36mload_llama_model\u001b[0;34m(modelname, checkpoint, checkpoint_dir, params_path, use_kv_cache, use_sdpa_with_kv_cache, weight_type, verbose, max_seq_len)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39massert\u001b[39;00m (\n\u001b[1;32m     82\u001b[0m     checkpoint \u001b[39mor\u001b[39;00m checkpoint_dir\n\u001b[1;32m     83\u001b[0m ) \u001b[39mand\u001b[39;00m params_path, \u001b[39m\"\u001b[39m\u001b[39mBoth checkpoint/checkpoint_dir and params can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be empty\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m logging\u001b[39m.\u001b[39minfo(\n\u001b[1;32m     85\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoading model with checkpoint=\u001b[39m\u001b[39m{\u001b[39;00mcheckpoint\u001b[39m}\u001b[39;00m\u001b[39m, params=\u001b[39m\u001b[39m{\u001b[39;00mparams_path\u001b[39m}\u001b[39;00m\u001b[39m, use_kv_cache=\u001b[39m\u001b[39m{\u001b[39;00muse_kv_cache\u001b[39m}\u001b[39;00m\u001b[39m, weight_type=\u001b[39m\u001b[39m{\u001b[39;00mweight_type\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     86\u001b[0m )\n\u001b[0;32m---> 87\u001b[0m model, example_inputs, _ \u001b[39m=\u001b[39m EagerModelFactory\u001b[39m.\u001b[39;49mcreate_model(\n\u001b[1;32m     88\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mllama2\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     89\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mLlama2Model\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     90\u001b[0m     checkpoint\u001b[39m=\u001b[39;49mcheckpoint,\n\u001b[1;32m     91\u001b[0m     checkpoint_dir\u001b[39m=\u001b[39;49mcheckpoint_dir,\n\u001b[1;32m     92\u001b[0m     params\u001b[39m=\u001b[39;49mparams_path,\n\u001b[1;32m     93\u001b[0m     use_kv_cache\u001b[39m=\u001b[39;49muse_kv_cache,\n\u001b[1;32m     94\u001b[0m     use_sdpa_with_kv_cache\u001b[39m=\u001b[39;49muse_sdpa_with_kv_cache,\n\u001b[1;32m     95\u001b[0m     fairseq2\u001b[39m=\u001b[39;49mweight_type \u001b[39m==\u001b[39;49m WeightType\u001b[39m.\u001b[39;49mFAIRSEQ2,\n\u001b[1;32m     96\u001b[0m     max_seq_len\u001b[39m=\u001b[39;49mmax_seq_len,\n\u001b[1;32m     97\u001b[0m )\n\u001b[1;32m     98\u001b[0m state_dict \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mstate_dict()\n\u001b[1;32m     99\u001b[0m dtype \u001b[39m=\u001b[39m state_dict[\u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(state_dict))]\u001b[39m.\u001b[39mdtype\n",
      "File \u001b[0;32m~/miniconda3/envs/executorch/lib/python3.11/site-packages/executorch/examples/models/model_factory.py:44\u001b[0m, in \u001b[0;36mEagerModelFactory.create_model\u001b[0;34m(module_name, model_class_name, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(module, model_class_name):\n\u001b[1;32m     43\u001b[0m     model_class \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(module, model_class_name)\n\u001b[0;32m---> 44\u001b[0m     model \u001b[39m=\u001b[39m model_class(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39mget_dynamic_shapes\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     46\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m     47\u001b[0m             model\u001b[39m.\u001b[39mget_eager_model(),\n\u001b[1;32m     48\u001b[0m             model\u001b[39m.\u001b[39mget_example_inputs(),\n\u001b[1;32m     49\u001b[0m             model\u001b[39m.\u001b[39mget_dynamic_shapes(),\n\u001b[1;32m     50\u001b[0m         )\n",
      "File \u001b[0;32m~/CLionProjects/executorch/examples/models/llama2/model.py:111\u001b[0m, in \u001b[0;36mLlama2Model.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m             checkpoint[key] \u001b[39m=\u001b[39m cps[\u001b[39m0\u001b[39m][key]\n\u001b[1;32m    110\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m     checkpoint \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(checkpoint_path, map_location\u001b[39m=\u001b[39;49mdevice, mmap\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    112\u001b[0m fairseq2_checkpoint \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mfairseq2\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    113\u001b[0m \u001b[39mif\u001b[39;00m fairseq2_checkpoint:\n",
      "File \u001b[0;32m~/miniconda3/envs/executorch/lib/python3.11/site-packages/torch/serialization.py:1046\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1042\u001b[0m     f_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(f, \u001b[39mstr\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mf\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1043\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmmap can only be used with files saved with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1044\u001b[0m                        \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`torch.save(\u001b[39m\u001b[39m{\u001b[39;00mf_name\u001b[39m}\u001b[39;00m\u001b[39m_use_new_zipfile_serialization=True), \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1045\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1046\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[1;32m   1047\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1048\u001b[0m         \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, _weights_only_unpickler, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m~/miniconda3/envs/executorch/lib/python3.11/site-packages/torch/serialization.py:498\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory"
     ]
    }
   ],
   "source": [
    "# stories110M\n",
    "\n",
    "checkpoint = \"stories110M.pt\"\n",
    "params = \"params_110M.json\"\n",
    "transforms = [\n",
    "    lambda m: WeightOnlyInt8QuantHandler(m).quantized_model(),\n",
    "    replace_sdpa_with_custom_op,\n",
    "]\n",
    "\n",
    "model = load_llama_model(\n",
    "    checkpoint=checkpoint,\n",
    "    params_path=params,\n",
    "    use_kv_cache=True,\n",
    "    use_sdpa_with_kv_cache=True\n",
    ").to_dtype(DType.fp32).source_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(model.model.layers[0].attention.n_local_kv_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from executorch.backends.apple.mps.partition.mps_partitioner import (\n",
    "    MPSPartitioner,\n",
    ")\n",
    "from executorch.exir.backend.backend_details import CompileSpec\n",
    "compile_specs = [CompileSpec(\"use_fp16\", bytes([True]))]\n",
    "\n",
    "partitioners = [\n",
    "    MPSPartitioner(compile_specs)\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass to replace aten::_weight_int8pack_mm to llama_cpp::_weight_int8pack_mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from executorch.exir.pass_base import ExportPass\n",
    "from executorch.examples.models.llama2.custom_ops.llama_cpp_linear import *\n",
    "from executorch.exir.dialects._ops import ops as exir_ops\n",
    "\n",
    "class ReplaceMMPass(ExportPass):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def call_operator(self, op, args, kwargs, meta):\n",
    "        if op == exir_ops.edge.aten._weight_int8pack_mm.default:\n",
    "            return super().call_operator(exir_ops.edge.llama_cpp._weight_int8pack_mm.default, args, kwargs, meta)\n",
    "        else:\n",
    "            return super().call_operator(op, args, kwargs, meta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2024-05-31 16:58:16,522 mps_partitioner.py:121] Found 7 subgraphs to be partitioned.\n",
      "[INFO 2024-05-31 16:58:16,578 mps_preprocess.py:115] Visiting: aten_view_copy_default_49, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:16,578 mps_preprocess.py:115] Visiting: aten_view_copy_default_55, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:16,579 mps_preprocess.py:115] Visiting: aten_view_copy_default_56, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:16,579 mps_preprocess.py:115] Visiting: aten__to_copy_default_31, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:16,580 mps_preprocess.py:115] Visiting: aten__to_copy_default_32, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:16,580 mps_preprocess.py:115] Visiting: aten__to_copy_default_33, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:16,580 mps_preprocess.py:115] Visiting: aten__to_copy_default_34, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:16,580 mps_preprocess.py:115] Visiting: aten__to_copy_default_35, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:16,581 mps_preprocess.py:115] Visiting: aten__to_copy_default_36, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:16,581 mps_preprocess.py:115] Visiting: aten__to_copy_default_37, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:16,581 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_47, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:16,582 mps_preprocess.py:115] Visiting: aten_permute_copy_default_31, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:16,582 mps_preprocess.py:115] Visiting: aten_permute_copy_default_32, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:16,582 mps_preprocess.py:115] Visiting: aten_permute_copy_default_33, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:16,582 mps_preprocess.py:115] Visiting: aten_permute_copy_default_34, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:16,583 mps_preprocess.py:115] Visiting: aten_permute_copy_default_35, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:16,583 mps_preprocess.py:115] Visiting: aten_permute_copy_default_36, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:16,583 mps_preprocess.py:115] Visiting: aten_permute_copy_default_37, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:16,583 mps_preprocess.py:115] Visiting: aten_mm_default_31, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:16,583 mps_preprocess.py:115] Visiting: aten_mul_tensor_106, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:16,584 mps_preprocess.py:115] Visiting: aten_add_tensor_27, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:16,584 mps_preprocess.py:115] Visiting: aten_mul_tensor_107, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:16,584 mps_preprocess.py:115] Visiting: aten_mean_dim_9, aten.mean.dim\n",
      "[INFO 2024-05-31 16:58:16,585 mps_preprocess.py:115] Visiting: aten_add_tensor_28, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:16,585 mps_preprocess.py:115] Visiting: aten_rsqrt_default_9, aten.rsqrt.default\n",
      "[INFO 2024-05-31 16:58:16,585 mps_preprocess.py:115] Visiting: aten_mul_tensor_108, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:16,585 mps_preprocess.py:115] Visiting: aten_mul_tensor_109, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:16,586 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_48, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:16,586 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_49, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:16,586 mps_preprocess.py:115] Visiting: aten_mm_default_32, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:16,586 mps_preprocess.py:115] Visiting: aten_mm_default_33, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:16,587 mps_preprocess.py:115] Visiting: aten_mul_tensor_110, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:16,587 mps_preprocess.py:115] Visiting: aten_mul_tensor_112, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:16,587 mps_preprocess.py:115] Visiting: aten_sigmoid_default_4, aten.sigmoid.default\n",
      "[INFO 2024-05-31 16:58:16,587 mps_preprocess.py:115] Visiting: aten_mul_tensor_111, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:16,588 mps_preprocess.py:115] Visiting: aten_mul_tensor_113, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:16,588 mps_preprocess.py:115] Visiting: aten_mm_default_34, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:16,588 mps_preprocess.py:115] Visiting: aten_mul_tensor_114, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:16,588 mps_preprocess.py:115] Visiting: aten_add_tensor_29, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:16,589 mps_preprocess.py:115] Visiting: aten_mul_tensor_115, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:16,589 mps_preprocess.py:115] Visiting: aten_mean_dim_10, aten.mean.dim\n",
      "[INFO 2024-05-31 16:58:16,589 mps_preprocess.py:115] Visiting: aten_add_tensor_30, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:16,589 mps_preprocess.py:115] Visiting: aten_rsqrt_default_10, aten.rsqrt.default\n",
      "[INFO 2024-05-31 16:58:16,589 mps_preprocess.py:115] Visiting: aten_mul_tensor_116, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:16,590 mps_preprocess.py:115] Visiting: aten_mul_tensor_117, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:16,590 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_50, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:16,590 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_51, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:16,590 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_52, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:16,591 mps_preprocess.py:115] Visiting: aten_mm_default_35, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:16,591 mps_preprocess.py:115] Visiting: aten_mm_default_36, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:16,591 mps_preprocess.py:115] Visiting: aten_mm_default_37, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:16,591 mps_preprocess.py:115] Visiting: aten_mul_tensor_118, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:16,731 mps_preprocess.py:115] Visiting: aten_mul_tensor_119, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:16,731 mps_preprocess.py:115] Visiting: aten_mul_tensor_120, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:16,731 mps_preprocess.py:115] Visiting: aten_view_copy_default_50, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:16,732 mps_preprocess.py:115] Visiting: aten_view_copy_default_51, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:16,732 mps_preprocess.py:115] Visiting: aten_view_copy_default_52, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:16,732 mps_preprocess.py:115] Visiting: aten_view_copy_default_53, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:16,732 mps_preprocess.py:115] Visiting: aten_view_copy_default_54, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:16,733 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_20, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 16:58:16,733 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_21, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 16:58:16,733 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_22, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 16:58:16,733 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_23, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 16:58:16,733 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_53, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:16,734 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_54, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:16,734 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_55, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:16,734 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_56, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:16,734 mps_preprocess.py:115] Visiting: aten_mul_tensor_121, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:16,735 mps_preprocess.py:115] Visiting: aten_mul_tensor_123, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:16,735 mps_preprocess.py:115] Visiting: aten_mul_tensor_122, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:16,735 mps_preprocess.py:115] Visiting: aten_mul_tensor_124, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:16,735 mps_preprocess.py:115] Visiting: aten_mul_tensor_125, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:16,736 mps_preprocess.py:115] Visiting: aten_mul_tensor_127, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:16,736 mps_preprocess.py:115] Visiting: aten_mul_tensor_126, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:16,736 mps_preprocess.py:115] Visiting: aten_mul_tensor_128, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:16,736 mps_preprocess.py:115] Visiting: aten_sub_tensor_10, aten.sub.Tensor\n",
      "[INFO 2024-05-31 16:58:16,737 mps_preprocess.py:115] Visiting: aten_add_tensor_31, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:16,737 mps_preprocess.py:115] Visiting: aten_sub_tensor_11, aten.sub.Tensor\n",
      "[INFO 2024-05-31 16:58:16,737 mps_preprocess.py:115] Visiting: aten_add_tensor_32, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:16,738 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_20, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 16:58:16,738 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_21, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 16:58:16,738 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_22, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 16:58:16,738 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_23, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 16:58:16,739 mps_preprocess.py:115] Visiting: aten_cat_default_10, aten.cat.default\n",
      "[INFO 2024-05-31 16:58:16,739 mps_preprocess.py:115] Visiting: aten_cat_default_11, aten.cat.default\n",
      "[INFO 2024-05-31 16:58:16,739 mps_preprocess.py:115] Visiting: aten_view_copy_default_57, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:16,739 mps_preprocess.py:115] Visiting: aten_view_copy_default_58, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:17,076 mps_preprocess.py:115] Visiting: aten_view_copy_default_45, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:17,077 mps_preprocess.py:115] Visiting: aten_view_copy_default_46, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:17,078 mps_preprocess.py:115] Visiting: aten_view_copy_default_39, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:17,078 mps_preprocess.py:115] Visiting: aten__to_copy_default_24, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:17,078 mps_preprocess.py:115] Visiting: aten__to_copy_default_25, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:17,078 mps_preprocess.py:115] Visiting: aten__to_copy_default_26, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:17,078 mps_preprocess.py:115] Visiting: aten__to_copy_default_27, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:17,079 mps_preprocess.py:115] Visiting: aten__to_copy_default_28, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:17,079 mps_preprocess.py:115] Visiting: aten__to_copy_default_29, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:17,079 mps_preprocess.py:115] Visiting: aten__to_copy_default_30, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:17,080 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_37, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:17,080 mps_preprocess.py:115] Visiting: aten_permute_copy_default_24, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:17,080 mps_preprocess.py:115] Visiting: aten_permute_copy_default_25, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:17,080 mps_preprocess.py:115] Visiting: aten_permute_copy_default_26, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:17,081 mps_preprocess.py:115] Visiting: aten_permute_copy_default_27, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:17,081 mps_preprocess.py:115] Visiting: aten_permute_copy_default_28, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:17,081 mps_preprocess.py:115] Visiting: aten_permute_copy_default_29, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:17,081 mps_preprocess.py:115] Visiting: aten_permute_copy_default_30, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:17,082 mps_preprocess.py:115] Visiting: aten_mm_default_24, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:17,082 mps_preprocess.py:115] Visiting: aten_mul_tensor_83, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,082 mps_preprocess.py:115] Visiting: aten_add_tensor_21, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:17,082 mps_preprocess.py:115] Visiting: aten_mul_tensor_84, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,082 mps_preprocess.py:115] Visiting: aten_mean_dim_7, aten.mean.dim\n",
      "[INFO 2024-05-31 16:58:17,083 mps_preprocess.py:115] Visiting: aten_add_tensor_22, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:17,083 mps_preprocess.py:115] Visiting: aten_rsqrt_default_7, aten.rsqrt.default\n",
      "[INFO 2024-05-31 16:58:17,083 mps_preprocess.py:115] Visiting: aten_mul_tensor_85, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,083 mps_preprocess.py:115] Visiting: aten_mul_tensor_86, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,084 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_38, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:17,084 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_39, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:17,084 mps_preprocess.py:115] Visiting: aten_mm_default_25, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:17,084 mps_preprocess.py:115] Visiting: aten_mm_default_26, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:17,085 mps_preprocess.py:115] Visiting: aten_mul_tensor_87, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,085 mps_preprocess.py:115] Visiting: aten_mul_tensor_89, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,085 mps_preprocess.py:115] Visiting: aten_sigmoid_default_3, aten.sigmoid.default\n",
      "[INFO 2024-05-31 16:58:17,085 mps_preprocess.py:115] Visiting: aten_mul_tensor_88, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,086 mps_preprocess.py:115] Visiting: aten_mul_tensor_90, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,086 mps_preprocess.py:115] Visiting: aten_mm_default_27, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:17,086 mps_preprocess.py:115] Visiting: aten_mul_tensor_91, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,086 mps_preprocess.py:115] Visiting: aten_add_tensor_23, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:17,086 mps_preprocess.py:115] Visiting: aten_mul_tensor_92, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,087 mps_preprocess.py:115] Visiting: aten_mean_dim_8, aten.mean.dim\n",
      "[INFO 2024-05-31 16:58:17,087 mps_preprocess.py:115] Visiting: aten_add_tensor_24, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:17,087 mps_preprocess.py:115] Visiting: aten_rsqrt_default_8, aten.rsqrt.default\n",
      "[INFO 2024-05-31 16:58:17,088 mps_preprocess.py:115] Visiting: aten_mul_tensor_93, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,088 mps_preprocess.py:115] Visiting: aten_mul_tensor_94, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,088 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_40, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:17,088 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_41, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:17,088 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_42, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:17,089 mps_preprocess.py:115] Visiting: aten_mm_default_28, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:17,089 mps_preprocess.py:115] Visiting: aten_mm_default_29, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:17,089 mps_preprocess.py:115] Visiting: aten_mm_default_30, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:17,090 mps_preprocess.py:115] Visiting: aten_mul_tensor_95, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,090 mps_preprocess.py:115] Visiting: aten_mul_tensor_96, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,090 mps_preprocess.py:115] Visiting: aten_mul_tensor_97, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,091 mps_preprocess.py:115] Visiting: aten_view_copy_default_40, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:17,091 mps_preprocess.py:115] Visiting: aten_view_copy_default_41, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:17,091 mps_preprocess.py:115] Visiting: aten_view_copy_default_42, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:17,092 mps_preprocess.py:115] Visiting: aten_view_copy_default_43, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:17,092 mps_preprocess.py:115] Visiting: aten_view_copy_default_44, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:17,093 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_16, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 16:58:17,093 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_17, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 16:58:17,093 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_18, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 16:58:17,094 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_19, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 16:58:17,094 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_43, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:17,094 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_44, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:17,094 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_45, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:17,095 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_46, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:17,095 mps_preprocess.py:115] Visiting: aten_mul_tensor_98, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,095 mps_preprocess.py:115] Visiting: aten_mul_tensor_100, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,095 mps_preprocess.py:115] Visiting: aten_mul_tensor_99, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,096 mps_preprocess.py:115] Visiting: aten_mul_tensor_101, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,096 mps_preprocess.py:115] Visiting: aten_mul_tensor_102, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,096 mps_preprocess.py:115] Visiting: aten_mul_tensor_104, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,097 mps_preprocess.py:115] Visiting: aten_mul_tensor_103, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,097 mps_preprocess.py:115] Visiting: aten_mul_tensor_105, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,097 mps_preprocess.py:115] Visiting: aten_sub_tensor_8, aten.sub.Tensor\n",
      "[INFO 2024-05-31 16:58:17,098 mps_preprocess.py:115] Visiting: aten_add_tensor_25, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:17,098 mps_preprocess.py:115] Visiting: aten_sub_tensor_9, aten.sub.Tensor\n",
      "[INFO 2024-05-31 16:58:17,098 mps_preprocess.py:115] Visiting: aten_add_tensor_26, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:17,098 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_16, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 16:58:17,099 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_17, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 16:58:17,099 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_18, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 16:58:17,099 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_19, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 16:58:17,099 mps_preprocess.py:115] Visiting: aten_cat_default_8, aten.cat.default\n",
      "[INFO 2024-05-31 16:58:17,100 mps_preprocess.py:115] Visiting: aten_cat_default_9, aten.cat.default\n",
      "[INFO 2024-05-31 16:58:17,100 mps_preprocess.py:115] Visiting: aten_view_copy_default_47, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:17,100 mps_preprocess.py:115] Visiting: aten_view_copy_default_48, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:17,362 mps_preprocess.py:115] Visiting: aten_view_copy_default_35, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:17,363 mps_preprocess.py:115] Visiting: aten_view_copy_default_36, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:17,364 mps_preprocess.py:115] Visiting: aten_view_copy_default_29, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:17,364 mps_preprocess.py:115] Visiting: aten__to_copy_default_17, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:17,364 mps_preprocess.py:115] Visiting: aten__to_copy_default_18, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:17,364 mps_preprocess.py:115] Visiting: aten__to_copy_default_19, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:17,365 mps_preprocess.py:115] Visiting: aten__to_copy_default_20, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:17,365 mps_preprocess.py:115] Visiting: aten__to_copy_default_21, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:17,365 mps_preprocess.py:115] Visiting: aten__to_copy_default_22, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:17,365 mps_preprocess.py:115] Visiting: aten__to_copy_default_23, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:17,365 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_27, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:17,366 mps_preprocess.py:115] Visiting: aten_permute_copy_default_17, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:17,366 mps_preprocess.py:115] Visiting: aten_permute_copy_default_18, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:17,366 mps_preprocess.py:115] Visiting: aten_permute_copy_default_19, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:17,366 mps_preprocess.py:115] Visiting: aten_permute_copy_default_20, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:17,367 mps_preprocess.py:115] Visiting: aten_permute_copy_default_21, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:17,367 mps_preprocess.py:115] Visiting: aten_permute_copy_default_22, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:17,367 mps_preprocess.py:115] Visiting: aten_permute_copy_default_23, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:17,367 mps_preprocess.py:115] Visiting: aten_mm_default_17, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:17,367 mps_preprocess.py:115] Visiting: aten_mul_tensor_60, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,367 mps_preprocess.py:115] Visiting: aten_add_tensor_15, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:17,368 mps_preprocess.py:115] Visiting: aten_mul_tensor_61, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,368 mps_preprocess.py:115] Visiting: aten_mean_dim_5, aten.mean.dim\n",
      "[INFO 2024-05-31 16:58:17,368 mps_preprocess.py:115] Visiting: aten_add_tensor_16, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:17,368 mps_preprocess.py:115] Visiting: aten_rsqrt_default_5, aten.rsqrt.default\n",
      "[INFO 2024-05-31 16:58:17,368 mps_preprocess.py:115] Visiting: aten_mul_tensor_62, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,369 mps_preprocess.py:115] Visiting: aten_mul_tensor_63, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,369 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_28, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:17,369 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_29, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:17,370 mps_preprocess.py:115] Visiting: aten_mm_default_18, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:17,370 mps_preprocess.py:115] Visiting: aten_mm_default_19, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:17,370 mps_preprocess.py:115] Visiting: aten_mul_tensor_64, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,370 mps_preprocess.py:115] Visiting: aten_mul_tensor_66, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,371 mps_preprocess.py:115] Visiting: aten_sigmoid_default_2, aten.sigmoid.default\n",
      "[INFO 2024-05-31 16:58:17,371 mps_preprocess.py:115] Visiting: aten_mul_tensor_65, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,371 mps_preprocess.py:115] Visiting: aten_mul_tensor_67, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,371 mps_preprocess.py:115] Visiting: aten_mm_default_20, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:17,371 mps_preprocess.py:115] Visiting: aten_mul_tensor_68, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,372 mps_preprocess.py:115] Visiting: aten_add_tensor_17, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:17,372 mps_preprocess.py:115] Visiting: aten_mul_tensor_69, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,372 mps_preprocess.py:115] Visiting: aten_mean_dim_6, aten.mean.dim\n",
      "[INFO 2024-05-31 16:58:17,373 mps_preprocess.py:115] Visiting: aten_add_tensor_18, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:17,373 mps_preprocess.py:115] Visiting: aten_rsqrt_default_6, aten.rsqrt.default\n",
      "[INFO 2024-05-31 16:58:17,373 mps_preprocess.py:115] Visiting: aten_mul_tensor_70, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,373 mps_preprocess.py:115] Visiting: aten_mul_tensor_71, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,373 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_30, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:17,374 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_31, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:17,374 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_32, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:17,374 mps_preprocess.py:115] Visiting: aten_mm_default_21, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:17,374 mps_preprocess.py:115] Visiting: aten_mm_default_22, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:17,375 mps_preprocess.py:115] Visiting: aten_mm_default_23, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:17,375 mps_preprocess.py:115] Visiting: aten_mul_tensor_72, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,375 mps_preprocess.py:115] Visiting: aten_mul_tensor_73, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,376 mps_preprocess.py:115] Visiting: aten_mul_tensor_74, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,377 mps_preprocess.py:115] Visiting: aten_view_copy_default_30, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:17,377 mps_preprocess.py:115] Visiting: aten_view_copy_default_31, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:17,378 mps_preprocess.py:115] Visiting: aten_view_copy_default_32, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:17,378 mps_preprocess.py:115] Visiting: aten_view_copy_default_33, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:17,379 mps_preprocess.py:115] Visiting: aten_view_copy_default_34, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:17,379 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_12, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 16:58:17,379 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_13, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 16:58:17,380 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_14, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 16:58:17,380 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_15, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 16:58:17,380 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_33, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:17,380 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_34, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:17,381 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_35, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:17,382 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_36, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:17,382 mps_preprocess.py:115] Visiting: aten_mul_tensor_75, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,383 mps_preprocess.py:115] Visiting: aten_mul_tensor_77, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,383 mps_preprocess.py:115] Visiting: aten_mul_tensor_76, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,383 mps_preprocess.py:115] Visiting: aten_mul_tensor_78, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,384 mps_preprocess.py:115] Visiting: aten_mul_tensor_79, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,385 mps_preprocess.py:115] Visiting: aten_mul_tensor_81, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,386 mps_preprocess.py:115] Visiting: aten_mul_tensor_80, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,386 mps_preprocess.py:115] Visiting: aten_mul_tensor_82, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,387 mps_preprocess.py:115] Visiting: aten_sub_tensor_6, aten.sub.Tensor\n",
      "[INFO 2024-05-31 16:58:17,388 mps_preprocess.py:115] Visiting: aten_add_tensor_19, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:17,389 mps_preprocess.py:115] Visiting: aten_sub_tensor_7, aten.sub.Tensor\n",
      "[INFO 2024-05-31 16:58:17,390 mps_preprocess.py:115] Visiting: aten_add_tensor_20, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:17,391 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_12, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 16:58:17,392 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_13, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 16:58:17,392 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_14, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 16:58:17,392 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_15, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 16:58:17,393 mps_preprocess.py:115] Visiting: aten_cat_default_6, aten.cat.default\n",
      "[INFO 2024-05-31 16:58:17,393 mps_preprocess.py:115] Visiting: aten_cat_default_7, aten.cat.default\n",
      "[INFO 2024-05-31 16:58:17,393 mps_preprocess.py:115] Visiting: aten_view_copy_default_37, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:17,394 mps_preprocess.py:115] Visiting: aten_view_copy_default_38, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:17,645 mps_preprocess.py:115] Visiting: aten_view_copy_default_25, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:17,646 mps_preprocess.py:115] Visiting: aten_view_copy_default_26, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:17,646 mps_preprocess.py:115] Visiting: aten_view_copy_default_19, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:17,647 mps_preprocess.py:115] Visiting: aten__to_copy_default_10, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:17,647 mps_preprocess.py:115] Visiting: aten__to_copy_default_11, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:17,647 mps_preprocess.py:115] Visiting: aten__to_copy_default_12, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:17,647 mps_preprocess.py:115] Visiting: aten__to_copy_default_13, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:17,648 mps_preprocess.py:115] Visiting: aten__to_copy_default_14, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:17,648 mps_preprocess.py:115] Visiting: aten__to_copy_default_15, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:17,648 mps_preprocess.py:115] Visiting: aten__to_copy_default_16, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:17,648 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_17, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:17,649 mps_preprocess.py:115] Visiting: aten_permute_copy_default_10, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:17,649 mps_preprocess.py:115] Visiting: aten_permute_copy_default_11, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:17,649 mps_preprocess.py:115] Visiting: aten_permute_copy_default_12, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:17,650 mps_preprocess.py:115] Visiting: aten_permute_copy_default_13, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:17,650 mps_preprocess.py:115] Visiting: aten_permute_copy_default_14, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:17,650 mps_preprocess.py:115] Visiting: aten_permute_copy_default_15, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:17,650 mps_preprocess.py:115] Visiting: aten_permute_copy_default_16, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:17,650 mps_preprocess.py:115] Visiting: aten_mm_default_10, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:17,651 mps_preprocess.py:115] Visiting: aten_mul_tensor_37, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,651 mps_preprocess.py:115] Visiting: aten_add_tensor_9, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:17,651 mps_preprocess.py:115] Visiting: aten_mul_tensor_38, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,651 mps_preprocess.py:115] Visiting: aten_mean_dim_3, aten.mean.dim\n",
      "[INFO 2024-05-31 16:58:17,652 mps_preprocess.py:115] Visiting: aten_add_tensor_10, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:17,652 mps_preprocess.py:115] Visiting: aten_rsqrt_default_3, aten.rsqrt.default\n",
      "[INFO 2024-05-31 16:58:17,652 mps_preprocess.py:115] Visiting: aten_mul_tensor_39, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,652 mps_preprocess.py:115] Visiting: aten_mul_tensor_40, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,653 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_18, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:17,653 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_19, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:17,653 mps_preprocess.py:115] Visiting: aten_mm_default_11, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:17,653 mps_preprocess.py:115] Visiting: aten_mm_default_12, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:17,653 mps_preprocess.py:115] Visiting: aten_mul_tensor_41, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,654 mps_preprocess.py:115] Visiting: aten_mul_tensor_43, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,654 mps_preprocess.py:115] Visiting: aten_sigmoid_default_1, aten.sigmoid.default\n",
      "[INFO 2024-05-31 16:58:17,654 mps_preprocess.py:115] Visiting: aten_mul_tensor_42, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,654 mps_preprocess.py:115] Visiting: aten_mul_tensor_44, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,655 mps_preprocess.py:115] Visiting: aten_mm_default_13, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:17,655 mps_preprocess.py:115] Visiting: aten_mul_tensor_45, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,655 mps_preprocess.py:115] Visiting: aten_add_tensor_11, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:17,656 mps_preprocess.py:115] Visiting: aten_mul_tensor_46, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,656 mps_preprocess.py:115] Visiting: aten_mean_dim_4, aten.mean.dim\n",
      "[INFO 2024-05-31 16:58:17,656 mps_preprocess.py:115] Visiting: aten_add_tensor_12, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:17,656 mps_preprocess.py:115] Visiting: aten_rsqrt_default_4, aten.rsqrt.default\n",
      "[INFO 2024-05-31 16:58:17,657 mps_preprocess.py:115] Visiting: aten_mul_tensor_47, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,657 mps_preprocess.py:115] Visiting: aten_mul_tensor_48, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,657 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_20, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:17,657 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_21, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:17,657 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_22, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:17,658 mps_preprocess.py:115] Visiting: aten_mm_default_14, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:17,658 mps_preprocess.py:115] Visiting: aten_mm_default_15, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:17,658 mps_preprocess.py:115] Visiting: aten_mm_default_16, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:17,659 mps_preprocess.py:115] Visiting: aten_mul_tensor_49, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,659 mps_preprocess.py:115] Visiting: aten_mul_tensor_50, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,660 mps_preprocess.py:115] Visiting: aten_mul_tensor_51, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,660 mps_preprocess.py:115] Visiting: aten_view_copy_default_20, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:17,660 mps_preprocess.py:115] Visiting: aten_view_copy_default_21, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:17,661 mps_preprocess.py:115] Visiting: aten_view_copy_default_22, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:17,661 mps_preprocess.py:115] Visiting: aten_view_copy_default_23, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:17,661 mps_preprocess.py:115] Visiting: aten_view_copy_default_24, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:17,661 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_8, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 16:58:17,662 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_9, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 16:58:17,662 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_10, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 16:58:17,662 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_11, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 16:58:17,663 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_23, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:17,663 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_24, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:17,663 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_25, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:17,663 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_26, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:17,664 mps_preprocess.py:115] Visiting: aten_mul_tensor_52, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,664 mps_preprocess.py:115] Visiting: aten_mul_tensor_54, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,664 mps_preprocess.py:115] Visiting: aten_mul_tensor_53, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,665 mps_preprocess.py:115] Visiting: aten_mul_tensor_55, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,665 mps_preprocess.py:115] Visiting: aten_mul_tensor_56, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,665 mps_preprocess.py:115] Visiting: aten_mul_tensor_58, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,665 mps_preprocess.py:115] Visiting: aten_mul_tensor_57, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,666 mps_preprocess.py:115] Visiting: aten_mul_tensor_59, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,666 mps_preprocess.py:115] Visiting: aten_sub_tensor_4, aten.sub.Tensor\n",
      "[INFO 2024-05-31 16:58:17,666 mps_preprocess.py:115] Visiting: aten_add_tensor_13, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:17,666 mps_preprocess.py:115] Visiting: aten_sub_tensor_5, aten.sub.Tensor\n",
      "[INFO 2024-05-31 16:58:17,667 mps_preprocess.py:115] Visiting: aten_add_tensor_14, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:17,667 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_8, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 16:58:17,667 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_9, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 16:58:17,667 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_10, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 16:58:17,668 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_11, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 16:58:17,668 mps_preprocess.py:115] Visiting: aten_cat_default_4, aten.cat.default\n",
      "[INFO 2024-05-31 16:58:17,668 mps_preprocess.py:115] Visiting: aten_cat_default_5, aten.cat.default\n",
      "[INFO 2024-05-31 16:58:17,669 mps_preprocess.py:115] Visiting: aten_view_copy_default_27, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:17,669 mps_preprocess.py:115] Visiting: aten_view_copy_default_28, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:17,933 mps_preprocess.py:115] Visiting: aten_view_copy_default_59, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:17,934 mps_preprocess.py:115] Visiting: aten__to_copy_default_38, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:17,934 mps_preprocess.py:115] Visiting: aten__to_copy_default_39, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:17,934 mps_preprocess.py:115] Visiting: aten__to_copy_default_40, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:17,934 mps_preprocess.py:115] Visiting: aten__to_copy_default_41, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:17,934 mps_preprocess.py:115] Visiting: aten__to_copy_default_42, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:17,935 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_57, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:17,935 mps_preprocess.py:115] Visiting: aten_permute_copy_default_38, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:17,935 mps_preprocess.py:115] Visiting: aten_permute_copy_default_39, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:17,935 mps_preprocess.py:115] Visiting: aten_permute_copy_default_40, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:17,936 mps_preprocess.py:115] Visiting: aten_permute_copy_default_41, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:17,936 mps_preprocess.py:115] Visiting: aten_permute_copy_default_42, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:17,936 mps_preprocess.py:115] Visiting: aten_mm_default_38, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:17,937 mps_preprocess.py:115] Visiting: aten_mul_tensor_129, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,937 mps_preprocess.py:115] Visiting: aten_add_tensor_33, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:17,937 mps_preprocess.py:115] Visiting: aten_mul_tensor_130, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,937 mps_preprocess.py:115] Visiting: aten_mean_dim_11, aten.mean.dim\n",
      "[INFO 2024-05-31 16:58:17,938 mps_preprocess.py:115] Visiting: aten_add_tensor_34, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:17,938 mps_preprocess.py:115] Visiting: aten_rsqrt_default_11, aten.rsqrt.default\n",
      "[INFO 2024-05-31 16:58:17,938 mps_preprocess.py:115] Visiting: aten_mul_tensor_131, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,938 mps_preprocess.py:115] Visiting: aten_mul_tensor_132, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,938 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_58, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:17,939 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_59, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:17,939 mps_preprocess.py:115] Visiting: aten_mm_default_39, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:17,940 mps_preprocess.py:115] Visiting: aten_mm_default_40, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:17,940 mps_preprocess.py:115] Visiting: aten_mul_tensor_133, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,940 mps_preprocess.py:115] Visiting: aten_mul_tensor_135, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,941 mps_preprocess.py:115] Visiting: aten_sigmoid_default_5, aten.sigmoid.default\n",
      "[INFO 2024-05-31 16:58:17,941 mps_preprocess.py:115] Visiting: aten_mul_tensor_134, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,941 mps_preprocess.py:115] Visiting: aten_mul_tensor_136, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,941 mps_preprocess.py:115] Visiting: aten_mm_default_41, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:17,942 mps_preprocess.py:115] Visiting: aten_mul_tensor_137, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,942 mps_preprocess.py:115] Visiting: aten_add_tensor_35, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:17,942 mps_preprocess.py:115] Visiting: aten_mul_tensor_138, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,942 mps_preprocess.py:115] Visiting: aten_mean_dim_12, aten.mean.dim\n",
      "[INFO 2024-05-31 16:58:17,943 mps_preprocess.py:115] Visiting: aten_add_tensor_36, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:17,943 mps_preprocess.py:115] Visiting: aten_rsqrt_default_12, aten.rsqrt.default\n",
      "[INFO 2024-05-31 16:58:17,944 mps_preprocess.py:115] Visiting: aten_mul_tensor_139, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,944 mps_preprocess.py:115] Visiting: aten_mul_tensor_140, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:17,944 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_60, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:17,944 mps_preprocess.py:115] Visiting: aten_mm_default_42, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:17,945 mps_preprocess.py:115] Visiting: aten_mul_tensor_141, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,633 mps_preprocess.py:115] Visiting: aten_view_copy_default_15, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:19,634 mps_preprocess.py:115] Visiting: aten_view_copy_default_16, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:19,634 mps_preprocess.py:115] Visiting: aten_view_copy_default_9, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:19,635 mps_preprocess.py:115] Visiting: aten__to_copy_default_3, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:19,635 mps_preprocess.py:115] Visiting: aten__to_copy_default_4, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:19,635 mps_preprocess.py:115] Visiting: aten__to_copy_default_5, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:19,635 mps_preprocess.py:115] Visiting: aten__to_copy_default_6, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:19,636 mps_preprocess.py:115] Visiting: aten__to_copy_default_7, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:19,636 mps_preprocess.py:115] Visiting: aten__to_copy_default_8, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:19,636 mps_preprocess.py:115] Visiting: aten__to_copy_default_9, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:19,636 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_7, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:19,637 mps_preprocess.py:115] Visiting: aten_permute_copy_default_3, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:19,637 mps_preprocess.py:115] Visiting: aten_permute_copy_default_4, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:19,637 mps_preprocess.py:115] Visiting: aten_permute_copy_default_5, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:19,637 mps_preprocess.py:115] Visiting: aten_permute_copy_default_6, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:19,638 mps_preprocess.py:115] Visiting: aten_permute_copy_default_7, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:19,638 mps_preprocess.py:115] Visiting: aten_permute_copy_default_8, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:19,638 mps_preprocess.py:115] Visiting: aten_permute_copy_default_9, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:19,638 mps_preprocess.py:115] Visiting: aten_mm_default_3, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:19,638 mps_preprocess.py:115] Visiting: aten_mul_tensor_14, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,639 mps_preprocess.py:115] Visiting: aten_add_tensor_3, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:19,639 mps_preprocess.py:115] Visiting: aten_mul_tensor_15, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,639 mps_preprocess.py:115] Visiting: aten_mean_dim_1, aten.mean.dim\n",
      "[INFO 2024-05-31 16:58:19,639 mps_preprocess.py:115] Visiting: aten_add_tensor_4, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:19,640 mps_preprocess.py:115] Visiting: aten_rsqrt_default_1, aten.rsqrt.default\n",
      "[INFO 2024-05-31 16:58:19,640 mps_preprocess.py:115] Visiting: aten_mul_tensor_16, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,640 mps_preprocess.py:115] Visiting: aten_mul_tensor_17, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,640 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_8, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:19,641 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_9, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:19,641 mps_preprocess.py:115] Visiting: aten_mm_default_4, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:19,641 mps_preprocess.py:115] Visiting: aten_mm_default_5, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:19,641 mps_preprocess.py:115] Visiting: aten_mul_tensor_18, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,642 mps_preprocess.py:115] Visiting: aten_mul_tensor_20, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,642 mps_preprocess.py:115] Visiting: aten_sigmoid_default, aten.sigmoid.default\n",
      "[INFO 2024-05-31 16:58:19,642 mps_preprocess.py:115] Visiting: aten_mul_tensor_19, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,642 mps_preprocess.py:115] Visiting: aten_mul_tensor_21, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,643 mps_preprocess.py:115] Visiting: aten_mm_default_6, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:19,643 mps_preprocess.py:115] Visiting: aten_mul_tensor_22, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,643 mps_preprocess.py:115] Visiting: aten_add_tensor_5, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:19,644 mps_preprocess.py:115] Visiting: aten_mul_tensor_23, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,644 mps_preprocess.py:115] Visiting: aten_mean_dim_2, aten.mean.dim\n",
      "[INFO 2024-05-31 16:58:19,644 mps_preprocess.py:115] Visiting: aten_add_tensor_6, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:19,645 mps_preprocess.py:115] Visiting: aten_rsqrt_default_2, aten.rsqrt.default\n",
      "[INFO 2024-05-31 16:58:19,645 mps_preprocess.py:115] Visiting: aten_mul_tensor_24, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,645 mps_preprocess.py:115] Visiting: aten_mul_tensor_25, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,645 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_10, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:19,646 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_11, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:19,646 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_12, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:19,646 mps_preprocess.py:115] Visiting: aten_mm_default_7, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:19,647 mps_preprocess.py:115] Visiting: aten_mm_default_8, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:19,647 mps_preprocess.py:115] Visiting: aten_mm_default_9, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:19,647 mps_preprocess.py:115] Visiting: aten_mul_tensor_26, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,647 mps_preprocess.py:115] Visiting: aten_mul_tensor_27, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,648 mps_preprocess.py:115] Visiting: aten_mul_tensor_28, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,648 mps_preprocess.py:115] Visiting: aten_view_copy_default_10, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:19,648 mps_preprocess.py:115] Visiting: aten_view_copy_default_11, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:19,649 mps_preprocess.py:115] Visiting: aten_view_copy_default_12, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:19,649 mps_preprocess.py:115] Visiting: aten_view_copy_default_13, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:19,649 mps_preprocess.py:115] Visiting: aten_view_copy_default_14, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:19,649 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_4, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 16:58:19,650 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_5, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 16:58:19,650 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_6, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 16:58:19,650 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_7, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 16:58:19,650 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_13, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:19,651 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_14, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:19,651 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_15, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:19,651 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_16, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:19,651 mps_preprocess.py:115] Visiting: aten_mul_tensor_29, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,652 mps_preprocess.py:115] Visiting: aten_mul_tensor_31, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,652 mps_preprocess.py:115] Visiting: aten_mul_tensor_30, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,652 mps_preprocess.py:115] Visiting: aten_mul_tensor_32, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,653 mps_preprocess.py:115] Visiting: aten_mul_tensor_33, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,654 mps_preprocess.py:115] Visiting: aten_mul_tensor_35, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,655 mps_preprocess.py:115] Visiting: aten_mul_tensor_34, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,656 mps_preprocess.py:115] Visiting: aten_mul_tensor_36, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,657 mps_preprocess.py:115] Visiting: aten_sub_tensor_2, aten.sub.Tensor\n",
      "[INFO 2024-05-31 16:58:19,658 mps_preprocess.py:115] Visiting: aten_add_tensor_7, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:19,660 mps_preprocess.py:115] Visiting: aten_sub_tensor_3, aten.sub.Tensor\n",
      "[INFO 2024-05-31 16:58:19,661 mps_preprocess.py:115] Visiting: aten_add_tensor_8, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:19,662 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_4, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 16:58:19,662 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_5, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 16:58:19,664 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_6, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 16:58:19,664 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_7, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 16:58:19,664 mps_preprocess.py:115] Visiting: aten_cat_default_2, aten.cat.default\n",
      "[INFO 2024-05-31 16:58:19,665 mps_preprocess.py:115] Visiting: aten_cat_default_3, aten.cat.default\n",
      "[INFO 2024-05-31 16:58:19,665 mps_preprocess.py:115] Visiting: aten_view_copy_default_17, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:19,665 mps_preprocess.py:115] Visiting: aten_view_copy_default_18, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:19,913 mps_preprocess.py:115] Visiting: aten_embedding_default, aten.embedding.default\n",
      "[INFO 2024-05-31 16:58:19,914 mps_preprocess.py:115] Visiting: aten_index_tensor, aten.index.Tensor\n",
      "[INFO 2024-05-31 16:58:19,914 mps_preprocess.py:115] Visiting: aten_index_tensor_1, aten.index.Tensor\n",
      "[INFO 2024-05-31 16:58:19,915 mps_preprocess.py:115] Visiting: aten__to_copy_default, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:19,915 mps_preprocess.py:115] Visiting: aten__to_copy_default_1, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:19,915 mps_preprocess.py:115] Visiting: aten__to_copy_default_2, aten._to_copy.default\n",
      "[INFO 2024-05-31 16:58:19,915 mps_preprocess.py:115] Visiting: aten_mul_tensor, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,916 mps_preprocess.py:115] Visiting: aten_view_copy_default_5, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:19,916 mps_preprocess.py:115] Visiting: aten_view_copy_default_6, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:19,916 mps_preprocess.py:115] Visiting: aten_permute_copy_default, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:19,916 mps_preprocess.py:115] Visiting: aten_permute_copy_default_1, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:19,917 mps_preprocess.py:115] Visiting: aten_permute_copy_default_2, aten.permute_copy.default\n",
      "[INFO 2024-05-31 16:58:19,917 mps_preprocess.py:115] Visiting: aten_mean_dim, aten.mean.dim\n",
      "[INFO 2024-05-31 16:58:19,917 mps_preprocess.py:115] Visiting: aten_add_tensor, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:19,918 mps_preprocess.py:115] Visiting: aten_rsqrt_default, aten.rsqrt.default\n",
      "[INFO 2024-05-31 16:58:19,918 mps_preprocess.py:115] Visiting: aten_mul_tensor_1, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,918 mps_preprocess.py:115] Visiting: aten_mul_tensor_2, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,918 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:19,919 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_1, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:19,919 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_2, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:19,919 mps_preprocess.py:115] Visiting: aten_mm_default, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:19,919 mps_preprocess.py:115] Visiting: aten_mm_default_1, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:19,920 mps_preprocess.py:115] Visiting: aten_mm_default_2, aten.mm.default\n",
      "[INFO 2024-05-31 16:58:19,920 mps_preprocess.py:115] Visiting: aten_mul_tensor_3, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,920 mps_preprocess.py:115] Visiting: aten_mul_tensor_4, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,920 mps_preprocess.py:115] Visiting: aten_mul_tensor_5, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,920 mps_preprocess.py:115] Visiting: aten_view_copy_default, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:19,921 mps_preprocess.py:115] Visiting: aten_view_copy_default_1, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:19,921 mps_preprocess.py:115] Visiting: aten_view_copy_default_2, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:19,921 mps_preprocess.py:115] Visiting: aten_view_copy_default_3, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:19,921 mps_preprocess.py:115] Visiting: aten_view_copy_default_4, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:19,922 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 16:58:19,922 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_1, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 16:58:19,923 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_2, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 16:58:19,923 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_3, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 16:58:19,923 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_3, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:19,924 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_4, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:19,924 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_5, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:19,924 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_6, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 16:58:19,924 mps_preprocess.py:115] Visiting: aten_mul_tensor_6, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,925 mps_preprocess.py:115] Visiting: aten_mul_tensor_8, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,925 mps_preprocess.py:115] Visiting: aten_mul_tensor_7, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,925 mps_preprocess.py:115] Visiting: aten_mul_tensor_9, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,925 mps_preprocess.py:115] Visiting: aten_mul_tensor_10, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,926 mps_preprocess.py:115] Visiting: aten_mul_tensor_12, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,926 mps_preprocess.py:115] Visiting: aten_mul_tensor_11, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,927 mps_preprocess.py:115] Visiting: aten_mul_tensor_13, aten.mul.Tensor\n",
      "[INFO 2024-05-31 16:58:19,927 mps_preprocess.py:115] Visiting: aten_sub_tensor, aten.sub.Tensor\n",
      "[INFO 2024-05-31 16:58:19,927 mps_preprocess.py:115] Visiting: aten_add_tensor_1, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:19,928 mps_preprocess.py:115] Visiting: aten_sub_tensor_1, aten.sub.Tensor\n",
      "[INFO 2024-05-31 16:58:19,928 mps_preprocess.py:115] Visiting: aten_add_tensor_2, aten.add.Tensor\n",
      "[INFO 2024-05-31 16:58:19,929 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 16:58:19,930 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_1, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 16:58:19,933 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_2, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 16:58:19,934 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_3, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 16:58:19,936 mps_preprocess.py:115] Visiting: aten_cat_default, aten.cat.default\n",
      "[INFO 2024-05-31 16:58:19,942 mps_preprocess.py:115] Visiting: aten_cat_default_1, aten.cat.default\n",
      "[INFO 2024-05-31 16:58:19,946 mps_preprocess.py:115] Visiting: aten_view_copy_default_7, aten.view_copy.default\n",
      "[INFO 2024-05-31 16:58:19,948 mps_preprocess.py:115] Visiting: aten_view_copy_default_8, aten.view_copy.default\n",
      "/Users/larryliu/miniconda3/envs/executorch/lib/python3.11/site-packages/executorch/exir/emit/_emitter.py:1474: UserWarning: Mutation on a buffer in the model is detected. ExecuTorch assumes buffers that are mutated in the graph have a meaningless initial state, only the shape and dtype will be serialized.\n",
      "  warnings.warn(\n",
      "[INFO 2024-05-31 16:58:23,487 builder.py:375] Required memory for activation in bytes: [0, 1904480]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<executorch.examples.models.llama2.builder.LlamaEdgeManager at 0x331a45fd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder = model.export_to_edge()\n",
    "builder.edge_manager = builder.edge_manager.transform([ReplaceMMPass()])\n",
    "builder.to_backend(partitioners).to_executorch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING 2024-05-31 12:55:46,709 xnnpack_partitioner.py:558] Nothing can be partitioned!\n",
      "/Users/larryliu/miniconda3/envs/executorch/lib/python3.11/site-packages/executorch/exir/emit/_emitter.py:1474: UserWarning: Mutation on a buffer in the model is detected. ExecuTorch assumes buffers that are mutated in the graph have a meaningless initial state, only the shape and dtype will be serialized.\n",
      "  warnings.warn(\n",
      "[INFO 2024-05-31 12:55:47,565 builder.py:375] Required memory for activation in bytes: [0, 1923776]\n",
      "[INFO 2024-05-31 12:55:47,580 utils.py:114] Saved exported program to stories15M_int8_xnnpack_llama_cpp.pte\n"
     ]
    }
   ],
   "source": [
    "from executorch.examples.models.llama2.lib.partitioner_lib import get_xnnpack_partitioner\n",
    "builder = model.export_to_edge()\n",
    "builder.edge_manager = builder.edge_manager.transform([ReplaceMMPass()])\n",
    "builder.to_backend([get_xnnpack_partitioner()]).to_executorch().save_to_pte(\"stories15M_int8_xnnpack_llama_cpp.pte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/larryliu/miniconda3/envs/executorch/lib/python3.11/site-packages/executorch/exir/emit/_emitter.py:1474: UserWarning: Mutation on a buffer in the model is detected. ExecuTorch assumes buffers that are mutated in the graph have a meaningless initial state, only the shape and dtype will be serialized.\n",
      "  warnings.warn(\n",
      "[INFO 2024-05-08 18:05:25,534 builder.py:341] Required memory for activation in bytes: [0, 418116608]\n"
     ]
    }
   ],
   "source": [
    "builder = model.export_to_edge(None).to_executorch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.edge_manager._edge_programs['forward'].graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph():\n",
      "  %b_layers_0_attention_sdpa_kv_cache_k_cache : [num_users=1] = placeholder[target=b_layers_0_attention_sdpa_kv_cache_k_cache]\n",
      "  %b_layers_0_attention_sdpa_kv_cache_v_cache : [num_users=1] = placeholder[target=b_layers_0_attention_sdpa_kv_cache_v_cache]\n",
      "  %b_layers_1_attention_sdpa_kv_cache_k_cache : [num_users=1] = placeholder[target=b_layers_1_attention_sdpa_kv_cache_k_cache]\n",
      "  %b_layers_1_attention_sdpa_kv_cache_v_cache : [num_users=1] = placeholder[target=b_layers_1_attention_sdpa_kv_cache_v_cache]\n",
      "  %b_layers_2_attention_sdpa_kv_cache_k_cache : [num_users=1] = placeholder[target=b_layers_2_attention_sdpa_kv_cache_k_cache]\n",
      "  %b_layers_2_attention_sdpa_kv_cache_v_cache : [num_users=1] = placeholder[target=b_layers_2_attention_sdpa_kv_cache_v_cache]\n",
      "  %b_layers_3_attention_sdpa_kv_cache_k_cache : [num_users=1] = placeholder[target=b_layers_3_attention_sdpa_kv_cache_k_cache]\n",
      "  %b_layers_3_attention_sdpa_kv_cache_v_cache : [num_users=1] = placeholder[target=b_layers_3_attention_sdpa_kv_cache_v_cache]\n",
      "  %b_layers_4_attention_sdpa_kv_cache_k_cache : [num_users=1] = placeholder[target=b_layers_4_attention_sdpa_kv_cache_k_cache]\n",
      "  %b_layers_4_attention_sdpa_kv_cache_v_cache : [num_users=1] = placeholder[target=b_layers_4_attention_sdpa_kv_cache_v_cache]\n",
      "  %b_layers_5_attention_sdpa_kv_cache_k_cache : [num_users=1] = placeholder[target=b_layers_5_attention_sdpa_kv_cache_k_cache]\n",
      "  %b_layers_5_attention_sdpa_kv_cache_v_cache : [num_users=1] = placeholder[target=b_layers_5_attention_sdpa_kv_cache_v_cache]\n",
      "  %tokens : [num_users=1] = placeholder[target=tokens]\n",
      "  %input_pos : [num_users=7] = placeholder[target=input_pos]\n",
      "  %_local_scalar_dense : [num_users=1] = call_function[target=torch.ops.aten._local_scalar_dense.default](args = (%input_pos,), kwargs = {})\n",
      "  %_local_scalar_dense_1 : [num_users=1] = call_function[target=torch.ops.aten._local_scalar_dense.default](args = (%input_pos,), kwargs = {})\n",
      "  %_local_scalar_dense_2 : [num_users=1] = call_function[target=torch.ops.aten._local_scalar_dense.default](args = (%input_pos,), kwargs = {})\n",
      "  %_local_scalar_dense_3 : [num_users=1] = call_function[target=torch.ops.aten._local_scalar_dense.default](args = (%input_pos,), kwargs = {})\n",
      "  %_local_scalar_dense_4 : [num_users=1] = call_function[target=torch.ops.aten._local_scalar_dense.default](args = (%input_pos,), kwargs = {})\n",
      "  %_local_scalar_dense_5 : [num_users=1] = call_function[target=torch.ops.aten._local_scalar_dense.default](args = (%input_pos,), kwargs = {})\n",
      "  %lowered_module_0 : [num_users=1] = get_attr[target=lowered_module_0]\n",
      "    backend_id: MPSBackend\n",
      "    lowered graph():\n",
      "      %p_tok_embeddings_weight : [num_users=1] = placeholder[target=p_tok_embeddings_weight]\n",
      "      %p_layers_0_attention_norm_weight : [num_users=1] = placeholder[target=p_layers_0_attention_norm_weight]\n",
      "      %b_freqs_cos : [num_users=1] = placeholder[target=b_freqs_cos]\n",
      "      %b_freqs_sin : [num_users=1] = placeholder[target=b_freqs_sin]\n",
      "      %b_layers_0_attention_wq_weight : [num_users=1] = placeholder[target=b_layers_0_attention_wq_weight]\n",
      "      %b_layers_0_attention_wq_scales : [num_users=1] = placeholder[target=b_layers_0_attention_wq_scales]\n",
      "      %b_layers_0_attention_wk_weight : [num_users=1] = placeholder[target=b_layers_0_attention_wk_weight]\n",
      "      %b_layers_0_attention_wk_scales : [num_users=1] = placeholder[target=b_layers_0_attention_wk_scales]\n",
      "      %b_layers_0_attention_wv_weight : [num_users=1] = placeholder[target=b_layers_0_attention_wv_weight]\n",
      "      %b_layers_0_attention_wv_scales : [num_users=1] = placeholder[target=b_layers_0_attention_wv_scales]\n",
      "      %_lifted_tensor_constant100 : [num_users=1] = placeholder[target=_lifted_tensor_constant100]\n",
      "      %tokens : [num_users=1] = placeholder[target=tokens]\n",
      "      %input_pos : [num_users=2] = placeholder[target=input_pos]\n",
      "      %aten_embedding_default : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.embedding.default](args = (%p_tok_embeddings_weight, %tokens), kwargs = {})\n",
      "      %aten_index_tensor : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.index.Tensor](args = (%b_freqs_cos, [%input_pos]), kwargs = {})\n",
      "      %aten_index_tensor_1 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.index.Tensor](args = (%b_freqs_sin, [%input_pos]), kwargs = {})\n",
      "      %aten__to_copy_default : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_0_attention_wq_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_1 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_0_attention_wk_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_2 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_0_attention_wv_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten_mul_tensor : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_embedding_default, %aten_embedding_default), kwargs = {})\n",
      "      %aten_view_copy_default_5 : [num_users=4] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_index_tensor, [1, 1, 1, 24]), kwargs = {})\n",
      "      %aten_view_copy_default_6 : [num_users=4] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_index_tensor_1, [1, 1, 1, 24]), kwargs = {})\n",
      "      %aten_permute_copy_default : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_1 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_1, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_2 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_2, [1, 0]), kwargs = {})\n",
      "      %aten_mean_dim : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mean.dim](args = (%aten_mul_tensor, [-1], True), kwargs = {})\n",
      "      %aten_add_tensor : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mean_dim, %_lifted_tensor_constant100), kwargs = {})\n",
      "      %aten_rsqrt_default : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.rsqrt.default](args = (%aten_add_tensor,), kwargs = {})\n",
      "      %aten_mul_tensor_1 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_embedding_default, %aten_rsqrt_default), kwargs = {})\n",
      "      %aten_mul_tensor_2 : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_1, %p_layers_0_attention_norm_weight), kwargs = {})\n",
      "      %aten_squeeze_copy_dims : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_2, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_1 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_2, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_2 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_2, [0]), kwargs = {})\n",
      "      %aten_mm_default : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims, %aten_permute_copy_default), kwargs = {})\n",
      "      %aten_mm_default_1 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_1, %aten_permute_copy_default_1), kwargs = {})\n",
      "      %aten_mm_default_2 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_2, %aten_permute_copy_default_2), kwargs = {})\n",
      "      %aten_mul_tensor_3 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default, %b_layers_0_attention_wq_scales), kwargs = {})\n",
      "      %aten_mul_tensor_4 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_1, %b_layers_0_attention_wk_scales), kwargs = {})\n",
      "      %aten_mul_tensor_5 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_2, %b_layers_0_attention_wv_scales), kwargs = {})\n",
      "      %aten_view_copy_default : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_mul_tensor_3, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_1 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_mul_tensor_4, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_2 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_mul_tensor_5, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_3 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_view_copy_default, [1, 1, 6, -1, 2]), kwargs = {})\n",
      "      %aten_view_copy_default_4 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_view_copy_default_1, [1, 1, 6, -1, 2]), kwargs = {})\n",
      "      %aten_slice_copy_tensor : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_3, 4, 0, 1), kwargs = {})\n",
      "      %aten_slice_copy_tensor_1 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_3, 4, 1, 2), kwargs = {})\n",
      "      %aten_slice_copy_tensor_2 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_4, 4, 0, 1), kwargs = {})\n",
      "      %aten_slice_copy_tensor_3 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_4, 4, 1, 2), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_3 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_4 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_1, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_5 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_2, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_6 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_3, [4]), kwargs = {})\n",
      "      %aten_mul_tensor_6 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_3, %aten_view_copy_default_5), kwargs = {})\n",
      "      %aten_mul_tensor_8 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_3, %aten_view_copy_default_6), kwargs = {})\n",
      "      %aten_mul_tensor_7 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_4, %aten_view_copy_default_6), kwargs = {})\n",
      "      %aten_mul_tensor_9 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_4, %aten_view_copy_default_5), kwargs = {})\n",
      "      %aten_mul_tensor_10 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_5, %aten_view_copy_default_5), kwargs = {})\n",
      "      %aten_mul_tensor_12 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_5, %aten_view_copy_default_6), kwargs = {})\n",
      "      %aten_mul_tensor_11 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_6, %aten_view_copy_default_6), kwargs = {})\n",
      "      %aten_mul_tensor_13 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_6, %aten_view_copy_default_5), kwargs = {})\n",
      "      %aten_sub_tensor : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sub.Tensor](args = (%aten_mul_tensor_6, %aten_mul_tensor_7), kwargs = {})\n",
      "      %aten_add_tensor_1 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mul_tensor_8, %aten_mul_tensor_9), kwargs = {})\n",
      "      %aten_sub_tensor_1 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sub.Tensor](args = (%aten_mul_tensor_10, %aten_mul_tensor_11), kwargs = {})\n",
      "      %aten_add_tensor_2 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mul_tensor_12, %aten_mul_tensor_13), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_sub_tensor, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_1 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_add_tensor_1, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_2 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_sub_tensor_1, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_3 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_add_tensor_2, 4), kwargs = {})\n",
      "      %aten_cat_default : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.cat.default](args = ([%aten_unsqueeze_copy_default, %aten_unsqueeze_copy_default_1], -1), kwargs = {})\n",
      "      %aten_cat_default_1 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.cat.default](args = ([%aten_unsqueeze_copy_default_2, %aten_unsqueeze_copy_default_3], -1), kwargs = {})\n",
      "      %aten_view_copy_default_7 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_cat_default, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_8 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_cat_default_1, [1, 1, 6, 48]), kwargs = {})\n",
      "      return (aten_embedding_default, aten_index_tensor, aten_index_tensor_1, aten_view_copy_default_2, aten_view_copy_default_7, aten_view_copy_default_8)\n",
      "  %executorch_call_delegate : [num_users=6] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_0, %tokens, %input_pos), kwargs = {})\n",
      "  %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate, 0), kwargs = {})\n",
      "  %getitem_1 : [num_users=5] = call_function[target=operator.getitem](args = (%executorch_call_delegate, 1), kwargs = {})\n",
      "  %getitem_2 : [num_users=5] = call_function[target=operator.getitem](args = (%executorch_call_delegate, 2), kwargs = {})\n",
      "  %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate, 3), kwargs = {})\n",
      "  %getitem_4 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate, 4), kwargs = {})\n",
      "  %getitem_5 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate, 5), kwargs = {})\n",
      "  %alloc : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 1, 6, 48), torch.float32),), kwargs = {})\n",
      "  %sdpa_with_kv_cache : [num_users=1] = call_function[target=torch.ops.llama.sdpa_with_kv_cache.out](args = (), kwargs = {query: %getitem_4, key: %getitem_5, value: %getitem_3, key_cache: %b_layers_0_attention_sdpa_kv_cache_k_cache, value_cache: %b_layers_0_attention_sdpa_kv_cache_v_cache, start_pos: %_local_scalar_dense, seq_len: 1, attn_mask: None, drpout_p: 0.0, is_causal: False, scale: None, out: %alloc})\n",
      "  %lowered_module_1 : [num_users=1] = get_attr[target=lowered_module_1]\n",
      "    backend_id: MPSBackend\n",
      "    lowered graph():\n",
      "      %p_layers_0_ffn_norm_weight : [num_users=1] = placeholder[target=p_layers_0_ffn_norm_weight]\n",
      "      %p_layers_1_attention_norm_weight : [num_users=1] = placeholder[target=p_layers_1_attention_norm_weight]\n",
      "      %b_layers_0_attention_wo_weight : [num_users=1] = placeholder[target=b_layers_0_attention_wo_weight]\n",
      "      %b_layers_0_attention_wo_scales : [num_users=1] = placeholder[target=b_layers_0_attention_wo_scales]\n",
      "      %b_layers_0_feed_forward_w1_weight : [num_users=1] = placeholder[target=b_layers_0_feed_forward_w1_weight]\n",
      "      %b_layers_0_feed_forward_w1_scales : [num_users=1] = placeholder[target=b_layers_0_feed_forward_w1_scales]\n",
      "      %b_layers_0_feed_forward_w3_weight : [num_users=1] = placeholder[target=b_layers_0_feed_forward_w3_weight]\n",
      "      %b_layers_0_feed_forward_w3_scales : [num_users=1] = placeholder[target=b_layers_0_feed_forward_w3_scales]\n",
      "      %b_layers_0_feed_forward_w2_weight : [num_users=1] = placeholder[target=b_layers_0_feed_forward_w2_weight]\n",
      "      %b_layers_0_feed_forward_w2_scales : [num_users=1] = placeholder[target=b_layers_0_feed_forward_w2_scales]\n",
      "      %b_layers_1_attention_wq_weight : [num_users=1] = placeholder[target=b_layers_1_attention_wq_weight]\n",
      "      %b_layers_1_attention_wq_scales : [num_users=1] = placeholder[target=b_layers_1_attention_wq_scales]\n",
      "      %b_layers_1_attention_wk_weight : [num_users=1] = placeholder[target=b_layers_1_attention_wk_weight]\n",
      "      %b_layers_1_attention_wk_scales : [num_users=1] = placeholder[target=b_layers_1_attention_wk_scales]\n",
      "      %b_layers_1_attention_wv_weight : [num_users=1] = placeholder[target=b_layers_1_attention_wv_weight]\n",
      "      %b_layers_1_attention_wv_scales : [num_users=1] = placeholder[target=b_layers_1_attention_wv_scales]\n",
      "      %_lifted_tensor_constant101 : [num_users=1] = placeholder[target=_lifted_tensor_constant101]\n",
      "      %_lifted_tensor_constant102 : [num_users=1] = placeholder[target=_lifted_tensor_constant102]\n",
      "      %aten_index_tensor : [num_users=1] = placeholder[target=aten_index_tensor]\n",
      "      %aten_index_tensor_1 : [num_users=1] = placeholder[target=aten_index_tensor_1]\n",
      "      %getitem : [num_users=1] = placeholder[target=getitem]\n",
      "      %aten_embedding_default : [num_users=1] = placeholder[target=aten_embedding_default]\n",
      "      %aten_view_copy_default_15 : [num_users=4] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_index_tensor, [1, 1, 1, 24]), kwargs = {})\n",
      "      %aten_view_copy_default_16 : [num_users=4] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_index_tensor_1, [1, 1, 1, 24]), kwargs = {})\n",
      "      %aten_view_copy_default_9 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%getitem, [1, 1, 288]), kwargs = {})\n",
      "      %aten__to_copy_default_3 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_0_attention_wo_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_4 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_0_feed_forward_w1_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_5 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_0_feed_forward_w3_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_6 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_0_feed_forward_w2_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_7 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_1_attention_wq_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_8 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_1_attention_wk_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_9 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_1_attention_wv_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten_squeeze_copy_dims_7 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_view_copy_default_9, [0]), kwargs = {})\n",
      "      %aten_permute_copy_default_3 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_3, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_4 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_4, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_5 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_5, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_6 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_6, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_7 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_7, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_8 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_8, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_9 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_9, [1, 0]), kwargs = {})\n",
      "      %aten_mm_default_3 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_7, %aten_permute_copy_default_3), kwargs = {})\n",
      "      %aten_mul_tensor_14 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_3, %b_layers_0_attention_wo_scales), kwargs = {})\n",
      "      %aten_add_tensor_3 : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_embedding_default, %aten_mul_tensor_14), kwargs = {})\n",
      "      %aten_mul_tensor_15 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_3, %aten_add_tensor_3), kwargs = {})\n",
      "      %aten_mean_dim_1 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mean.dim](args = (%aten_mul_tensor_15, [-1], True), kwargs = {})\n",
      "      %aten_add_tensor_4 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mean_dim_1, %_lifted_tensor_constant101), kwargs = {})\n",
      "      %aten_rsqrt_default_1 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.rsqrt.default](args = (%aten_add_tensor_4,), kwargs = {})\n",
      "      %aten_mul_tensor_16 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_3, %aten_rsqrt_default_1), kwargs = {})\n",
      "      %aten_mul_tensor_17 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_16, %p_layers_0_ffn_norm_weight), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_8 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_17, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_9 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_17, [0]), kwargs = {})\n",
      "      %aten_mm_default_4 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_8, %aten_permute_copy_default_4), kwargs = {})\n",
      "      %aten_mm_default_5 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_9, %aten_permute_copy_default_5), kwargs = {})\n",
      "      %aten_mul_tensor_18 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_4, %b_layers_0_feed_forward_w1_scales), kwargs = {})\n",
      "      %aten_mul_tensor_20 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_5, %b_layers_0_feed_forward_w3_scales), kwargs = {})\n",
      "      %aten_sigmoid_default : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sigmoid.default](args = (%aten_mul_tensor_18,), kwargs = {})\n",
      "      %aten_mul_tensor_19 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_18, %aten_sigmoid_default), kwargs = {})\n",
      "      %aten_mul_tensor_21 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_19, %aten_mul_tensor_20), kwargs = {})\n",
      "      %aten_mm_default_6 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_mul_tensor_21, %aten_permute_copy_default_6), kwargs = {})\n",
      "      %aten_mul_tensor_22 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_6, %b_layers_0_feed_forward_w2_scales), kwargs = {})\n",
      "      %aten_add_tensor_5 : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_add_tensor_3, %aten_mul_tensor_22), kwargs = {})\n",
      "      %aten_mul_tensor_23 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_5, %aten_add_tensor_5), kwargs = {})\n",
      "      %aten_mean_dim_2 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mean.dim](args = (%aten_mul_tensor_23, [-1], True), kwargs = {})\n",
      "      %aten_add_tensor_6 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mean_dim_2, %_lifted_tensor_constant102), kwargs = {})\n",
      "      %aten_rsqrt_default_2 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.rsqrt.default](args = (%aten_add_tensor_6,), kwargs = {})\n",
      "      %aten_mul_tensor_24 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_5, %aten_rsqrt_default_2), kwargs = {})\n",
      "      %aten_mul_tensor_25 : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_24, %p_layers_1_attention_norm_weight), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_10 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_25, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_11 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_25, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_12 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_25, [0]), kwargs = {})\n",
      "      %aten_mm_default_7 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_10, %aten_permute_copy_default_7), kwargs = {})\n",
      "      %aten_mm_default_8 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_11, %aten_permute_copy_default_8), kwargs = {})\n",
      "      %aten_mm_default_9 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_12, %aten_permute_copy_default_9), kwargs = {})\n",
      "      %aten_mul_tensor_26 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_7, %b_layers_1_attention_wq_scales), kwargs = {})\n",
      "      %aten_mul_tensor_27 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_8, %b_layers_1_attention_wk_scales), kwargs = {})\n",
      "      %aten_mul_tensor_28 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_9, %b_layers_1_attention_wv_scales), kwargs = {})\n",
      "      %aten_view_copy_default_10 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_mul_tensor_26, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_11 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_mul_tensor_27, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_12 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_mul_tensor_28, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_13 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_view_copy_default_10, [1, 1, 6, -1, 2]), kwargs = {})\n",
      "      %aten_view_copy_default_14 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_view_copy_default_11, [1, 1, 6, -1, 2]), kwargs = {})\n",
      "      %aten_slice_copy_tensor_4 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_13, 4, 0, 1), kwargs = {})\n",
      "      %aten_slice_copy_tensor_5 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_13, 4, 1, 2), kwargs = {})\n",
      "      %aten_slice_copy_tensor_6 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_14, 4, 0, 1), kwargs = {})\n",
      "      %aten_slice_copy_tensor_7 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_14, 4, 1, 2), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_13 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_4, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_14 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_5, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_15 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_6, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_16 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_7, [4]), kwargs = {})\n",
      "      %aten_mul_tensor_29 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_13, %aten_view_copy_default_15), kwargs = {})\n",
      "      %aten_mul_tensor_31 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_13, %aten_view_copy_default_16), kwargs = {})\n",
      "      %aten_mul_tensor_30 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_14, %aten_view_copy_default_16), kwargs = {})\n",
      "      %aten_mul_tensor_32 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_14, %aten_view_copy_default_15), kwargs = {})\n",
      "      %aten_mul_tensor_33 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_15, %aten_view_copy_default_15), kwargs = {})\n",
      "      %aten_mul_tensor_35 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_15, %aten_view_copy_default_16), kwargs = {})\n",
      "      %aten_mul_tensor_34 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_16, %aten_view_copy_default_16), kwargs = {})\n",
      "      %aten_mul_tensor_36 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_16, %aten_view_copy_default_15), kwargs = {})\n",
      "      %aten_sub_tensor_2 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sub.Tensor](args = (%aten_mul_tensor_29, %aten_mul_tensor_30), kwargs = {})\n",
      "      %aten_add_tensor_7 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mul_tensor_31, %aten_mul_tensor_32), kwargs = {})\n",
      "      %aten_sub_tensor_3 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sub.Tensor](args = (%aten_mul_tensor_33, %aten_mul_tensor_34), kwargs = {})\n",
      "      %aten_add_tensor_8 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mul_tensor_35, %aten_mul_tensor_36), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_4 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_sub_tensor_2, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_5 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_add_tensor_7, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_6 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_sub_tensor_3, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_7 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_add_tensor_8, 4), kwargs = {})\n",
      "      %aten_cat_default_2 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.cat.default](args = ([%aten_unsqueeze_copy_default_4, %aten_unsqueeze_copy_default_5], -1), kwargs = {})\n",
      "      %aten_cat_default_3 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.cat.default](args = ([%aten_unsqueeze_copy_default_6, %aten_unsqueeze_copy_default_7], -1), kwargs = {})\n",
      "      %aten_view_copy_default_17 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_cat_default_2, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_18 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_cat_default_3, [1, 1, 6, 48]), kwargs = {})\n",
      "      return (aten_add_tensor_5, aten_view_copy_default_12, aten_view_copy_default_17, aten_view_copy_default_18)\n",
      "  %executorch_call_delegate_1 : [num_users=4] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_1, %getitem_1, %getitem_2, %sdpa_with_kv_cache, %getitem), kwargs = {})\n",
      "  %getitem_6 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_1, 0), kwargs = {})\n",
      "  %getitem_7 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_1, 1), kwargs = {})\n",
      "  %getitem_8 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_1, 2), kwargs = {})\n",
      "  %getitem_9 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_1, 3), kwargs = {})\n",
      "  %alloc_1 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 1, 6, 48), torch.float32),), kwargs = {})\n",
      "  %sdpa_with_kv_cache_1 : [num_users=1] = call_function[target=torch.ops.llama.sdpa_with_kv_cache.out](args = (), kwargs = {query: %getitem_8, key: %getitem_9, value: %getitem_7, key_cache: %b_layers_1_attention_sdpa_kv_cache_k_cache, value_cache: %b_layers_1_attention_sdpa_kv_cache_v_cache, start_pos: %_local_scalar_dense_1, seq_len: 1, attn_mask: None, drpout_p: 0.0, is_causal: False, scale: None, out: %alloc_1})\n",
      "  %lowered_module_2 : [num_users=1] = get_attr[target=lowered_module_2]\n",
      "    backend_id: MPSBackend\n",
      "    lowered graph():\n",
      "      %p_layers_1_ffn_norm_weight : [num_users=1] = placeholder[target=p_layers_1_ffn_norm_weight]\n",
      "      %p_layers_2_attention_norm_weight : [num_users=1] = placeholder[target=p_layers_2_attention_norm_weight]\n",
      "      %b_layers_1_attention_wo_weight : [num_users=1] = placeholder[target=b_layers_1_attention_wo_weight]\n",
      "      %b_layers_1_attention_wo_scales : [num_users=1] = placeholder[target=b_layers_1_attention_wo_scales]\n",
      "      %b_layers_1_feed_forward_w1_weight : [num_users=1] = placeholder[target=b_layers_1_feed_forward_w1_weight]\n",
      "      %b_layers_1_feed_forward_w1_scales : [num_users=1] = placeholder[target=b_layers_1_feed_forward_w1_scales]\n",
      "      %b_layers_1_feed_forward_w3_weight : [num_users=1] = placeholder[target=b_layers_1_feed_forward_w3_weight]\n",
      "      %b_layers_1_feed_forward_w3_scales : [num_users=1] = placeholder[target=b_layers_1_feed_forward_w3_scales]\n",
      "      %b_layers_1_feed_forward_w2_weight : [num_users=1] = placeholder[target=b_layers_1_feed_forward_w2_weight]\n",
      "      %b_layers_1_feed_forward_w2_scales : [num_users=1] = placeholder[target=b_layers_1_feed_forward_w2_scales]\n",
      "      %b_layers_2_attention_wq_weight : [num_users=1] = placeholder[target=b_layers_2_attention_wq_weight]\n",
      "      %b_layers_2_attention_wq_scales : [num_users=1] = placeholder[target=b_layers_2_attention_wq_scales]\n",
      "      %b_layers_2_attention_wk_weight : [num_users=1] = placeholder[target=b_layers_2_attention_wk_weight]\n",
      "      %b_layers_2_attention_wk_scales : [num_users=1] = placeholder[target=b_layers_2_attention_wk_scales]\n",
      "      %b_layers_2_attention_wv_weight : [num_users=1] = placeholder[target=b_layers_2_attention_wv_weight]\n",
      "      %b_layers_2_attention_wv_scales : [num_users=1] = placeholder[target=b_layers_2_attention_wv_scales]\n",
      "      %_lifted_tensor_constant103 : [num_users=1] = placeholder[target=_lifted_tensor_constant103]\n",
      "      %_lifted_tensor_constant104 : [num_users=1] = placeholder[target=_lifted_tensor_constant104]\n",
      "      %aten_index_tensor : [num_users=1] = placeholder[target=aten_index_tensor]\n",
      "      %aten_index_tensor_1 : [num_users=1] = placeholder[target=aten_index_tensor_1]\n",
      "      %getitem_3 : [num_users=1] = placeholder[target=getitem_3]\n",
      "      %aten_add_tensor_5 : [num_users=1] = placeholder[target=aten_add_tensor_5]\n",
      "      %aten_view_copy_default_25 : [num_users=4] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_index_tensor, [1, 1, 1, 24]), kwargs = {})\n",
      "      %aten_view_copy_default_26 : [num_users=4] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_index_tensor_1, [1, 1, 1, 24]), kwargs = {})\n",
      "      %aten_view_copy_default_19 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%getitem_3, [1, 1, 288]), kwargs = {})\n",
      "      %aten__to_copy_default_10 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_1_attention_wo_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_11 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_1_feed_forward_w1_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_12 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_1_feed_forward_w3_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_13 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_1_feed_forward_w2_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_14 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_2_attention_wq_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_15 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_2_attention_wk_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_16 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_2_attention_wv_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten_squeeze_copy_dims_17 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_view_copy_default_19, [0]), kwargs = {})\n",
      "      %aten_permute_copy_default_10 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_10, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_11 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_11, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_12 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_12, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_13 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_13, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_14 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_14, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_15 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_15, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_16 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_16, [1, 0]), kwargs = {})\n",
      "      %aten_mm_default_10 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_17, %aten_permute_copy_default_10), kwargs = {})\n",
      "      %aten_mul_tensor_37 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_10, %b_layers_1_attention_wo_scales), kwargs = {})\n",
      "      %aten_add_tensor_9 : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_add_tensor_5, %aten_mul_tensor_37), kwargs = {})\n",
      "      %aten_mul_tensor_38 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_9, %aten_add_tensor_9), kwargs = {})\n",
      "      %aten_mean_dim_3 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mean.dim](args = (%aten_mul_tensor_38, [-1], True), kwargs = {})\n",
      "      %aten_add_tensor_10 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mean_dim_3, %_lifted_tensor_constant103), kwargs = {})\n",
      "      %aten_rsqrt_default_3 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.rsqrt.default](args = (%aten_add_tensor_10,), kwargs = {})\n",
      "      %aten_mul_tensor_39 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_9, %aten_rsqrt_default_3), kwargs = {})\n",
      "      %aten_mul_tensor_40 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_39, %p_layers_1_ffn_norm_weight), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_18 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_40, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_19 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_40, [0]), kwargs = {})\n",
      "      %aten_mm_default_11 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_18, %aten_permute_copy_default_11), kwargs = {})\n",
      "      %aten_mm_default_12 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_19, %aten_permute_copy_default_12), kwargs = {})\n",
      "      %aten_mul_tensor_41 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_11, %b_layers_1_feed_forward_w1_scales), kwargs = {})\n",
      "      %aten_mul_tensor_43 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_12, %b_layers_1_feed_forward_w3_scales), kwargs = {})\n",
      "      %aten_sigmoid_default_1 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sigmoid.default](args = (%aten_mul_tensor_41,), kwargs = {})\n",
      "      %aten_mul_tensor_42 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_41, %aten_sigmoid_default_1), kwargs = {})\n",
      "      %aten_mul_tensor_44 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_42, %aten_mul_tensor_43), kwargs = {})\n",
      "      %aten_mm_default_13 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_mul_tensor_44, %aten_permute_copy_default_13), kwargs = {})\n",
      "      %aten_mul_tensor_45 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_13, %b_layers_1_feed_forward_w2_scales), kwargs = {})\n",
      "      %aten_add_tensor_11 : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_add_tensor_9, %aten_mul_tensor_45), kwargs = {})\n",
      "      %aten_mul_tensor_46 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_11, %aten_add_tensor_11), kwargs = {})\n",
      "      %aten_mean_dim_4 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mean.dim](args = (%aten_mul_tensor_46, [-1], True), kwargs = {})\n",
      "      %aten_add_tensor_12 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mean_dim_4, %_lifted_tensor_constant104), kwargs = {})\n",
      "      %aten_rsqrt_default_4 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.rsqrt.default](args = (%aten_add_tensor_12,), kwargs = {})\n",
      "      %aten_mul_tensor_47 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_11, %aten_rsqrt_default_4), kwargs = {})\n",
      "      %aten_mul_tensor_48 : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_47, %p_layers_2_attention_norm_weight), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_20 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_48, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_21 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_48, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_22 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_48, [0]), kwargs = {})\n",
      "      %aten_mm_default_14 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_20, %aten_permute_copy_default_14), kwargs = {})\n",
      "      %aten_mm_default_15 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_21, %aten_permute_copy_default_15), kwargs = {})\n",
      "      %aten_mm_default_16 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_22, %aten_permute_copy_default_16), kwargs = {})\n",
      "      %aten_mul_tensor_49 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_14, %b_layers_2_attention_wq_scales), kwargs = {})\n",
      "      %aten_mul_tensor_50 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_15, %b_layers_2_attention_wk_scales), kwargs = {})\n",
      "      %aten_mul_tensor_51 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_16, %b_layers_2_attention_wv_scales), kwargs = {})\n",
      "      %aten_view_copy_default_20 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_mul_tensor_49, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_21 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_mul_tensor_50, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_22 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_mul_tensor_51, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_23 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_view_copy_default_20, [1, 1, 6, -1, 2]), kwargs = {})\n",
      "      %aten_view_copy_default_24 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_view_copy_default_21, [1, 1, 6, -1, 2]), kwargs = {})\n",
      "      %aten_slice_copy_tensor_8 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_23, 4, 0, 1), kwargs = {})\n",
      "      %aten_slice_copy_tensor_9 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_23, 4, 1, 2), kwargs = {})\n",
      "      %aten_slice_copy_tensor_10 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_24, 4, 0, 1), kwargs = {})\n",
      "      %aten_slice_copy_tensor_11 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_24, 4, 1, 2), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_23 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_8, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_24 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_9, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_25 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_10, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_26 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_11, [4]), kwargs = {})\n",
      "      %aten_mul_tensor_52 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_23, %aten_view_copy_default_25), kwargs = {})\n",
      "      %aten_mul_tensor_54 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_23, %aten_view_copy_default_26), kwargs = {})\n",
      "      %aten_mul_tensor_53 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_24, %aten_view_copy_default_26), kwargs = {})\n",
      "      %aten_mul_tensor_55 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_24, %aten_view_copy_default_25), kwargs = {})\n",
      "      %aten_mul_tensor_56 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_25, %aten_view_copy_default_25), kwargs = {})\n",
      "      %aten_mul_tensor_58 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_25, %aten_view_copy_default_26), kwargs = {})\n",
      "      %aten_mul_tensor_57 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_26, %aten_view_copy_default_26), kwargs = {})\n",
      "      %aten_mul_tensor_59 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_26, %aten_view_copy_default_25), kwargs = {})\n",
      "      %aten_sub_tensor_4 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sub.Tensor](args = (%aten_mul_tensor_52, %aten_mul_tensor_53), kwargs = {})\n",
      "      %aten_add_tensor_13 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mul_tensor_54, %aten_mul_tensor_55), kwargs = {})\n",
      "      %aten_sub_tensor_5 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sub.Tensor](args = (%aten_mul_tensor_56, %aten_mul_tensor_57), kwargs = {})\n",
      "      %aten_add_tensor_14 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mul_tensor_58, %aten_mul_tensor_59), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_8 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_sub_tensor_4, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_9 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_add_tensor_13, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_10 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_sub_tensor_5, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_11 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_add_tensor_14, 4), kwargs = {})\n",
      "      %aten_cat_default_4 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.cat.default](args = ([%aten_unsqueeze_copy_default_8, %aten_unsqueeze_copy_default_9], -1), kwargs = {})\n",
      "      %aten_cat_default_5 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.cat.default](args = ([%aten_unsqueeze_copy_default_10, %aten_unsqueeze_copy_default_11], -1), kwargs = {})\n",
      "      %aten_view_copy_default_27 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_cat_default_4, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_28 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_cat_default_5, [1, 1, 6, 48]), kwargs = {})\n",
      "      return (aten_add_tensor_11, aten_view_copy_default_22, aten_view_copy_default_27, aten_view_copy_default_28)\n",
      "  %executorch_call_delegate_2 : [num_users=4] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_2, %getitem_1, %getitem_2, %sdpa_with_kv_cache_1, %getitem_6), kwargs = {})\n",
      "  %getitem_10 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_2, 0), kwargs = {})\n",
      "  %getitem_11 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_2, 1), kwargs = {})\n",
      "  %getitem_12 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_2, 2), kwargs = {})\n",
      "  %getitem_13 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_2, 3), kwargs = {})\n",
      "  %alloc_2 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 1, 6, 48), torch.float32),), kwargs = {})\n",
      "  %sdpa_with_kv_cache_2 : [num_users=1] = call_function[target=torch.ops.llama.sdpa_with_kv_cache.out](args = (), kwargs = {query: %getitem_12, key: %getitem_13, value: %getitem_11, key_cache: %b_layers_2_attention_sdpa_kv_cache_k_cache, value_cache: %b_layers_2_attention_sdpa_kv_cache_v_cache, start_pos: %_local_scalar_dense_2, seq_len: 1, attn_mask: None, drpout_p: 0.0, is_causal: False, scale: None, out: %alloc_2})\n",
      "  %lowered_module_3 : [num_users=1] = get_attr[target=lowered_module_3]\n",
      "    backend_id: MPSBackend\n",
      "    lowered graph():\n",
      "      %p_layers_2_ffn_norm_weight : [num_users=1] = placeholder[target=p_layers_2_ffn_norm_weight]\n",
      "      %p_layers_3_attention_norm_weight : [num_users=1] = placeholder[target=p_layers_3_attention_norm_weight]\n",
      "      %b_layers_2_attention_wo_weight : [num_users=1] = placeholder[target=b_layers_2_attention_wo_weight]\n",
      "      %b_layers_2_attention_wo_scales : [num_users=1] = placeholder[target=b_layers_2_attention_wo_scales]\n",
      "      %b_layers_2_feed_forward_w1_weight : [num_users=1] = placeholder[target=b_layers_2_feed_forward_w1_weight]\n",
      "      %b_layers_2_feed_forward_w1_scales : [num_users=1] = placeholder[target=b_layers_2_feed_forward_w1_scales]\n",
      "      %b_layers_2_feed_forward_w3_weight : [num_users=1] = placeholder[target=b_layers_2_feed_forward_w3_weight]\n",
      "      %b_layers_2_feed_forward_w3_scales : [num_users=1] = placeholder[target=b_layers_2_feed_forward_w3_scales]\n",
      "      %b_layers_2_feed_forward_w2_weight : [num_users=1] = placeholder[target=b_layers_2_feed_forward_w2_weight]\n",
      "      %b_layers_2_feed_forward_w2_scales : [num_users=1] = placeholder[target=b_layers_2_feed_forward_w2_scales]\n",
      "      %b_layers_3_attention_wq_weight : [num_users=1] = placeholder[target=b_layers_3_attention_wq_weight]\n",
      "      %b_layers_3_attention_wq_scales : [num_users=1] = placeholder[target=b_layers_3_attention_wq_scales]\n",
      "      %b_layers_3_attention_wk_weight : [num_users=1] = placeholder[target=b_layers_3_attention_wk_weight]\n",
      "      %b_layers_3_attention_wk_scales : [num_users=1] = placeholder[target=b_layers_3_attention_wk_scales]\n",
      "      %b_layers_3_attention_wv_weight : [num_users=1] = placeholder[target=b_layers_3_attention_wv_weight]\n",
      "      %b_layers_3_attention_wv_scales : [num_users=1] = placeholder[target=b_layers_3_attention_wv_scales]\n",
      "      %_lifted_tensor_constant105 : [num_users=1] = placeholder[target=_lifted_tensor_constant105]\n",
      "      %_lifted_tensor_constant106 : [num_users=1] = placeholder[target=_lifted_tensor_constant106]\n",
      "      %aten_index_tensor : [num_users=1] = placeholder[target=aten_index_tensor]\n",
      "      %aten_index_tensor_1 : [num_users=1] = placeholder[target=aten_index_tensor_1]\n",
      "      %getitem_6 : [num_users=1] = placeholder[target=getitem_6]\n",
      "      %aten_add_tensor_11 : [num_users=1] = placeholder[target=aten_add_tensor_11]\n",
      "      %aten_view_copy_default_35 : [num_users=4] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_index_tensor, [1, 1, 1, 24]), kwargs = {})\n",
      "      %aten_view_copy_default_36 : [num_users=4] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_index_tensor_1, [1, 1, 1, 24]), kwargs = {})\n",
      "      %aten_view_copy_default_29 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%getitem_6, [1, 1, 288]), kwargs = {})\n",
      "      %aten__to_copy_default_17 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_2_attention_wo_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_18 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_2_feed_forward_w1_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_19 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_2_feed_forward_w3_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_20 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_2_feed_forward_w2_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_21 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_3_attention_wq_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_22 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_3_attention_wk_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_23 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_3_attention_wv_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten_squeeze_copy_dims_27 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_view_copy_default_29, [0]), kwargs = {})\n",
      "      %aten_permute_copy_default_17 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_17, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_18 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_18, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_19 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_19, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_20 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_20, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_21 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_21, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_22 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_22, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_23 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_23, [1, 0]), kwargs = {})\n",
      "      %aten_mm_default_17 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_27, %aten_permute_copy_default_17), kwargs = {})\n",
      "      %aten_mul_tensor_60 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_17, %b_layers_2_attention_wo_scales), kwargs = {})\n",
      "      %aten_add_tensor_15 : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_add_tensor_11, %aten_mul_tensor_60), kwargs = {})\n",
      "      %aten_mul_tensor_61 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_15, %aten_add_tensor_15), kwargs = {})\n",
      "      %aten_mean_dim_5 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mean.dim](args = (%aten_mul_tensor_61, [-1], True), kwargs = {})\n",
      "      %aten_add_tensor_16 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mean_dim_5, %_lifted_tensor_constant105), kwargs = {})\n",
      "      %aten_rsqrt_default_5 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.rsqrt.default](args = (%aten_add_tensor_16,), kwargs = {})\n",
      "      %aten_mul_tensor_62 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_15, %aten_rsqrt_default_5), kwargs = {})\n",
      "      %aten_mul_tensor_63 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_62, %p_layers_2_ffn_norm_weight), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_28 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_63, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_29 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_63, [0]), kwargs = {})\n",
      "      %aten_mm_default_18 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_28, %aten_permute_copy_default_18), kwargs = {})\n",
      "      %aten_mm_default_19 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_29, %aten_permute_copy_default_19), kwargs = {})\n",
      "      %aten_mul_tensor_64 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_18, %b_layers_2_feed_forward_w1_scales), kwargs = {})\n",
      "      %aten_mul_tensor_66 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_19, %b_layers_2_feed_forward_w3_scales), kwargs = {})\n",
      "      %aten_sigmoid_default_2 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sigmoid.default](args = (%aten_mul_tensor_64,), kwargs = {})\n",
      "      %aten_mul_tensor_65 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_64, %aten_sigmoid_default_2), kwargs = {})\n",
      "      %aten_mul_tensor_67 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_65, %aten_mul_tensor_66), kwargs = {})\n",
      "      %aten_mm_default_20 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_mul_tensor_67, %aten_permute_copy_default_20), kwargs = {})\n",
      "      %aten_mul_tensor_68 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_20, %b_layers_2_feed_forward_w2_scales), kwargs = {})\n",
      "      %aten_add_tensor_17 : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_add_tensor_15, %aten_mul_tensor_68), kwargs = {})\n",
      "      %aten_mul_tensor_69 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_17, %aten_add_tensor_17), kwargs = {})\n",
      "      %aten_mean_dim_6 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mean.dim](args = (%aten_mul_tensor_69, [-1], True), kwargs = {})\n",
      "      %aten_add_tensor_18 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mean_dim_6, %_lifted_tensor_constant106), kwargs = {})\n",
      "      %aten_rsqrt_default_6 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.rsqrt.default](args = (%aten_add_tensor_18,), kwargs = {})\n",
      "      %aten_mul_tensor_70 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_17, %aten_rsqrt_default_6), kwargs = {})\n",
      "      %aten_mul_tensor_71 : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_70, %p_layers_3_attention_norm_weight), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_30 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_71, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_31 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_71, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_32 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_71, [0]), kwargs = {})\n",
      "      %aten_mm_default_21 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_30, %aten_permute_copy_default_21), kwargs = {})\n",
      "      %aten_mm_default_22 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_31, %aten_permute_copy_default_22), kwargs = {})\n",
      "      %aten_mm_default_23 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_32, %aten_permute_copy_default_23), kwargs = {})\n",
      "      %aten_mul_tensor_72 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_21, %b_layers_3_attention_wq_scales), kwargs = {})\n",
      "      %aten_mul_tensor_73 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_22, %b_layers_3_attention_wk_scales), kwargs = {})\n",
      "      %aten_mul_tensor_74 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_23, %b_layers_3_attention_wv_scales), kwargs = {})\n",
      "      %aten_view_copy_default_30 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_mul_tensor_72, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_31 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_mul_tensor_73, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_32 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_mul_tensor_74, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_33 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_view_copy_default_30, [1, 1, 6, -1, 2]), kwargs = {})\n",
      "      %aten_view_copy_default_34 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_view_copy_default_31, [1, 1, 6, -1, 2]), kwargs = {})\n",
      "      %aten_slice_copy_tensor_12 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_33, 4, 0, 1), kwargs = {})\n",
      "      %aten_slice_copy_tensor_13 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_33, 4, 1, 2), kwargs = {})\n",
      "      %aten_slice_copy_tensor_14 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_34, 4, 0, 1), kwargs = {})\n",
      "      %aten_slice_copy_tensor_15 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_34, 4, 1, 2), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_33 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_12, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_34 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_13, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_35 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_14, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_36 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_15, [4]), kwargs = {})\n",
      "      %aten_mul_tensor_75 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_33, %aten_view_copy_default_35), kwargs = {})\n",
      "      %aten_mul_tensor_77 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_33, %aten_view_copy_default_36), kwargs = {})\n",
      "      %aten_mul_tensor_76 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_34, %aten_view_copy_default_36), kwargs = {})\n",
      "      %aten_mul_tensor_78 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_34, %aten_view_copy_default_35), kwargs = {})\n",
      "      %aten_mul_tensor_79 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_35, %aten_view_copy_default_35), kwargs = {})\n",
      "      %aten_mul_tensor_81 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_35, %aten_view_copy_default_36), kwargs = {})\n",
      "      %aten_mul_tensor_80 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_36, %aten_view_copy_default_36), kwargs = {})\n",
      "      %aten_mul_tensor_82 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_36, %aten_view_copy_default_35), kwargs = {})\n",
      "      %aten_sub_tensor_6 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sub.Tensor](args = (%aten_mul_tensor_75, %aten_mul_tensor_76), kwargs = {})\n",
      "      %aten_add_tensor_19 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mul_tensor_77, %aten_mul_tensor_78), kwargs = {})\n",
      "      %aten_sub_tensor_7 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sub.Tensor](args = (%aten_mul_tensor_79, %aten_mul_tensor_80), kwargs = {})\n",
      "      %aten_add_tensor_20 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mul_tensor_81, %aten_mul_tensor_82), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_12 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_sub_tensor_6, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_13 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_add_tensor_19, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_14 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_sub_tensor_7, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_15 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_add_tensor_20, 4), kwargs = {})\n",
      "      %aten_cat_default_6 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.cat.default](args = ([%aten_unsqueeze_copy_default_12, %aten_unsqueeze_copy_default_13], -1), kwargs = {})\n",
      "      %aten_cat_default_7 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.cat.default](args = ([%aten_unsqueeze_copy_default_14, %aten_unsqueeze_copy_default_15], -1), kwargs = {})\n",
      "      %aten_view_copy_default_37 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_cat_default_6, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_38 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_cat_default_7, [1, 1, 6, 48]), kwargs = {})\n",
      "      return (aten_add_tensor_17, aten_view_copy_default_32, aten_view_copy_default_37, aten_view_copy_default_38)\n",
      "  %executorch_call_delegate_3 : [num_users=4] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_3, %getitem_1, %getitem_2, %sdpa_with_kv_cache_2, %getitem_10), kwargs = {})\n",
      "  %getitem_14 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_3, 0), kwargs = {})\n",
      "  %getitem_15 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_3, 1), kwargs = {})\n",
      "  %getitem_16 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_3, 2), kwargs = {})\n",
      "  %getitem_17 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_3, 3), kwargs = {})\n",
      "  %alloc_3 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 1, 6, 48), torch.float32),), kwargs = {})\n",
      "  %sdpa_with_kv_cache_3 : [num_users=1] = call_function[target=torch.ops.llama.sdpa_with_kv_cache.out](args = (), kwargs = {query: %getitem_16, key: %getitem_17, value: %getitem_15, key_cache: %b_layers_3_attention_sdpa_kv_cache_k_cache, value_cache: %b_layers_3_attention_sdpa_kv_cache_v_cache, start_pos: %_local_scalar_dense_3, seq_len: 1, attn_mask: None, drpout_p: 0.0, is_causal: False, scale: None, out: %alloc_3})\n",
      "  %lowered_module_4 : [num_users=1] = get_attr[target=lowered_module_4]\n",
      "    backend_id: MPSBackend\n",
      "    lowered graph():\n",
      "      %p_layers_3_ffn_norm_weight : [num_users=1] = placeholder[target=p_layers_3_ffn_norm_weight]\n",
      "      %p_layers_4_attention_norm_weight : [num_users=1] = placeholder[target=p_layers_4_attention_norm_weight]\n",
      "      %b_layers_3_attention_wo_weight : [num_users=1] = placeholder[target=b_layers_3_attention_wo_weight]\n",
      "      %b_layers_3_attention_wo_scales : [num_users=1] = placeholder[target=b_layers_3_attention_wo_scales]\n",
      "      %b_layers_3_feed_forward_w1_weight : [num_users=1] = placeholder[target=b_layers_3_feed_forward_w1_weight]\n",
      "      %b_layers_3_feed_forward_w1_scales : [num_users=1] = placeholder[target=b_layers_3_feed_forward_w1_scales]\n",
      "      %b_layers_3_feed_forward_w3_weight : [num_users=1] = placeholder[target=b_layers_3_feed_forward_w3_weight]\n",
      "      %b_layers_3_feed_forward_w3_scales : [num_users=1] = placeholder[target=b_layers_3_feed_forward_w3_scales]\n",
      "      %b_layers_3_feed_forward_w2_weight : [num_users=1] = placeholder[target=b_layers_3_feed_forward_w2_weight]\n",
      "      %b_layers_3_feed_forward_w2_scales : [num_users=1] = placeholder[target=b_layers_3_feed_forward_w2_scales]\n",
      "      %b_layers_4_attention_wq_weight : [num_users=1] = placeholder[target=b_layers_4_attention_wq_weight]\n",
      "      %b_layers_4_attention_wq_scales : [num_users=1] = placeholder[target=b_layers_4_attention_wq_scales]\n",
      "      %b_layers_4_attention_wk_weight : [num_users=1] = placeholder[target=b_layers_4_attention_wk_weight]\n",
      "      %b_layers_4_attention_wk_scales : [num_users=1] = placeholder[target=b_layers_4_attention_wk_scales]\n",
      "      %b_layers_4_attention_wv_weight : [num_users=1] = placeholder[target=b_layers_4_attention_wv_weight]\n",
      "      %b_layers_4_attention_wv_scales : [num_users=1] = placeholder[target=b_layers_4_attention_wv_scales]\n",
      "      %_lifted_tensor_constant107 : [num_users=1] = placeholder[target=_lifted_tensor_constant107]\n",
      "      %_lifted_tensor_constant108 : [num_users=1] = placeholder[target=_lifted_tensor_constant108]\n",
      "      %aten_index_tensor : [num_users=1] = placeholder[target=aten_index_tensor]\n",
      "      %aten_index_tensor_1 : [num_users=1] = placeholder[target=aten_index_tensor_1]\n",
      "      %getitem_9 : [num_users=1] = placeholder[target=getitem_9]\n",
      "      %aten_add_tensor_17 : [num_users=1] = placeholder[target=aten_add_tensor_17]\n",
      "      %aten_view_copy_default_45 : [num_users=4] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_index_tensor, [1, 1, 1, 24]), kwargs = {})\n",
      "      %aten_view_copy_default_46 : [num_users=4] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_index_tensor_1, [1, 1, 1, 24]), kwargs = {})\n",
      "      %aten_view_copy_default_39 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%getitem_9, [1, 1, 288]), kwargs = {})\n",
      "      %aten__to_copy_default_24 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_3_attention_wo_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_25 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_3_feed_forward_w1_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_26 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_3_feed_forward_w3_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_27 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_3_feed_forward_w2_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_28 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_4_attention_wq_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_29 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_4_attention_wk_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_30 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_4_attention_wv_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten_squeeze_copy_dims_37 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_view_copy_default_39, [0]), kwargs = {})\n",
      "      %aten_permute_copy_default_24 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_24, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_25 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_25, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_26 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_26, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_27 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_27, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_28 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_28, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_29 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_29, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_30 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_30, [1, 0]), kwargs = {})\n",
      "      %aten_mm_default_24 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_37, %aten_permute_copy_default_24), kwargs = {})\n",
      "      %aten_mul_tensor_83 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_24, %b_layers_3_attention_wo_scales), kwargs = {})\n",
      "      %aten_add_tensor_21 : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_add_tensor_17, %aten_mul_tensor_83), kwargs = {})\n",
      "      %aten_mul_tensor_84 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_21, %aten_add_tensor_21), kwargs = {})\n",
      "      %aten_mean_dim_7 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mean.dim](args = (%aten_mul_tensor_84, [-1], True), kwargs = {})\n",
      "      %aten_add_tensor_22 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mean_dim_7, %_lifted_tensor_constant107), kwargs = {})\n",
      "      %aten_rsqrt_default_7 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.rsqrt.default](args = (%aten_add_tensor_22,), kwargs = {})\n",
      "      %aten_mul_tensor_85 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_21, %aten_rsqrt_default_7), kwargs = {})\n",
      "      %aten_mul_tensor_86 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_85, %p_layers_3_ffn_norm_weight), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_38 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_86, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_39 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_86, [0]), kwargs = {})\n",
      "      %aten_mm_default_25 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_38, %aten_permute_copy_default_25), kwargs = {})\n",
      "      %aten_mm_default_26 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_39, %aten_permute_copy_default_26), kwargs = {})\n",
      "      %aten_mul_tensor_87 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_25, %b_layers_3_feed_forward_w1_scales), kwargs = {})\n",
      "      %aten_mul_tensor_89 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_26, %b_layers_3_feed_forward_w3_scales), kwargs = {})\n",
      "      %aten_sigmoid_default_3 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sigmoid.default](args = (%aten_mul_tensor_87,), kwargs = {})\n",
      "      %aten_mul_tensor_88 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_87, %aten_sigmoid_default_3), kwargs = {})\n",
      "      %aten_mul_tensor_90 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_88, %aten_mul_tensor_89), kwargs = {})\n",
      "      %aten_mm_default_27 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_mul_tensor_90, %aten_permute_copy_default_27), kwargs = {})\n",
      "      %aten_mul_tensor_91 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_27, %b_layers_3_feed_forward_w2_scales), kwargs = {})\n",
      "      %aten_add_tensor_23 : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_add_tensor_21, %aten_mul_tensor_91), kwargs = {})\n",
      "      %aten_mul_tensor_92 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_23, %aten_add_tensor_23), kwargs = {})\n",
      "      %aten_mean_dim_8 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mean.dim](args = (%aten_mul_tensor_92, [-1], True), kwargs = {})\n",
      "      %aten_add_tensor_24 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mean_dim_8, %_lifted_tensor_constant108), kwargs = {})\n",
      "      %aten_rsqrt_default_8 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.rsqrt.default](args = (%aten_add_tensor_24,), kwargs = {})\n",
      "      %aten_mul_tensor_93 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_23, %aten_rsqrt_default_8), kwargs = {})\n",
      "      %aten_mul_tensor_94 : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_93, %p_layers_4_attention_norm_weight), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_40 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_94, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_41 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_94, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_42 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_94, [0]), kwargs = {})\n",
      "      %aten_mm_default_28 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_40, %aten_permute_copy_default_28), kwargs = {})\n",
      "      %aten_mm_default_29 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_41, %aten_permute_copy_default_29), kwargs = {})\n",
      "      %aten_mm_default_30 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_42, %aten_permute_copy_default_30), kwargs = {})\n",
      "      %aten_mul_tensor_95 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_28, %b_layers_4_attention_wq_scales), kwargs = {})\n",
      "      %aten_mul_tensor_96 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_29, %b_layers_4_attention_wk_scales), kwargs = {})\n",
      "      %aten_mul_tensor_97 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_30, %b_layers_4_attention_wv_scales), kwargs = {})\n",
      "      %aten_view_copy_default_40 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_mul_tensor_95, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_41 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_mul_tensor_96, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_42 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_mul_tensor_97, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_43 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_view_copy_default_40, [1, 1, 6, -1, 2]), kwargs = {})\n",
      "      %aten_view_copy_default_44 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_view_copy_default_41, [1, 1, 6, -1, 2]), kwargs = {})\n",
      "      %aten_slice_copy_tensor_16 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_43, 4, 0, 1), kwargs = {})\n",
      "      %aten_slice_copy_tensor_17 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_43, 4, 1, 2), kwargs = {})\n",
      "      %aten_slice_copy_tensor_18 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_44, 4, 0, 1), kwargs = {})\n",
      "      %aten_slice_copy_tensor_19 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_44, 4, 1, 2), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_43 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_16, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_44 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_17, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_45 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_18, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_46 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_19, [4]), kwargs = {})\n",
      "      %aten_mul_tensor_98 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_43, %aten_view_copy_default_45), kwargs = {})\n",
      "      %aten_mul_tensor_100 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_43, %aten_view_copy_default_46), kwargs = {})\n",
      "      %aten_mul_tensor_99 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_44, %aten_view_copy_default_46), kwargs = {})\n",
      "      %aten_mul_tensor_101 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_44, %aten_view_copy_default_45), kwargs = {})\n",
      "      %aten_mul_tensor_102 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_45, %aten_view_copy_default_45), kwargs = {})\n",
      "      %aten_mul_tensor_104 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_45, %aten_view_copy_default_46), kwargs = {})\n",
      "      %aten_mul_tensor_103 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_46, %aten_view_copy_default_46), kwargs = {})\n",
      "      %aten_mul_tensor_105 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_46, %aten_view_copy_default_45), kwargs = {})\n",
      "      %aten_sub_tensor_8 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sub.Tensor](args = (%aten_mul_tensor_98, %aten_mul_tensor_99), kwargs = {})\n",
      "      %aten_add_tensor_25 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mul_tensor_100, %aten_mul_tensor_101), kwargs = {})\n",
      "      %aten_sub_tensor_9 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sub.Tensor](args = (%aten_mul_tensor_102, %aten_mul_tensor_103), kwargs = {})\n",
      "      %aten_add_tensor_26 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mul_tensor_104, %aten_mul_tensor_105), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_16 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_sub_tensor_8, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_17 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_add_tensor_25, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_18 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_sub_tensor_9, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_19 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_add_tensor_26, 4), kwargs = {})\n",
      "      %aten_cat_default_8 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.cat.default](args = ([%aten_unsqueeze_copy_default_16, %aten_unsqueeze_copy_default_17], -1), kwargs = {})\n",
      "      %aten_cat_default_9 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.cat.default](args = ([%aten_unsqueeze_copy_default_18, %aten_unsqueeze_copy_default_19], -1), kwargs = {})\n",
      "      %aten_view_copy_default_47 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_cat_default_8, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_48 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_cat_default_9, [1, 1, 6, 48]), kwargs = {})\n",
      "      return (aten_add_tensor_23, aten_view_copy_default_42, aten_view_copy_default_47, aten_view_copy_default_48)\n",
      "  %executorch_call_delegate_4 : [num_users=4] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_4, %getitem_1, %getitem_2, %sdpa_with_kv_cache_3, %getitem_14), kwargs = {})\n",
      "  %getitem_18 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_4, 0), kwargs = {})\n",
      "  %getitem_19 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_4, 1), kwargs = {})\n",
      "  %getitem_20 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_4, 2), kwargs = {})\n",
      "  %getitem_21 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_4, 3), kwargs = {})\n",
      "  %alloc_4 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 1, 6, 48), torch.float32),), kwargs = {})\n",
      "  %sdpa_with_kv_cache_4 : [num_users=1] = call_function[target=torch.ops.llama.sdpa_with_kv_cache.out](args = (), kwargs = {query: %getitem_20, key: %getitem_21, value: %getitem_19, key_cache: %b_layers_4_attention_sdpa_kv_cache_k_cache, value_cache: %b_layers_4_attention_sdpa_kv_cache_v_cache, start_pos: %_local_scalar_dense_4, seq_len: 1, attn_mask: None, drpout_p: 0.0, is_causal: False, scale: None, out: %alloc_4})\n",
      "  %lowered_module_5 : [num_users=1] = get_attr[target=lowered_module_5]\n",
      "    backend_id: MPSBackend\n",
      "    lowered graph():\n",
      "      %p_layers_4_ffn_norm_weight : [num_users=1] = placeholder[target=p_layers_4_ffn_norm_weight]\n",
      "      %p_layers_5_attention_norm_weight : [num_users=1] = placeholder[target=p_layers_5_attention_norm_weight]\n",
      "      %b_layers_4_attention_wo_weight : [num_users=1] = placeholder[target=b_layers_4_attention_wo_weight]\n",
      "      %b_layers_4_attention_wo_scales : [num_users=1] = placeholder[target=b_layers_4_attention_wo_scales]\n",
      "      %b_layers_4_feed_forward_w1_weight : [num_users=1] = placeholder[target=b_layers_4_feed_forward_w1_weight]\n",
      "      %b_layers_4_feed_forward_w1_scales : [num_users=1] = placeholder[target=b_layers_4_feed_forward_w1_scales]\n",
      "      %b_layers_4_feed_forward_w3_weight : [num_users=1] = placeholder[target=b_layers_4_feed_forward_w3_weight]\n",
      "      %b_layers_4_feed_forward_w3_scales : [num_users=1] = placeholder[target=b_layers_4_feed_forward_w3_scales]\n",
      "      %b_layers_4_feed_forward_w2_weight : [num_users=1] = placeholder[target=b_layers_4_feed_forward_w2_weight]\n",
      "      %b_layers_4_feed_forward_w2_scales : [num_users=1] = placeholder[target=b_layers_4_feed_forward_w2_scales]\n",
      "      %b_layers_5_attention_wq_weight : [num_users=1] = placeholder[target=b_layers_5_attention_wq_weight]\n",
      "      %b_layers_5_attention_wq_scales : [num_users=1] = placeholder[target=b_layers_5_attention_wq_scales]\n",
      "      %b_layers_5_attention_wk_weight : [num_users=1] = placeholder[target=b_layers_5_attention_wk_weight]\n",
      "      %b_layers_5_attention_wk_scales : [num_users=1] = placeholder[target=b_layers_5_attention_wk_scales]\n",
      "      %b_layers_5_attention_wv_weight : [num_users=1] = placeholder[target=b_layers_5_attention_wv_weight]\n",
      "      %b_layers_5_attention_wv_scales : [num_users=1] = placeholder[target=b_layers_5_attention_wv_scales]\n",
      "      %_lifted_tensor_constant109 : [num_users=1] = placeholder[target=_lifted_tensor_constant109]\n",
      "      %_lifted_tensor_constant110 : [num_users=1] = placeholder[target=_lifted_tensor_constant110]\n",
      "      %getitem_12 : [num_users=1] = placeholder[target=getitem_12]\n",
      "      %aten_index_tensor : [num_users=1] = placeholder[target=aten_index_tensor]\n",
      "      %aten_index_tensor_1 : [num_users=1] = placeholder[target=aten_index_tensor_1]\n",
      "      %aten_add_tensor_23 : [num_users=1] = placeholder[target=aten_add_tensor_23]\n",
      "      %aten_view_copy_default_49 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%getitem_12, [1, 1, 288]), kwargs = {})\n",
      "      %aten_view_copy_default_55 : [num_users=4] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_index_tensor, [1, 1, 1, 24]), kwargs = {})\n",
      "      %aten_view_copy_default_56 : [num_users=4] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_index_tensor_1, [1, 1, 1, 24]), kwargs = {})\n",
      "      %aten__to_copy_default_31 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_4_attention_wo_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_32 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_4_feed_forward_w1_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_33 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_4_feed_forward_w3_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_34 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_4_feed_forward_w2_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_35 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_5_attention_wq_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_36 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_5_attention_wk_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_37 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_5_attention_wv_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten_squeeze_copy_dims_47 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_view_copy_default_49, [0]), kwargs = {})\n",
      "      %aten_permute_copy_default_31 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_31, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_32 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_32, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_33 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_33, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_34 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_34, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_35 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_35, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_36 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_36, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_37 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_37, [1, 0]), kwargs = {})\n",
      "      %aten_mm_default_31 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_47, %aten_permute_copy_default_31), kwargs = {})\n",
      "      %aten_mul_tensor_106 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_31, %b_layers_4_attention_wo_scales), kwargs = {})\n",
      "      %aten_add_tensor_27 : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_add_tensor_23, %aten_mul_tensor_106), kwargs = {})\n",
      "      %aten_mul_tensor_107 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_27, %aten_add_tensor_27), kwargs = {})\n",
      "      %aten_mean_dim_9 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mean.dim](args = (%aten_mul_tensor_107, [-1], True), kwargs = {})\n",
      "      %aten_add_tensor_28 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mean_dim_9, %_lifted_tensor_constant109), kwargs = {})\n",
      "      %aten_rsqrt_default_9 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.rsqrt.default](args = (%aten_add_tensor_28,), kwargs = {})\n",
      "      %aten_mul_tensor_108 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_27, %aten_rsqrt_default_9), kwargs = {})\n",
      "      %aten_mul_tensor_109 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_108, %p_layers_4_ffn_norm_weight), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_48 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_109, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_49 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_109, [0]), kwargs = {})\n",
      "      %aten_mm_default_32 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_48, %aten_permute_copy_default_32), kwargs = {})\n",
      "      %aten_mm_default_33 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_49, %aten_permute_copy_default_33), kwargs = {})\n",
      "      %aten_mul_tensor_110 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_32, %b_layers_4_feed_forward_w1_scales), kwargs = {})\n",
      "      %aten_mul_tensor_112 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_33, %b_layers_4_feed_forward_w3_scales), kwargs = {})\n",
      "      %aten_sigmoid_default_4 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sigmoid.default](args = (%aten_mul_tensor_110,), kwargs = {})\n",
      "      %aten_mul_tensor_111 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_110, %aten_sigmoid_default_4), kwargs = {})\n",
      "      %aten_mul_tensor_113 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_111, %aten_mul_tensor_112), kwargs = {})\n",
      "      %aten_mm_default_34 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_mul_tensor_113, %aten_permute_copy_default_34), kwargs = {})\n",
      "      %aten_mul_tensor_114 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_34, %b_layers_4_feed_forward_w2_scales), kwargs = {})\n",
      "      %aten_add_tensor_29 : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_add_tensor_27, %aten_mul_tensor_114), kwargs = {})\n",
      "      %aten_mul_tensor_115 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_29, %aten_add_tensor_29), kwargs = {})\n",
      "      %aten_mean_dim_10 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mean.dim](args = (%aten_mul_tensor_115, [-1], True), kwargs = {})\n",
      "      %aten_add_tensor_30 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mean_dim_10, %_lifted_tensor_constant110), kwargs = {})\n",
      "      %aten_rsqrt_default_10 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.rsqrt.default](args = (%aten_add_tensor_30,), kwargs = {})\n",
      "      %aten_mul_tensor_116 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_29, %aten_rsqrt_default_10), kwargs = {})\n",
      "      %aten_mul_tensor_117 : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_116, %p_layers_5_attention_norm_weight), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_50 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_117, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_51 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_117, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_52 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_117, [0]), kwargs = {})\n",
      "      %aten_mm_default_35 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_50, %aten_permute_copy_default_35), kwargs = {})\n",
      "      %aten_mm_default_36 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_51, %aten_permute_copy_default_36), kwargs = {})\n",
      "      %aten_mm_default_37 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_52, %aten_permute_copy_default_37), kwargs = {})\n",
      "      %aten_mul_tensor_118 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_35, %b_layers_5_attention_wq_scales), kwargs = {})\n",
      "      %aten_mul_tensor_119 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_36, %b_layers_5_attention_wk_scales), kwargs = {})\n",
      "      %aten_mul_tensor_120 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_37, %b_layers_5_attention_wv_scales), kwargs = {})\n",
      "      %aten_view_copy_default_50 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_mul_tensor_118, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_51 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_mul_tensor_119, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_52 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_mul_tensor_120, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_53 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_view_copy_default_50, [1, 1, 6, -1, 2]), kwargs = {})\n",
      "      %aten_view_copy_default_54 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_view_copy_default_51, [1, 1, 6, -1, 2]), kwargs = {})\n",
      "      %aten_slice_copy_tensor_20 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_53, 4, 0, 1), kwargs = {})\n",
      "      %aten_slice_copy_tensor_21 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_53, 4, 1, 2), kwargs = {})\n",
      "      %aten_slice_copy_tensor_22 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_54, 4, 0, 1), kwargs = {})\n",
      "      %aten_slice_copy_tensor_23 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.slice_copy.Tensor](args = (%aten_view_copy_default_54, 4, 1, 2), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_53 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_20, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_54 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_21, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_55 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_22, [4]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_56 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_slice_copy_tensor_23, [4]), kwargs = {})\n",
      "      %aten_mul_tensor_121 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_53, %aten_view_copy_default_55), kwargs = {})\n",
      "      %aten_mul_tensor_123 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_53, %aten_view_copy_default_56), kwargs = {})\n",
      "      %aten_mul_tensor_122 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_54, %aten_view_copy_default_56), kwargs = {})\n",
      "      %aten_mul_tensor_124 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_54, %aten_view_copy_default_55), kwargs = {})\n",
      "      %aten_mul_tensor_125 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_55, %aten_view_copy_default_55), kwargs = {})\n",
      "      %aten_mul_tensor_127 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_55, %aten_view_copy_default_56), kwargs = {})\n",
      "      %aten_mul_tensor_126 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_56, %aten_view_copy_default_56), kwargs = {})\n",
      "      %aten_mul_tensor_128 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_squeeze_copy_dims_56, %aten_view_copy_default_55), kwargs = {})\n",
      "      %aten_sub_tensor_10 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sub.Tensor](args = (%aten_mul_tensor_121, %aten_mul_tensor_122), kwargs = {})\n",
      "      %aten_add_tensor_31 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mul_tensor_123, %aten_mul_tensor_124), kwargs = {})\n",
      "      %aten_sub_tensor_11 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sub.Tensor](args = (%aten_mul_tensor_125, %aten_mul_tensor_126), kwargs = {})\n",
      "      %aten_add_tensor_32 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mul_tensor_127, %aten_mul_tensor_128), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_20 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_sub_tensor_10, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_21 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_add_tensor_31, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_22 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_sub_tensor_11, 4), kwargs = {})\n",
      "      %aten_unsqueeze_copy_default_23 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.unsqueeze_copy.default](args = (%aten_add_tensor_32, 4), kwargs = {})\n",
      "      %aten_cat_default_10 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.cat.default](args = ([%aten_unsqueeze_copy_default_20, %aten_unsqueeze_copy_default_21], -1), kwargs = {})\n",
      "      %aten_cat_default_11 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.cat.default](args = ([%aten_unsqueeze_copy_default_22, %aten_unsqueeze_copy_default_23], -1), kwargs = {})\n",
      "      %aten_view_copy_default_57 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_cat_default_10, [1, 1, 6, 48]), kwargs = {})\n",
      "      %aten_view_copy_default_58 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%aten_cat_default_11, [1, 1, 6, 48]), kwargs = {})\n",
      "      return (aten_add_tensor_29, aten_view_copy_default_52, aten_view_copy_default_57, aten_view_copy_default_58)\n",
      "  %executorch_call_delegate_5 : [num_users=4] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_5, %sdpa_with_kv_cache_4, %getitem_1, %getitem_2, %getitem_18), kwargs = {})\n",
      "  %getitem_22 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_5, 0), kwargs = {})\n",
      "  %getitem_23 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_5, 1), kwargs = {})\n",
      "  %getitem_24 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_5, 2), kwargs = {})\n",
      "  %getitem_25 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_5, 3), kwargs = {})\n",
      "  %alloc_5 : [num_users=1] = call_function[target=executorch.exir.memory.alloc](args = (((1, 1, 6, 48), torch.float32),), kwargs = {})\n",
      "  %sdpa_with_kv_cache_5 : [num_users=1] = call_function[target=torch.ops.llama.sdpa_with_kv_cache.out](args = (), kwargs = {query: %getitem_24, key: %getitem_25, value: %getitem_23, key_cache: %b_layers_5_attention_sdpa_kv_cache_k_cache, value_cache: %b_layers_5_attention_sdpa_kv_cache_v_cache, start_pos: %_local_scalar_dense_5, seq_len: 1, attn_mask: None, drpout_p: 0.0, is_causal: False, scale: None, out: %alloc_5})\n",
      "  %lowered_module_6 : [num_users=1] = get_attr[target=lowered_module_6]\n",
      "    backend_id: MPSBackend\n",
      "    lowered graph():\n",
      "      %p_layers_5_ffn_norm_weight : [num_users=1] = placeholder[target=p_layers_5_ffn_norm_weight]\n",
      "      %p_norm_weight : [num_users=1] = placeholder[target=p_norm_weight]\n",
      "      %b_layers_5_attention_wo_weight : [num_users=1] = placeholder[target=b_layers_5_attention_wo_weight]\n",
      "      %b_layers_5_attention_wo_scales : [num_users=1] = placeholder[target=b_layers_5_attention_wo_scales]\n",
      "      %b_layers_5_feed_forward_w1_weight : [num_users=1] = placeholder[target=b_layers_5_feed_forward_w1_weight]\n",
      "      %b_layers_5_feed_forward_w1_scales : [num_users=1] = placeholder[target=b_layers_5_feed_forward_w1_scales]\n",
      "      %b_layers_5_feed_forward_w3_weight : [num_users=1] = placeholder[target=b_layers_5_feed_forward_w3_weight]\n",
      "      %b_layers_5_feed_forward_w3_scales : [num_users=1] = placeholder[target=b_layers_5_feed_forward_w3_scales]\n",
      "      %b_layers_5_feed_forward_w2_weight : [num_users=1] = placeholder[target=b_layers_5_feed_forward_w2_weight]\n",
      "      %b_layers_5_feed_forward_w2_scales : [num_users=1] = placeholder[target=b_layers_5_feed_forward_w2_scales]\n",
      "      %b_output_weight : [num_users=1] = placeholder[target=b_output_weight]\n",
      "      %b_output_scales : [num_users=1] = placeholder[target=b_output_scales]\n",
      "      %_lifted_tensor_constant111 : [num_users=1] = placeholder[target=_lifted_tensor_constant111]\n",
      "      %_lifted_tensor_constant112 : [num_users=1] = placeholder[target=_lifted_tensor_constant112]\n",
      "      %getitem_15 : [num_users=1] = placeholder[target=getitem_15]\n",
      "      %getitem_18 : [num_users=1] = placeholder[target=getitem_18]\n",
      "      %aten_view_copy_default_59 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.view_copy.default](args = (%getitem_15, [1, 1, 288]), kwargs = {})\n",
      "      %aten__to_copy_default_38 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_5_attention_wo_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_39 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_5_feed_forward_w1_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_40 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_5_feed_forward_w3_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_41 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_layers_5_feed_forward_w2_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten__to_copy_default_42 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten._to_copy.default](args = (%b_output_weight,), kwargs = {dtype: torch.float32})\n",
      "      %aten_squeeze_copy_dims_57 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_view_copy_default_59, [0]), kwargs = {})\n",
      "      %aten_permute_copy_default_38 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_38, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_39 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_39, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_40 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_40, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_41 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_41, [1, 0]), kwargs = {})\n",
      "      %aten_permute_copy_default_42 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.permute_copy.default](args = (%aten__to_copy_default_42, [1, 0]), kwargs = {})\n",
      "      %aten_mm_default_38 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_57, %aten_permute_copy_default_38), kwargs = {})\n",
      "      %aten_mul_tensor_129 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_38, %b_layers_5_attention_wo_scales), kwargs = {})\n",
      "      %aten_add_tensor_33 : [num_users=3] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%getitem_18, %aten_mul_tensor_129), kwargs = {})\n",
      "      %aten_mul_tensor_130 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_33, %aten_add_tensor_33), kwargs = {})\n",
      "      %aten_mean_dim_11 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mean.dim](args = (%aten_mul_tensor_130, [-1], True), kwargs = {})\n",
      "      %aten_add_tensor_34 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mean_dim_11, %_lifted_tensor_constant111), kwargs = {})\n",
      "      %aten_rsqrt_default_11 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.rsqrt.default](args = (%aten_add_tensor_34,), kwargs = {})\n",
      "      %aten_mul_tensor_131 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_33, %aten_rsqrt_default_11), kwargs = {})\n",
      "      %aten_mul_tensor_132 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_131, %p_layers_5_ffn_norm_weight), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_58 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_132, [0]), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_59 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_132, [0]), kwargs = {})\n",
      "      %aten_mm_default_39 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_58, %aten_permute_copy_default_39), kwargs = {})\n",
      "      %aten_mm_default_40 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_59, %aten_permute_copy_default_40), kwargs = {})\n",
      "      %aten_mul_tensor_133 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_39, %b_layers_5_feed_forward_w1_scales), kwargs = {})\n",
      "      %aten_mul_tensor_135 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_40, %b_layers_5_feed_forward_w3_scales), kwargs = {})\n",
      "      %aten_sigmoid_default_5 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.sigmoid.default](args = (%aten_mul_tensor_133,), kwargs = {})\n",
      "      %aten_mul_tensor_134 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_133, %aten_sigmoid_default_5), kwargs = {})\n",
      "      %aten_mul_tensor_136 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_134, %aten_mul_tensor_135), kwargs = {})\n",
      "      %aten_mm_default_41 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_mul_tensor_136, %aten_permute_copy_default_41), kwargs = {})\n",
      "      %aten_mul_tensor_137 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_41, %b_layers_5_feed_forward_w2_scales), kwargs = {})\n",
      "      %aten_add_tensor_35 : [num_users=2] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_add_tensor_33, %aten_mul_tensor_137), kwargs = {})\n",
      "      %aten_mul_tensor_138 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_35, %aten_add_tensor_35), kwargs = {})\n",
      "      %aten_mean_dim_12 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mean.dim](args = (%aten_mul_tensor_138, [-1], True), kwargs = {})\n",
      "      %aten_add_tensor_36 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.add.Tensor](args = (%aten_mean_dim_12, %_lifted_tensor_constant112), kwargs = {})\n",
      "      %aten_rsqrt_default_12 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.rsqrt.default](args = (%aten_add_tensor_36,), kwargs = {})\n",
      "      %aten_mul_tensor_139 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_add_tensor_35, %aten_rsqrt_default_12), kwargs = {})\n",
      "      %aten_mul_tensor_140 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mul_tensor_139, %p_norm_weight), kwargs = {})\n",
      "      %aten_squeeze_copy_dims_60 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.squeeze_copy.dims](args = (%aten_mul_tensor_140, [0]), kwargs = {})\n",
      "      %aten_mm_default_42 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mm.default](args = (%aten_squeeze_copy_dims_60, %aten_permute_copy_default_42), kwargs = {})\n",
      "      %aten_mul_tensor_141 : [num_users=1] = call_function[target=executorch.exir.dialects.edge._ops.aten.mul.Tensor](args = (%aten_mm_default_42, %b_output_scales), kwargs = {})\n",
      "      return (aten_mul_tensor_141,)\n",
      "  %executorch_call_delegate_6 : [num_users=1] = call_function[target=torch.ops.higher_order.executorch_call_delegate](args = (%lowered_module_6, %sdpa_with_kv_cache_5, %getitem_22), kwargs = {})\n",
      "  %getitem_26 : [num_users=1] = call_function[target=operator.getitem](args = (%executorch_call_delegate_6, 0), kwargs = {})\n",
      "  return (getitem_26,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from executorch.exir.backend.utils import print_delegated_graph\n",
    "\n",
    "print_delegated_graph(builder.export_program.exported_program(\"forward\").graph_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2024-05-31 13:39:36,765 utils.py:114] Saved exported program to stories15M_int8_mps_llama_cpp.pte\n"
     ]
    }
   ],
   "source": [
    "builder.save_to_pte(\"stories15M_int8_mps_llama_cpp.pte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2024-05-31 17:00:06,644 utils.py:114] Saved exported program to stories15M_int8_mps_sdpa.pte\n"
     ]
    }
   ],
   "source": [
    "builder.save_to_pte(\"stories15M_int8_mps_sdpa.pte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2024-05-31 13:32:31,174 utils.py:114] Saved exported program to stories15M_int8_llama_cpp.pte\n"
     ]
    }
   ],
   "source": [
    "builder.save_to_pte(\"stories15M_int8_llama_cpp.pte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from executorch.extension.pybindings.portable_lib import _get_operator_names\n",
    "\n",
    "names = _get_operator_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aten::sym_size.int\n",
      "aten::_local_scalar_dense\n",
      "aten::sym_numel\n",
      "executorch_prim::add.Scalar\n",
      "executorch_prim::sub.Scalar\n",
      "executorch_prim::mul.Scalar\n",
      "executorch_prim::floordiv.Scalar\n",
      "executorch_prim::truediv.Scalar\n",
      "executorch_prim::eq.Scalar\n",
      "executorch_prim::gt.Scalar\n",
      "executorch_prim::lt.Scalar\n",
      "executorch_prim::ge.Scalar\n",
      "executorch_prim::le.Scalar\n",
      "executorch_prim::floordiv.int\n",
      "executorch_prim::et_copy_index.tensor\n",
      "executorch_prim::et_view.default\n",
      "aten::_cdist_forward.out\n",
      "aten::_log_softmax.out\n",
      "aten::_native_batch_norm_legit.out\n",
      "aten::_native_batch_norm_legit.no_stats_out\n",
      "aten::_native_batch_norm_legit_no_training.out\n",
      "aten::_pdist_forward.out\n",
      "aten::_softmax.out\n",
      "aten::_to_copy.out\n",
      "aten::abs.out\n",
      "aten::acos.out\n",
      "aten::acosh.out\n",
      "aten::add.out\n",
      "aten::add.Scalar_out\n",
      "aten::addmm.out\n",
      "aten::alias_copy.out\n",
      "aten::amax.out\n",
      "aten::amin.out\n",
      "aten::any.all_out\n",
      "aten::any.dims_out\n",
      "aten::any.out\n",
      "aten::arange.out\n",
      "aten::arange.start_out\n",
      "aten::argmax.out\n",
      "aten::argmin.out\n",
      "aten::as_strided_copy.out\n",
      "aten::asin.out\n",
      "aten::asinh.out\n",
      "aten::atan.out\n",
      "aten::atan2.out\n",
      "aten::atanh.out\n",
      "aten::avg_pool2d.out\n",
      "aten::bitwise_and.Scalar_out\n",
      "aten::bitwise_and.Tensor_out\n",
      "aten::bitwise_not.out\n",
      "aten::bitwise_or.Scalar_out\n",
      "aten::bitwise_or.Tensor_out\n",
      "aten::bitwise_xor.Scalar_out\n",
      "aten::bitwise_xor.Tensor_out\n",
      "aten::bmm.out\n",
      "aten::cat.out\n",
      "aten::ceil.out\n",
      "aten::clamp.out\n",
      "aten::clamp.Tensor_out\n",
      "aten::clone.out\n",
      "aten::constant_pad_nd.out\n",
      "aten::convolution.out\n",
      "aten::copy.out\n",
      "aten::copy_\n",
      "aten::cos.out\n",
      "aten::cosh.out\n",
      "aten::cumsum.out\n",
      "aten::detach_copy.out\n",
      "aten::diagonal_copy.out\n",
      "aten::div.out\n",
      "aten::div.Scalar_mode_out\n",
      "aten::div.Scalar_out\n",
      "aten::div.out_mode\n",
      "aten::embedding.out\n",
      "aten::empty.out\n",
      "aten::eq.Scalar_out\n",
      "aten::eq.Tensor_out\n",
      "aten::erf.out\n",
      "aten::exp.out\n",
      "aten::expand_copy.out\n",
      "aten::expm1.out\n",
      "aten::fill.Scalar_out\n",
      "aten::fill.Tensor_out\n",
      "aten::flip.out\n",
      "aten::floor.out\n",
      "aten::floor_divide.out\n",
      "aten::fmod.Tensor_out\n",
      "aten::fmod.Scalar_out\n",
      "aten::full.out\n",
      "aten::full_like.out\n",
      "aten::ge.Scalar_out\n",
      "aten::ge.Tensor_out\n",
      "aten::gelu.out\n",
      "aten::glu.out\n",
      "aten::gt.Scalar_out\n",
      "aten::gt.Tensor_out\n",
      "aten::hardtanh.out\n",
      "aten::index.Tensor_out\n",
      "aten::index_put.out\n",
      "aten::index_select.out\n",
      "aten::isinf.out\n",
      "aten::isnan.out\n",
      "aten::le.Scalar_out\n",
      "aten::le.Tensor_out\n",
      "aten::leaky_relu.out\n",
      "aten::lift_fresh_copy.out\n",
      "aten::log.out\n",
      "aten::log10.out\n",
      "aten::log1p.out\n",
      "aten::log2.out\n",
      "aten::logical_and.out\n",
      "aten::logical_not.out\n",
      "aten::logical_or.out\n",
      "aten::logical_xor.out\n",
      "aten::logit.out\n",
      "aten::lt.Scalar_out\n",
      "aten::lt.Tensor_out\n",
      "aten::masked_fill.Scalar_out\n",
      "aten::max.dim_max\n",
      "aten::maximum.out\n",
      "aten::max_pool2d_with_indices.out\n",
      "aten::mean.out\n",
      "aten::min.dim_min\n",
      "aten::minimum.out\n",
      "aten::mm.out\n",
      "aten::mul.out\n",
      "aten::mul.Scalar_out\n",
      "aten::native_group_norm.out\n",
      "aten::native_layer_norm.out\n",
      "aten::ne.Scalar_out\n",
      "aten::ne.Tensor_out\n",
      "aten::neg.out\n",
      "aten::nonzero.out\n",
      "aten::ones.out\n",
      "aten::permute_copy.out\n",
      "aten::pixel_shuffle.out\n",
      "aten::pow.Scalar_out\n",
      "aten::pow.Tensor_Scalar_out\n",
      "aten::pow.Tensor_Tensor_out\n",
      "aten::prod.int_out\n",
      "aten::prod.out\n",
      "aten::reciprocal.out\n",
      "aten::relu.out\n",
      "aten::remainder.Tensor_out\n",
      "aten::remainder.Scalar_out\n",
      "aten::repeat.out\n",
      "aten::reflection_pad1d.out\n",
      "aten::reflection_pad2d.out\n",
      "aten::reflection_pad3d.out\n",
      "aten::replication_pad1d.out\n",
      "aten::replication_pad2d.out\n",
      "aten::replication_pad3d.out\n",
      "aten::roll.out\n",
      "aten::round.out\n",
      "aten::rsqrt.out\n",
      "aten::rsub.Scalar_out\n",
      "aten::scalar_tensor.out\n",
      "aten::scatter_add.out\n",
      "aten::select_copy.int_out\n",
      "aten::select_scatter.out\n",
      "aten::sigmoid.out\n",
      "aten::sign.out\n",
      "aten::sin.out\n",
      "aten::sinh.out\n",
      "aten::slice_copy.Tensor_out\n",
      "aten::slice_scatter.out\n",
      "aten::split_copy.Tensor_out\n",
      "aten::split_with_sizes_copy.out\n",
      "aten::sqrt.out\n",
      "aten::squeeze_copy.dim_out\n",
      "aten::squeeze_copy.dims_out\n",
      "aten::stack.out\n",
      "aten::sub.out\n",
      "aten::sub.Scalar_out\n",
      "aten::sum.IntList_out\n",
      "aten::t_copy.out\n",
      "aten::tan.out\n",
      "aten::tanh.out\n",
      "aten::transpose_copy.int_out\n",
      "aten::tril.out\n",
      "aten::trunc.out\n",
      "aten::unbind_copy.int_out\n",
      "aten::unsqueeze_copy.out\n",
      "aten::var.correction_out\n",
      "aten::var.out\n",
      "aten::view_copy.out\n",
      "aten::where.self_out\n",
      "aten::zeros.out\n",
      "dim_order_ops::_to_dim_order_copy.out\n",
      "quantized_decomposed::add.out\n",
      "quantized_decomposed::choose_qparams.Tensor_out\n",
      "quantized_decomposed::dequantize_per_tensor.out\n",
      "quantized_decomposed::dequantize_per_tensor.Tensor_out\n",
      "quantized_decomposed::quantize_per_channel.out\n",
      "quantized_decomposed::dequantize_per_channel.out\n",
      "quantized_decomposed::embedding_byte.out\n",
      "quantized_decomposed::embedding_byte.dtype_out\n",
      "quantized_decomposed::embedding_4bit.out\n",
      "quantized_decomposed::embedding_4bit.dtype_out\n",
      "quantized_decomposed::mixed_mm.out\n",
      "quantized_decomposed::mixed_linear.out\n",
      "quantized_decomposed::quantize_per_tensor.out\n",
      "quantized_decomposed::quantize_per_tensor.Tensor_out\n",
      "llama::sdpa_with_kv_cache.out\n",
      "llama_cpp::_weight_int8pack_mm.out\n",
      "205\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(names))\n",
    "print(len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2024-05-08 14:39:40,251 utils.py:112] Saved exported program to stories110M_int8.pte\n"
     ]
    }
   ],
   "source": [
    "builder.save_to_pte(\"stories110M_int8.pte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2024-05-08 17:58:18,813 utils.py:113] Saved exported program to stories110M_int8_mps.pte\n"
     ]
    }
   ],
   "source": [
    "builder.save_to_pte(\"stories110M_int8_mps.pte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from executorch.extension.pybindings.portable_lib import _load_for_executorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "op1 = torch.ops.aten._weight_int8pack_mm.default\n",
    "op2 = torch.ops.llama_cpp._weight_int8pack_mm.default\n",
    "mps_device = torch.device(\"mps\")  # Device object representing GPU.\n",
    "\n",
    "A = torch.randn(4, 8, dtype=torch.float, device=mps_device)\n",
    "B = torch.ones(8, 8, dtype=torch.int8, device=mps_device)\n",
    "scales = torch.randn(8, dtype=torch.float, device=mps_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2965,  0.1211, -0.0488, -0.0738, -0.5035,  0.7861,  0.9290,  0.6385],\n",
      "        [ 2.7348,  1.1167, -0.4501, -0.6806, -4.6442,  7.2503,  8.5685,  5.8890],\n",
      "        [-0.0908, -0.0371,  0.0149,  0.0226,  0.1542, -0.2408, -0.2845, -0.1956],\n",
      "        [-2.2508, -0.9191,  0.3705,  0.5601,  3.8223, -5.9672, -7.0521, -4.8468]],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "C1 = op1(A, B, scales)\n",
    "print(C1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2965,  0.1211, -0.0488, -0.0738, -0.5035,  0.7861,  0.9290,  0.6385],\n",
      "        [ 2.7348,  1.1167, -0.4501, -0.6806, -4.6442,  7.2503,  8.5685,  5.8890],\n",
      "        [-0.0908, -0.0371,  0.0149,  0.0226,  0.1542, -0.2408, -0.2845, -0.1956],\n",
      "        [-2.2508, -0.9191,  0.3705,  0.5601,  3.8223, -5.9672, -7.0521, -4.8468]],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "C2 = op2(A, B, scales)\n",
    "print(C2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(C1, C2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentencepiece import SentencePieceProcessor as SPP\n",
    "\n",
    "sp_model = SPP(model_file=\"tokenizer.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 9038, 2501, 263, 931]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Once upon a time\"\n",
    "\n",
    "prompt_tokens = sp_model.encode(prompt)\n",
    "prompt_tokens = [sp_model.bos_id()] + prompt_tokens\n",
    "print(prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "t = torch.tensor([[prompt_tokens[0]]], dtype=torch.int64)\n",
    "pos = torch.tensor([0], dtype=torch.int64)\n",
    "\n",
    "model.model(t, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge module\n",
    "builder = model.export_to_edge()\n",
    "edge_module = builder.edge_manager._edge_programs[\"forward\"].module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom op module\n",
    "builder.edge_manager = builder.edge_manager.transform([ReplaceMMPass()])\n",
    "custom_op_module = builder.edge_manager._edge_programs[\"forward\"].module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING 2024-05-29 15:00:15,232 xnnpack_partitioner.py:558] Nothing can be partitioned!\n",
      "/Users/larryliu/miniconda3/envs/executorch/lib/python3.11/site-packages/executorch/exir/emit/_emitter.py:1474: UserWarning: Mutation on a buffer in the model is detected. ExecuTorch assumes buffers that are mutated in the graph have a meaningless initial state, only the shape and dtype will be serialized.\n",
      "  warnings.warn(\n",
      "[INFO 2024-05-29 15:00:17,022 builder.py:375] Required memory for activation in bytes: [0, 4587264]\n",
      "[program.cpp:130] InternalConsistency verification requested but not available\n"
     ]
    }
   ],
   "source": [
    "# try xnnpack\n",
    "from executorch.examples.models.llama2.lib.partitioner_lib import get_xnnpack_partitioner\n",
    "builder.to_backend([get_xnnpack_partitioner()]).to_executorch()\n",
    "\n",
    "from executorch.extension.pybindings._portable_lib import _load_for_executorch_from_buffer\n",
    "\n",
    "ep = _load_for_executorch_from_buffer(builder.export_program.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2024-05-31 15:10:47,125 mps_partitioner.py:121] Found 31 subgraphs to be partitioned.\n",
      "[INFO 2024-05-31 15:10:47,158 mps_preprocess.py:115] Visiting: aten_sigmoid_default_5, aten.sigmoid.default\n",
      "[INFO 2024-05-31 15:10:47,159 mps_preprocess.py:115] Visiting: aten_mul_tensor_94, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:47,159 mps_preprocess.py:115] Visiting: aten_mul_tensor_95, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:47,301 mps_preprocess.py:115] Visiting: aten_add_tensor_33, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:47,302 mps_preprocess.py:115] Visiting: aten_mul_tensor_91, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:47,302 mps_preprocess.py:115] Visiting: aten_mean_dim_11, aten.mean.dim\n",
      "[INFO 2024-05-31 15:10:47,303 mps_preprocess.py:115] Visiting: aten_add_tensor_34, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:47,303 mps_preprocess.py:115] Visiting: aten_rsqrt_default_11, aten.rsqrt.default\n",
      "[INFO 2024-05-31 15:10:47,303 mps_preprocess.py:115] Visiting: aten_mul_tensor_92, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:47,303 mps_preprocess.py:115] Visiting: aten_mul_tensor_93, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:47,303 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_58, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:47,304 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_59, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:47,512 mps_preprocess.py:115] Visiting: aten_view_copy_default_59, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:47,513 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_57, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:47,599 mps_preprocess.py:115] Visiting: aten_view_copy_default_55, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:47,600 mps_preprocess.py:115] Visiting: aten_view_copy_default_56, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:47,600 mps_preprocess.py:115] Visiting: aten_view_copy_default_50, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:47,600 mps_preprocess.py:115] Visiting: aten_view_copy_default_51, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:47,600 mps_preprocess.py:115] Visiting: aten_view_copy_default_52, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:47,601 mps_preprocess.py:115] Visiting: aten_view_copy_default_53, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:47,601 mps_preprocess.py:115] Visiting: aten_view_copy_default_54, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:47,601 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_20, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 15:10:47,601 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_21, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 15:10:47,602 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_22, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 15:10:47,602 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_23, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 15:10:47,602 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_53, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:47,602 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_54, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:47,602 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_55, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:47,603 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_56, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:47,603 mps_preprocess.py:115] Visiting: aten_mul_tensor_83, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:47,603 mps_preprocess.py:115] Visiting: aten_mul_tensor_85, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:47,603 mps_preprocess.py:115] Visiting: aten_mul_tensor_84, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:47,603 mps_preprocess.py:115] Visiting: aten_mul_tensor_86, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:47,603 mps_preprocess.py:115] Visiting: aten_mul_tensor_87, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:47,604 mps_preprocess.py:115] Visiting: aten_mul_tensor_89, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:47,604 mps_preprocess.py:115] Visiting: aten_mul_tensor_88, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:47,604 mps_preprocess.py:115] Visiting: aten_mul_tensor_90, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:47,604 mps_preprocess.py:115] Visiting: aten_sub_tensor_10, aten.sub.Tensor\n",
      "[INFO 2024-05-31 15:10:47,604 mps_preprocess.py:115] Visiting: aten_add_tensor_31, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:47,605 mps_preprocess.py:115] Visiting: aten_sub_tensor_11, aten.sub.Tensor\n",
      "[INFO 2024-05-31 15:10:47,605 mps_preprocess.py:115] Visiting: aten_add_tensor_32, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:47,605 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_20, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 15:10:47,605 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_21, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 15:10:47,605 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_22, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 15:10:47,606 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_23, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 15:10:47,606 mps_preprocess.py:115] Visiting: aten_cat_default_10, aten.cat.default\n",
      "[INFO 2024-05-31 15:10:47,606 mps_preprocess.py:115] Visiting: aten_cat_default_11, aten.cat.default\n",
      "[INFO 2024-05-31 15:10:47,606 mps_preprocess.py:115] Visiting: aten_view_copy_default_57, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:47,607 mps_preprocess.py:115] Visiting: aten_view_copy_default_58, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:47,701 mps_preprocess.py:115] Visiting: aten_add_tensor_29, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:47,702 mps_preprocess.py:115] Visiting: aten_mul_tensor_80, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:47,702 mps_preprocess.py:115] Visiting: aten_mean_dim_10, aten.mean.dim\n",
      "[INFO 2024-05-31 15:10:47,702 mps_preprocess.py:115] Visiting: aten_add_tensor_30, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:47,702 mps_preprocess.py:115] Visiting: aten_rsqrt_default_10, aten.rsqrt.default\n",
      "[INFO 2024-05-31 15:10:47,703 mps_preprocess.py:115] Visiting: aten_mul_tensor_81, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:47,703 mps_preprocess.py:115] Visiting: aten_mul_tensor_82, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:47,703 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_50, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:47,703 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_51, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:47,703 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_52, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:47,784 mps_preprocess.py:115] Visiting: aten_sigmoid_default_4, aten.sigmoid.default\n",
      "[INFO 2024-05-31 15:10:47,785 mps_preprocess.py:115] Visiting: aten_mul_tensor_78, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:47,785 mps_preprocess.py:115] Visiting: aten_mul_tensor_79, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:47,865 mps_preprocess.py:115] Visiting: aten_add_tensor_27, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:47,866 mps_preprocess.py:115] Visiting: aten_mul_tensor_75, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:47,866 mps_preprocess.py:115] Visiting: aten_mean_dim_9, aten.mean.dim\n",
      "[INFO 2024-05-31 15:10:47,866 mps_preprocess.py:115] Visiting: aten_add_tensor_28, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:47,867 mps_preprocess.py:115] Visiting: aten_rsqrt_default_9, aten.rsqrt.default\n",
      "[INFO 2024-05-31 15:10:47,867 mps_preprocess.py:115] Visiting: aten_mul_tensor_76, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:47,867 mps_preprocess.py:115] Visiting: aten_mul_tensor_77, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:47,867 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_48, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:47,868 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_49, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:47,946 mps_preprocess.py:115] Visiting: aten_view_copy_default_49, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:47,947 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_47, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:48,032 mps_preprocess.py:115] Visiting: aten_view_copy_default_45, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:48,033 mps_preprocess.py:115] Visiting: aten_view_copy_default_46, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:48,033 mps_preprocess.py:115] Visiting: aten_view_copy_default_40, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:48,034 mps_preprocess.py:115] Visiting: aten_view_copy_default_41, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:48,034 mps_preprocess.py:115] Visiting: aten_view_copy_default_42, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:48,034 mps_preprocess.py:115] Visiting: aten_view_copy_default_43, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:48,034 mps_preprocess.py:115] Visiting: aten_view_copy_default_44, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:48,034 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_16, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 15:10:48,035 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_17, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 15:10:48,035 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_18, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 15:10:48,035 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_19, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 15:10:48,035 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_43, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:48,035 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_44, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:48,035 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_45, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:48,036 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_46, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:48,036 mps_preprocess.py:115] Visiting: aten_mul_tensor_67, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,036 mps_preprocess.py:115] Visiting: aten_mul_tensor_69, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,036 mps_preprocess.py:115] Visiting: aten_mul_tensor_68, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,036 mps_preprocess.py:115] Visiting: aten_mul_tensor_70, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,037 mps_preprocess.py:115] Visiting: aten_mul_tensor_71, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,037 mps_preprocess.py:115] Visiting: aten_mul_tensor_73, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,037 mps_preprocess.py:115] Visiting: aten_mul_tensor_72, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,037 mps_preprocess.py:115] Visiting: aten_mul_tensor_74, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,038 mps_preprocess.py:115] Visiting: aten_sub_tensor_8, aten.sub.Tensor\n",
      "[INFO 2024-05-31 15:10:48,038 mps_preprocess.py:115] Visiting: aten_add_tensor_25, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:48,038 mps_preprocess.py:115] Visiting: aten_sub_tensor_9, aten.sub.Tensor\n",
      "[INFO 2024-05-31 15:10:48,038 mps_preprocess.py:115] Visiting: aten_add_tensor_26, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:48,038 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_16, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 15:10:48,039 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_17, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 15:10:48,039 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_18, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 15:10:48,039 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_19, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 15:10:48,039 mps_preprocess.py:115] Visiting: aten_cat_default_8, aten.cat.default\n",
      "[INFO 2024-05-31 15:10:48,039 mps_preprocess.py:115] Visiting: aten_cat_default_9, aten.cat.default\n",
      "[INFO 2024-05-31 15:10:48,040 mps_preprocess.py:115] Visiting: aten_view_copy_default_47, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:48,040 mps_preprocess.py:115] Visiting: aten_view_copy_default_48, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:48,126 mps_preprocess.py:115] Visiting: aten_add_tensor_23, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:48,127 mps_preprocess.py:115] Visiting: aten_mul_tensor_64, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,127 mps_preprocess.py:115] Visiting: aten_mean_dim_8, aten.mean.dim\n",
      "[INFO 2024-05-31 15:10:48,128 mps_preprocess.py:115] Visiting: aten_add_tensor_24, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:48,128 mps_preprocess.py:115] Visiting: aten_rsqrt_default_8, aten.rsqrt.default\n",
      "[INFO 2024-05-31 15:10:48,128 mps_preprocess.py:115] Visiting: aten_mul_tensor_65, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,128 mps_preprocess.py:115] Visiting: aten_mul_tensor_66, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,128 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_40, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:48,129 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_41, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:48,129 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_42, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:48,208 mps_preprocess.py:115] Visiting: aten_sigmoid_default_3, aten.sigmoid.default\n",
      "[INFO 2024-05-31 15:10:48,209 mps_preprocess.py:115] Visiting: aten_mul_tensor_62, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,209 mps_preprocess.py:115] Visiting: aten_mul_tensor_63, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,285 mps_preprocess.py:115] Visiting: aten_add_tensor_21, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:48,286 mps_preprocess.py:115] Visiting: aten_mul_tensor_59, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,286 mps_preprocess.py:115] Visiting: aten_mean_dim_7, aten.mean.dim\n",
      "[INFO 2024-05-31 15:10:48,286 mps_preprocess.py:115] Visiting: aten_add_tensor_22, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:48,286 mps_preprocess.py:115] Visiting: aten_rsqrt_default_7, aten.rsqrt.default\n",
      "[INFO 2024-05-31 15:10:48,287 mps_preprocess.py:115] Visiting: aten_mul_tensor_60, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,287 mps_preprocess.py:115] Visiting: aten_mul_tensor_61, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,287 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_38, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:48,288 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_39, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:48,367 mps_preprocess.py:115] Visiting: aten_view_copy_default_39, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:48,368 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_37, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:48,449 mps_preprocess.py:115] Visiting: aten_view_copy_default_35, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:48,450 mps_preprocess.py:115] Visiting: aten_view_copy_default_36, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:48,450 mps_preprocess.py:115] Visiting: aten_view_copy_default_30, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:48,450 mps_preprocess.py:115] Visiting: aten_view_copy_default_31, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:48,450 mps_preprocess.py:115] Visiting: aten_view_copy_default_32, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:48,451 mps_preprocess.py:115] Visiting: aten_view_copy_default_33, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:48,451 mps_preprocess.py:115] Visiting: aten_view_copy_default_34, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:48,451 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_12, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 15:10:48,451 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_13, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 15:10:48,451 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_14, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 15:10:48,452 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_15, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 15:10:48,452 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_33, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:48,452 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_34, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:48,452 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_35, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:48,453 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_36, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:48,453 mps_preprocess.py:115] Visiting: aten_mul_tensor_51, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,453 mps_preprocess.py:115] Visiting: aten_mul_tensor_53, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,453 mps_preprocess.py:115] Visiting: aten_mul_tensor_52, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,453 mps_preprocess.py:115] Visiting: aten_mul_tensor_54, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,453 mps_preprocess.py:115] Visiting: aten_mul_tensor_55, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,454 mps_preprocess.py:115] Visiting: aten_mul_tensor_57, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,454 mps_preprocess.py:115] Visiting: aten_mul_tensor_56, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,454 mps_preprocess.py:115] Visiting: aten_mul_tensor_58, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,454 mps_preprocess.py:115] Visiting: aten_sub_tensor_6, aten.sub.Tensor\n",
      "[INFO 2024-05-31 15:10:48,454 mps_preprocess.py:115] Visiting: aten_add_tensor_19, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:48,455 mps_preprocess.py:115] Visiting: aten_sub_tensor_7, aten.sub.Tensor\n",
      "[INFO 2024-05-31 15:10:48,455 mps_preprocess.py:115] Visiting: aten_add_tensor_20, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:48,455 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_12, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 15:10:48,455 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_13, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 15:10:48,455 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_14, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 15:10:48,456 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_15, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 15:10:48,456 mps_preprocess.py:115] Visiting: aten_cat_default_6, aten.cat.default\n",
      "[INFO 2024-05-31 15:10:48,456 mps_preprocess.py:115] Visiting: aten_cat_default_7, aten.cat.default\n",
      "[INFO 2024-05-31 15:10:48,456 mps_preprocess.py:115] Visiting: aten_view_copy_default_37, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:48,456 mps_preprocess.py:115] Visiting: aten_view_copy_default_38, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:48,553 mps_preprocess.py:115] Visiting: aten_add_tensor_17, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:48,554 mps_preprocess.py:115] Visiting: aten_mul_tensor_48, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,554 mps_preprocess.py:115] Visiting: aten_mean_dim_6, aten.mean.dim\n",
      "[INFO 2024-05-31 15:10:48,555 mps_preprocess.py:115] Visiting: aten_add_tensor_18, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:48,555 mps_preprocess.py:115] Visiting: aten_rsqrt_default_6, aten.rsqrt.default\n",
      "[INFO 2024-05-31 15:10:48,555 mps_preprocess.py:115] Visiting: aten_mul_tensor_49, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,555 mps_preprocess.py:115] Visiting: aten_mul_tensor_50, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,555 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_30, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:48,556 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_31, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:48,556 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_32, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:48,633 mps_preprocess.py:115] Visiting: aten_sigmoid_default_2, aten.sigmoid.default\n",
      "[INFO 2024-05-31 15:10:48,634 mps_preprocess.py:115] Visiting: aten_mul_tensor_46, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,634 mps_preprocess.py:115] Visiting: aten_mul_tensor_47, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,709 mps_preprocess.py:115] Visiting: aten_add_tensor_15, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:48,710 mps_preprocess.py:115] Visiting: aten_mul_tensor_43, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,710 mps_preprocess.py:115] Visiting: aten_mean_dim_5, aten.mean.dim\n",
      "[INFO 2024-05-31 15:10:48,710 mps_preprocess.py:115] Visiting: aten_add_tensor_16, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:48,711 mps_preprocess.py:115] Visiting: aten_rsqrt_default_5, aten.rsqrt.default\n",
      "[INFO 2024-05-31 15:10:48,711 mps_preprocess.py:115] Visiting: aten_mul_tensor_44, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,711 mps_preprocess.py:115] Visiting: aten_mul_tensor_45, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,711 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_28, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:48,712 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_29, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:48,787 mps_preprocess.py:115] Visiting: aten_view_copy_default_29, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:48,788 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_27, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:48,866 mps_preprocess.py:115] Visiting: aten_view_copy_default_25, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:48,867 mps_preprocess.py:115] Visiting: aten_view_copy_default_26, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:48,867 mps_preprocess.py:115] Visiting: aten_view_copy_default_20, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:48,867 mps_preprocess.py:115] Visiting: aten_view_copy_default_21, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:48,867 mps_preprocess.py:115] Visiting: aten_view_copy_default_22, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:48,868 mps_preprocess.py:115] Visiting: aten_view_copy_default_23, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:48,868 mps_preprocess.py:115] Visiting: aten_view_copy_default_24, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:48,868 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_8, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 15:10:48,868 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_9, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 15:10:48,868 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_10, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 15:10:48,869 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_11, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 15:10:48,869 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_23, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:48,869 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_24, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:48,869 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_25, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:48,869 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_26, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:48,870 mps_preprocess.py:115] Visiting: aten_mul_tensor_35, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,870 mps_preprocess.py:115] Visiting: aten_mul_tensor_37, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,870 mps_preprocess.py:115] Visiting: aten_mul_tensor_36, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,870 mps_preprocess.py:115] Visiting: aten_mul_tensor_38, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,870 mps_preprocess.py:115] Visiting: aten_mul_tensor_39, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,871 mps_preprocess.py:115] Visiting: aten_mul_tensor_41, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,871 mps_preprocess.py:115] Visiting: aten_mul_tensor_40, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,871 mps_preprocess.py:115] Visiting: aten_mul_tensor_42, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,871 mps_preprocess.py:115] Visiting: aten_sub_tensor_4, aten.sub.Tensor\n",
      "[INFO 2024-05-31 15:10:48,871 mps_preprocess.py:115] Visiting: aten_add_tensor_13, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:48,872 mps_preprocess.py:115] Visiting: aten_sub_tensor_5, aten.sub.Tensor\n",
      "[INFO 2024-05-31 15:10:48,872 mps_preprocess.py:115] Visiting: aten_add_tensor_14, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:48,872 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_8, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 15:10:48,872 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_9, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 15:10:48,872 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_10, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 15:10:48,872 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_11, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 15:10:48,873 mps_preprocess.py:115] Visiting: aten_cat_default_4, aten.cat.default\n",
      "[INFO 2024-05-31 15:10:48,873 mps_preprocess.py:115] Visiting: aten_cat_default_5, aten.cat.default\n",
      "[INFO 2024-05-31 15:10:48,873 mps_preprocess.py:115] Visiting: aten_view_copy_default_27, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:48,873 mps_preprocess.py:115] Visiting: aten_view_copy_default_28, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:48,957 mps_preprocess.py:115] Visiting: aten_add_tensor_11, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:48,958 mps_preprocess.py:115] Visiting: aten_mul_tensor_32, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,958 mps_preprocess.py:115] Visiting: aten_mean_dim_4, aten.mean.dim\n",
      "[INFO 2024-05-31 15:10:48,959 mps_preprocess.py:115] Visiting: aten_add_tensor_12, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:48,959 mps_preprocess.py:115] Visiting: aten_rsqrt_default_4, aten.rsqrt.default\n",
      "[INFO 2024-05-31 15:10:48,959 mps_preprocess.py:115] Visiting: aten_mul_tensor_33, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,959 mps_preprocess.py:115] Visiting: aten_mul_tensor_34, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:48,960 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_20, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:48,960 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_21, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:48,960 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_22, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:49,036 mps_preprocess.py:115] Visiting: aten_sigmoid_default_1, aten.sigmoid.default\n",
      "[INFO 2024-05-31 15:10:49,037 mps_preprocess.py:115] Visiting: aten_mul_tensor_30, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,037 mps_preprocess.py:115] Visiting: aten_mul_tensor_31, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,110 mps_preprocess.py:115] Visiting: aten_add_tensor_9, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:49,110 mps_preprocess.py:115] Visiting: aten_mul_tensor_27, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,111 mps_preprocess.py:115] Visiting: aten_mean_dim_3, aten.mean.dim\n",
      "[INFO 2024-05-31 15:10:49,111 mps_preprocess.py:115] Visiting: aten_add_tensor_10, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:49,111 mps_preprocess.py:115] Visiting: aten_rsqrt_default_3, aten.rsqrt.default\n",
      "[INFO 2024-05-31 15:10:49,111 mps_preprocess.py:115] Visiting: aten_mul_tensor_28, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,112 mps_preprocess.py:115] Visiting: aten_mul_tensor_29, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,112 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_18, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:49,112 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_19, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:49,187 mps_preprocess.py:115] Visiting: aten_view_copy_default_19, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:49,187 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_17, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:49,265 mps_preprocess.py:115] Visiting: aten_view_copy_default_15, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:49,266 mps_preprocess.py:115] Visiting: aten_view_copy_default_16, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:49,266 mps_preprocess.py:115] Visiting: aten_view_copy_default_10, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:49,266 mps_preprocess.py:115] Visiting: aten_view_copy_default_11, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:49,266 mps_preprocess.py:115] Visiting: aten_view_copy_default_12, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:49,267 mps_preprocess.py:115] Visiting: aten_view_copy_default_13, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:49,267 mps_preprocess.py:115] Visiting: aten_view_copy_default_14, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:49,267 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_4, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 15:10:49,267 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_5, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 15:10:49,267 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_6, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 15:10:49,268 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_7, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 15:10:49,268 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_13, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:49,268 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_14, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:49,268 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_15, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:49,269 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_16, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:49,269 mps_preprocess.py:115] Visiting: aten_mul_tensor_19, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,269 mps_preprocess.py:115] Visiting: aten_mul_tensor_21, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,269 mps_preprocess.py:115] Visiting: aten_mul_tensor_20, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,269 mps_preprocess.py:115] Visiting: aten_mul_tensor_22, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,270 mps_preprocess.py:115] Visiting: aten_mul_tensor_23, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,270 mps_preprocess.py:115] Visiting: aten_mul_tensor_25, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,270 mps_preprocess.py:115] Visiting: aten_mul_tensor_24, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,270 mps_preprocess.py:115] Visiting: aten_mul_tensor_26, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,271 mps_preprocess.py:115] Visiting: aten_sub_tensor_2, aten.sub.Tensor\n",
      "[INFO 2024-05-31 15:10:49,271 mps_preprocess.py:115] Visiting: aten_add_tensor_7, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:49,271 mps_preprocess.py:115] Visiting: aten_sub_tensor_3, aten.sub.Tensor\n",
      "[INFO 2024-05-31 15:10:49,271 mps_preprocess.py:115] Visiting: aten_add_tensor_8, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:49,272 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_4, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 15:10:49,272 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_5, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 15:10:49,272 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_6, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 15:10:49,272 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_7, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 15:10:49,272 mps_preprocess.py:115] Visiting: aten_cat_default_2, aten.cat.default\n",
      "[INFO 2024-05-31 15:10:49,273 mps_preprocess.py:115] Visiting: aten_cat_default_3, aten.cat.default\n",
      "[INFO 2024-05-31 15:10:49,273 mps_preprocess.py:115] Visiting: aten_view_copy_default_17, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:49,273 mps_preprocess.py:115] Visiting: aten_view_copy_default_18, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:49,353 mps_preprocess.py:115] Visiting: aten_add_tensor_5, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:49,354 mps_preprocess.py:115] Visiting: aten_mul_tensor_16, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,354 mps_preprocess.py:115] Visiting: aten_mean_dim_2, aten.mean.dim\n",
      "[INFO 2024-05-31 15:10:49,355 mps_preprocess.py:115] Visiting: aten_add_tensor_6, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:49,355 mps_preprocess.py:115] Visiting: aten_rsqrt_default_2, aten.rsqrt.default\n",
      "[INFO 2024-05-31 15:10:49,355 mps_preprocess.py:115] Visiting: aten_mul_tensor_17, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,356 mps_preprocess.py:115] Visiting: aten_mul_tensor_18, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,356 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_10, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:49,356 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_11, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:49,357 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_12, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:49,429 mps_preprocess.py:115] Visiting: aten_sigmoid_default, aten.sigmoid.default\n",
      "[INFO 2024-05-31 15:10:49,429 mps_preprocess.py:115] Visiting: aten_mul_tensor_14, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,430 mps_preprocess.py:115] Visiting: aten_mul_tensor_15, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,498 mps_preprocess.py:115] Visiting: aten_add_tensor_3, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:49,499 mps_preprocess.py:115] Visiting: aten_mul_tensor_11, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,499 mps_preprocess.py:115] Visiting: aten_mean_dim_1, aten.mean.dim\n",
      "[INFO 2024-05-31 15:10:49,499 mps_preprocess.py:115] Visiting: aten_add_tensor_4, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:49,500 mps_preprocess.py:115] Visiting: aten_rsqrt_default_1, aten.rsqrt.default\n",
      "[INFO 2024-05-31 15:10:49,500 mps_preprocess.py:115] Visiting: aten_mul_tensor_12, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,500 mps_preprocess.py:115] Visiting: aten_mul_tensor_13, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,500 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_8, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:49,501 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_9, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:49,573 mps_preprocess.py:115] Visiting: aten_add_tensor_35, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:49,574 mps_preprocess.py:115] Visiting: aten_mul_tensor_96, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,574 mps_preprocess.py:115] Visiting: aten_mean_dim_12, aten.mean.dim\n",
      "[INFO 2024-05-31 15:10:49,574 mps_preprocess.py:115] Visiting: aten_add_tensor_36, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:49,574 mps_preprocess.py:115] Visiting: aten_rsqrt_default_12, aten.rsqrt.default\n",
      "[INFO 2024-05-31 15:10:49,575 mps_preprocess.py:115] Visiting: aten_mul_tensor_97, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,575 mps_preprocess.py:115] Visiting: aten_mul_tensor_98, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,575 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_60, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:49,643 mps_preprocess.py:115] Visiting: aten_view_copy_default_9, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:49,643 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_7, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:49,734 mps_preprocess.py:115] Visiting: aten_view_copy_default, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:49,735 mps_preprocess.py:115] Visiting: aten_view_copy_default_1, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:49,735 mps_preprocess.py:115] Visiting: aten_view_copy_default_2, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:49,735 mps_preprocess.py:115] Visiting: aten_index_tensor, aten.index.Tensor\n",
      "[INFO 2024-05-31 15:10:49,735 mps_preprocess.py:115] Visiting: aten_index_tensor_1, aten.index.Tensor\n",
      "[INFO 2024-05-31 15:10:49,735 mps_preprocess.py:115] Visiting: aten_view_copy_default_3, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:49,736 mps_preprocess.py:115] Visiting: aten_view_copy_default_4, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:49,736 mps_preprocess.py:115] Visiting: aten_view_copy_default_5, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:49,736 mps_preprocess.py:115] Visiting: aten_view_copy_default_6, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:49,736 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 15:10:49,736 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_1, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 15:10:49,737 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_2, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 15:10:49,737 mps_preprocess.py:115] Visiting: aten_slice_copy_tensor_3, aten.slice_copy.Tensor\n",
      "[INFO 2024-05-31 15:10:49,737 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_3, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:49,737 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_4, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:49,737 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_5, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:49,737 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_6, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:49,738 mps_preprocess.py:115] Visiting: aten_mul_tensor_3, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,738 mps_preprocess.py:115] Visiting: aten_mul_tensor_5, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,738 mps_preprocess.py:115] Visiting: aten_mul_tensor_4, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,738 mps_preprocess.py:115] Visiting: aten_mul_tensor_6, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,738 mps_preprocess.py:115] Visiting: aten_mul_tensor_7, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,738 mps_preprocess.py:115] Visiting: aten_mul_tensor_9, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,739 mps_preprocess.py:115] Visiting: aten_mul_tensor_8, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,739 mps_preprocess.py:115] Visiting: aten_mul_tensor_10, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,739 mps_preprocess.py:115] Visiting: aten_sub_tensor, aten.sub.Tensor\n",
      "[INFO 2024-05-31 15:10:49,739 mps_preprocess.py:115] Visiting: aten_add_tensor_1, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:49,739 mps_preprocess.py:115] Visiting: aten_sub_tensor_1, aten.sub.Tensor\n",
      "[INFO 2024-05-31 15:10:49,739 mps_preprocess.py:115] Visiting: aten_add_tensor_2, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:49,740 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 15:10:49,740 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_1, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 15:10:49,740 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_2, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 15:10:49,740 mps_preprocess.py:115] Visiting: aten_unsqueeze_copy_default_3, aten.unsqueeze_copy.default\n",
      "[INFO 2024-05-31 15:10:49,740 mps_preprocess.py:115] Visiting: aten_cat_default, aten.cat.default\n",
      "[INFO 2024-05-31 15:10:49,741 mps_preprocess.py:115] Visiting: aten_cat_default_1, aten.cat.default\n",
      "[INFO 2024-05-31 15:10:49,741 mps_preprocess.py:115] Visiting: aten_view_copy_default_7, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:49,741 mps_preprocess.py:115] Visiting: aten_view_copy_default_8, aten.view_copy.default\n",
      "[INFO 2024-05-31 15:10:49,847 mps_preprocess.py:115] Visiting: aten_embedding_default, aten.embedding.default\n",
      "[INFO 2024-05-31 15:10:49,848 mps_preprocess.py:115] Visiting: aten_mul_tensor, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,848 mps_preprocess.py:115] Visiting: aten_mean_dim, aten.mean.dim\n",
      "[INFO 2024-05-31 15:10:49,848 mps_preprocess.py:115] Visiting: aten_add_tensor, aten.add.Tensor\n",
      "[INFO 2024-05-31 15:10:49,849 mps_preprocess.py:115] Visiting: aten_rsqrt_default, aten.rsqrt.default\n",
      "[INFO 2024-05-31 15:10:49,849 mps_preprocess.py:115] Visiting: aten_mul_tensor_1, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,849 mps_preprocess.py:115] Visiting: aten_mul_tensor_2, aten.mul.Tensor\n",
      "[INFO 2024-05-31 15:10:49,849 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:49,850 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_1, aten.squeeze_copy.dims\n",
      "[INFO 2024-05-31 15:10:49,850 mps_preprocess.py:115] Visiting: aten_squeeze_copy_dims_2, aten.squeeze_copy.dims\n"
     ]
    }
   ],
   "source": [
    "# lowered to mps module\n",
    "builder.to_backend(partitioners)\n",
    "mps_module = builder.edge_manager._edge_programs[\"forward\"].module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[program.cpp:130] InternalConsistency verification requested but not available\n",
      "loc(\"mps_broadcast_to\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/91a344b1-f985-11ee-b563-fe8bc7981bff/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":1392:0)): error: 'anec.broadcast' op failed: input cannot be broadcasted to the target shape\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loc(\"mps_broadcast_to\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/91a344b1-f985-11ee-b563-fe8bc7981bff/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":1392:0)): error: 'anec.broadcast' op failed: input cannot be broadcasted to the target shape\n"
     ]
    }
   ],
   "source": [
    "# to executorch\n",
    "builder.to_executorch()\n",
    "from executorch.extension.pybindings._portable_lib import _load_for_executorch_from_buffer\n",
    "\n",
    "ep = _load_for_executorch_from_buffer(builder.export_program.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.8514,  1.4318, -5.8515,  ..., -5.8277, -5.8277, -5.8515]],\n",
      "       grad_fn=<NotImplemented>)\n"
     ]
    }
   ],
   "source": [
    "# eager result\n",
    "print(model.model.forward(t, pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.8514,  1.4318, -5.8515,  ..., -5.8277, -5.8277, -5.8515]],\n",
      "       grad_fn=<NotImplemented>)\n"
     ]
    }
   ],
   "source": [
    "# edge result\n",
    "print(edge_module.forward(t, pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.8514,  1.4318, -5.8515,  ..., -5.8277, -5.8277, -5.8515]],\n",
      "       grad_fn=<NotImplemented>)\n"
     ]
    }
   ],
   "source": [
    "# edge with custom op\n",
    "print(custom_op_module.forward(t, pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.8514,  1.4318, -5.8515,  ..., -5.8277, -5.8277, -5.8515]],\n",
      "       grad_fn=<NotImplemented>)\n"
     ]
    }
   ],
   "source": [
    "# mps result\n",
    "print(mps_module.forward(t, pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-5.8514,  1.4318, -5.8515,  ..., -5.8276, -5.8277, -5.8515]])]\n"
     ]
    }
   ],
   "source": [
    "#ep result\n",
    "print(ep.forward((t, pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<executorch.exir.program._program.ExecutorchProgramManager object at 0x35aba4cd0>\n"
     ]
    }
   ],
   "source": [
    "print(builder.export_program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2024-05-29 15:02:07,059 utils.py:114] Saved exported program to stories15M_int8_xnnpack_llama_cpp.pte\n"
     ]
    }
   ],
   "source": [
    "builder.save_to_pte(\"stories15M_int8_xnnpack_llama_cpp.pte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction(instr_args=KernelCall(op_index=2, args=[221, 235, 245, 245]))\n"
     ]
    }
   ],
   "source": [
    "print(builder.export_program._emitter_output.program.execution_plan[0].chains[0].instructions[35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mps_module = builder.export_program.exported_program(\"forward\").graph_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from executorch.exir.delegate import executorch_call_delegate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(lowered_module_0, tokens)\n",
      "(lowered_module_1, llama_cpp__weight_int8pack_mm_default, llama_cpp__weight_int8pack_mm_default_1, llama_cpp__weight_int8pack_mm_default_2, input_pos)\n",
      "(lowered_module_2, sdpa_with_kv_cache)\n",
      "(lowered_module_3, getitem, llama_cpp__weight_int8pack_mm_default_3)\n",
      "(lowered_module_4, llama_cpp__weight_int8pack_mm_default_4, llama_cpp__weight_int8pack_mm_default_5)\n",
      "(lowered_module_5, getitem_10, llama_cpp__weight_int8pack_mm_default_6)\n",
      "(lowered_module_6, getitem_5, getitem_6, llama_cpp__weight_int8pack_mm_default_7, llama_cpp__weight_int8pack_mm_default_8, llama_cpp__weight_int8pack_mm_default_9)\n",
      "(lowered_module_7, sdpa_with_kv_cache_1)\n",
      "(lowered_module_8, getitem_14, llama_cpp__weight_int8pack_mm_default_10)\n",
      "(lowered_module_9, llama_cpp__weight_int8pack_mm_default_11, llama_cpp__weight_int8pack_mm_default_12)\n",
      "(lowered_module_10, getitem_22, llama_cpp__weight_int8pack_mm_default_13)\n",
      "(lowered_module_11, getitem_5, getitem_6, llama_cpp__weight_int8pack_mm_default_14, llama_cpp__weight_int8pack_mm_default_15, llama_cpp__weight_int8pack_mm_default_16)\n",
      "(lowered_module_12, sdpa_with_kv_cache_2)\n",
      "(lowered_module_13, getitem_26, llama_cpp__weight_int8pack_mm_default_17)\n",
      "(lowered_module_14, llama_cpp__weight_int8pack_mm_default_18, llama_cpp__weight_int8pack_mm_default_19)\n",
      "(lowered_module_15, getitem_34, llama_cpp__weight_int8pack_mm_default_20)\n",
      "(lowered_module_16, getitem_5, getitem_6, llama_cpp__weight_int8pack_mm_default_21, llama_cpp__weight_int8pack_mm_default_22, llama_cpp__weight_int8pack_mm_default_23)\n",
      "(lowered_module_17, sdpa_with_kv_cache_3)\n",
      "(lowered_module_18, getitem_38, llama_cpp__weight_int8pack_mm_default_24)\n",
      "(lowered_module_19, llama_cpp__weight_int8pack_mm_default_25, llama_cpp__weight_int8pack_mm_default_26)\n",
      "(lowered_module_20, getitem_46, llama_cpp__weight_int8pack_mm_default_27)\n",
      "(lowered_module_21, getitem_5, getitem_6, llama_cpp__weight_int8pack_mm_default_28, llama_cpp__weight_int8pack_mm_default_29, llama_cpp__weight_int8pack_mm_default_30)\n",
      "(lowered_module_22, sdpa_with_kv_cache_4)\n",
      "(lowered_module_23, getitem_50, llama_cpp__weight_int8pack_mm_default_31)\n",
      "(lowered_module_24, llama_cpp__weight_int8pack_mm_default_32, llama_cpp__weight_int8pack_mm_default_33)\n",
      "(lowered_module_25, getitem_58, llama_cpp__weight_int8pack_mm_default_34)\n",
      "(lowered_module_26, getitem_5, getitem_6, llama_cpp__weight_int8pack_mm_default_35, llama_cpp__weight_int8pack_mm_default_36, llama_cpp__weight_int8pack_mm_default_37)\n",
      "(lowered_module_27, sdpa_with_kv_cache_5)\n",
      "(lowered_module_28, getitem_62, llama_cpp__weight_int8pack_mm_default_38)\n",
      "(lowered_module_29, llama_cpp__weight_int8pack_mm_default_39, llama_cpp__weight_int8pack_mm_default_40)\n",
      "(lowered_module_30, getitem_70, llama_cpp__weight_int8pack_mm_default_41)\n"
     ]
    }
   ],
   "source": [
    "for node in mps_module.graph.nodes:\n",
    "    if node.target == executorch_call_delegate:\n",
    "        print(node.args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "executorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
